{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "231fd1fc180f449ca258d07a7a119b3a",
      "cf1c47bad3154763aba233c890e03ea1",
      "cc84fa50b0474d96b58265c77ae3c756",
      "d3dd0b99423f4593a1d4e9dbded83da1",
      "0ce5ccd7162245e5a40e52d9bcb2f8af",
      "11aa875433c94ecdb7c9d5cb450fe049",
      "996eca95ad58400a9d85c8426cd584f8",
      "a069cf545f534ce3b88fea8168d505d5",
      "14c9091cdf304cdfbcf9464f07390c9f",
      "4f7a1c9c81ea4945b9c97635d1542a54",
      "2c0df1a3570d4707854d5a941325a4c0",
      "23b8f48359c644aca65923ab366acb06",
      "556fbbbbb3cf45eebf313d4328a2b8a1",
      "68604bf106d04bb888fd5a4e8557ec83",
      "1ec6dab0093a4a25b70ddfbbc3064311",
      "879a46a3084b4f8cbd0beab1cabc41db",
      "3d73913da63b4400a78b35f017ac3093",
      "0eb58470bfb142dca33497ee4b647c3b",
      "d614aee36bcc4506b9d8f3818f9b19a0",
      "05e505ed659f4c97b6ddd55f90b162fe",
      "845b78b379414586b0044c2b3a46036b",
      "2b08bc777e554917a405b5be35eb1da6",
      "9856d182e3904f6aada33f927af455c4",
      "2a78c646586743e78e77ac3233c3854e",
      "20b0d76b1bd14de8a9a126ca1e62a13b",
      "bb1bb0d26315450f86936fa5384169a6",
      "c8f6424e46a24146a815356313d2def8",
      "12dc3cce24824735b97e8789fb01e77a",
      "00a175706fc348d68578e739da15a75d",
      "0442c7fac7a9436cbb765eb7ae292e14",
      "83db8b50d706434b9a993ea611dfc746",
      "0da1d8c74b864e17b162cb09d1585d2c",
      "e403cfc1766042f8be395fb393b4ee56",
      "58157bd3d5ed4169a68790118c4db4cb",
      "52f358de4cfb4bd99b428847997d80c1",
      "4f6418b0b6934deeabcf4a9ea1a11497",
      "a9d778ca184b494fb97f63a45a7fa358",
      "fe53f38c73524d838e3daecf04c9b937",
      "d239b76c38004f35a4914525f4a23a4d",
      "42ce5e7287984db2b0f0a731381193d7",
      "6cb0c7246def42778f8508160c308575",
      "5844608062e34fc38d51a843a51bccc0",
      "5a4edba3c5de48e3b27bd8124e291f83",
      "6249e679ace849cb82e4b54cf86f30f5",
      "298787fa51554b21bb29c115fa13bbdc",
      "f26c33a7bae344ea9f29bc96bfc325b8",
      "0d88086521df4c4080b2386129f4af00",
      "eb762eb8398042e4914430f61586777d",
      "a5cc87c1fb2e4317a64ff9e526f59e8b",
      "9fd7cb6a25ac4138bc050555fc6f53b5",
      "db859b6bd6834b41b02b7d641d86e823",
      "bbdb75c8f40744078b4b589f8dd59831",
      "2c1b7410473548d7bb82cb9cc8ed1c9b",
      "a8500a8686ce47338490733dc75beb32",
      "b7d307f1bebe48dd88481d0ddba87d5b"
     ]
    },
    "id": "RE47xgz5UJ3s",
    "outputId": "376ae5d8-bc56-4607-e901-261e47fb553c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mini-ImageNet Data Module for Vision Transformer Training\n",
    "Loads Mini-ImageNet dataset from Hugging Face datasets library\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Tuple, Optional, Dict, Any\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MiniImageNetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for Mini-ImageNet from Hugging Face datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str = \"train\",\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "        cache_dir: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Mini-ImageNet dataset\n",
    "\n",
    "        Args:\n",
    "            split: Dataset split - 'train', 'validation', or 'test'\n",
    "            transform: Torchvision transforms to apply\n",
    "            cache_dir: Directory to cache downloaded dataset\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load dataset from Hugging Face\n",
    "        print(f\"Loading Mini-ImageNet {split} split from Hugging Face...\")\n",
    "        self.dataset = load_dataset(\"zh-plus/tiny-imagenet\", split=split, cache_dir=cache_dir)\n",
    "\n",
    "        print(f\"Loaded {len(self.dataset)} samples\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "\n",
    "        Args:\n",
    "            idx: Index of the sample\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (image_tensor, label)\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # Get image and label\n",
    "        image = sample['image']  # PIL Image\n",
    "        label = sample['label']  # int\n",
    "\n",
    "        # Convert grayscale to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Default: convert PIL to tensor\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class MiniImageNetDataModule:\n",
    "    \"\"\"\n",
    "    Data module to handle Mini-ImageNet dataset loading and preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 4,\n",
    "        image_size: int = 256,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        augment_train: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Mini-ImageNet data module\n",
    "\n",
    "        Args:\n",
    "            batch_size: Batch size for data loaders\n",
    "            num_workers: Number of workers for data loading\n",
    "            image_size: Size to resize images (default 256 for Mini-ImageNet)\n",
    "            cache_dir: Directory to cache dataset\n",
    "            augment_train: Whether to apply data augmentation to training set\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.image_size = image_size\n",
    "        self.cache_dir = cache_dir\n",
    "        self.augment_train = augment_train\n",
    "\n",
    "        # Setup transforms\n",
    "        self.setup_transforms()\n",
    "\n",
    "        # Initialize datasets\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def setup_transforms(self):\n",
    "        \"\"\"Setup data transforms for training, validation, and testing\"\"\"\n",
    "\n",
    "        # Base transforms for validation/test\n",
    "        # Using bilinear interpolation (default) for high-quality resizing\n",
    "        base_transforms = [\n",
    "            # transforms.Resize((self.image_size, self.image_size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "\n",
    "        # Training transforms with augmentation\n",
    "        if self.augment_train:\n",
    "            train_transforms = [\n",
    "                # Resize to slightly larger size for random crop\n",
    "                # transforms.Resize(int(self.image_size * 1.15), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                # transforms.RandomCrop(self.image_size),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                # Random erasing for better generalization\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))\n",
    "            ]\n",
    "            self.train_transform = transforms.Compose(train_transforms)\n",
    "        else:\n",
    "            self.train_transform = transforms.Compose(base_transforms)\n",
    "\n",
    "        # Validation/Test transforms (no augmentation)\n",
    "        self.val_transform = transforms.Compose(base_transforms)\n",
    "        self.test_transform = transforms.Compose(base_transforms)\n",
    "\n",
    "    def setup_datasets(self):\n",
    "        \"\"\"Setup train, validation, and test datasets\"\"\"\n",
    "        print(\"Setting up Mini-ImageNet datasets...\")\n",
    "\n",
    "        self.train_dataset = MiniImageNetDataset(\n",
    "            split=\"train\",\n",
    "            transform=self.train_transform,\n",
    "            cache_dir=self.cache_dir\n",
    "        )\n",
    "\n",
    "        self.val_dataset = MiniImageNetDataset(\n",
    "            split=\"valid\",\n",
    "            transform=self.val_transform,\n",
    "            cache_dir=self.cache_dir\n",
    "        )\n",
    "\n",
    "        # self.test_dataset = MiniImageNetDataset(\n",
    "        #     split=\"test\",\n",
    "        #     transform=self.test_transform,\n",
    "        #     cache_dir=self.cache_dir\n",
    "        # )\n",
    "\n",
    "        print(f\"Train dataset: {len(self.train_dataset)} samples\")\n",
    "        print(f\"Validation dataset: {len(self.val_dataset)} samples\")\n",
    "        # print(f\"Test dataset: {len(self.test_dataset)} samples\")\n",
    "\n",
    "    def get_dataloaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Get train, validation, and test data loaders\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (train_loader, val_loader, test_loader)\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None or self.val_dataset is None or self.test_dataset is None:\n",
    "            self.setup_datasets()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True  # For stable batch norm statistics\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # test_loader = DataLoader(\n",
    "        #     self.test_dataset,\n",
    "        #     batch_size=self.batch_size,\n",
    "        #     shuffle=False,\n",
    "        #     num_workers=self.num_workers,\n",
    "        #     pin_memory=True\n",
    "        # )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def get_sample_batch(self, split: str = \"train\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a sample batch for testing\n",
    "\n",
    "        Args:\n",
    "            split: Which split to sample from ('train', 'validation', or 'test')\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (images, labels) tensors\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None or self.val_dataset is None or self.test_dataset is None:\n",
    "            self.setup_datasets()\n",
    "\n",
    "        if split == \"train\":\n",
    "            dataset = self.train_dataset\n",
    "        elif split == \"validation\" or split == \"valid\":\n",
    "            dataset = self.val_dataset\n",
    "        else:\n",
    "            dataset = self.test_dataset\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return next(iter(loader))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the data module\n",
    "    print(\"Testing Mini-ImageNet Data Module\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create data module\n",
    "    data_module = MiniImageNetDataModule(\n",
    "        batch_size=16,\n",
    "        num_workers=2,\n",
    "        image_size=256,\n",
    "        augment_train=True\n",
    "    )\n",
    "\n",
    "    # Setup datasets\n",
    "    data_module.setup_datasets()\n",
    "\n",
    "    # Get basic info\n",
    "    print(f\"\\nDataset Information:\")\n",
    "    print(f\"Train dataset size: {len(data_module.train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(data_module.val_dataset)}\")\n",
    "    # print(f\"Test dataset size: {len(data_module.test_dataset)}\")\n",
    "    print(f\"Number of classes: 100\")\n",
    "\n",
    "    # Get a sample from train dataset\n",
    "    print(f\"\\nSample from training set:\")\n",
    "    image, label = data_module.train_dataset[0]\n",
    "    print(f\"Image shape: {image.shape}\")\n",
    "    print(f\"Image dtype: {image.dtype}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Image min/max values: {image.min():.3f} / {image.max():.3f}\")\n",
    "\n",
    "    # Get a sample batch\n",
    "    print(f\"\\nSample batch:\")\n",
    "    images, labels = data_module.get_sample_batch(\"train\")\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    print(f\"First 5 labels: {labels[:5].tolist()}\")\n",
    "\n",
    "    # Test data loaders\n",
    "    print(f\"\\nTesting data loaders:\")\n",
    "    train_loader, val_loader = data_module.get_dataloaders()\n",
    "    print(f\"Train loader batches: {len(train_loader)}\")\n",
    "    print(f\"Validation loader batches: {len(val_loader)}\")\n",
    "    # print(f\"Test loader batches: {len(test_loader)}\")\n",
    "\n",
    "    # Test a few batches\n",
    "    print(f\"\\nTesting first batch from train loader:\")\n",
    "    batch_images, batch_labels = next(iter(train_loader))\n",
    "    print(f\"Batch shape: {batch_images.shape}\")\n",
    "    print(f\"Labels shape: {batch_labels.shape}\")\n",
    "\n",
    "    print(f\"\\nTesting first batch from validation loader:\")\n",
    "    batch_images, batch_labels = next(iter(val_loader))\n",
    "    print(f\"Batch shape: {batch_images.shape}\")\n",
    "    print(f\"Labels shape: {batch_labels.shape}\")\n",
    "\n",
    "    print(f\"\\nMini-ImageNet Data Module test completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModulatedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention with latent token modulation and learnable aggregation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_latents, num_heads=1, qkv_bias=True, dropout=0.0,\n",
    "                 modulate_v=True, aggregation='max'):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} must be divisible by num_heads {num_heads}\"\n",
    "        assert aggregation in ['mean', 'max', 'learned_weight', 'attention', 'gated'], \\\n",
    "            f\"aggregation must be one of ['mean', 'max', 'learned_weight', 'attention', 'gated'], got {aggregation}\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.modulate_v = modulate_v\n",
    "        self.aggregation = aggregation\n",
    "        self.num_latents = num_latents\n",
    "        \n",
    "        # --- VISUALIZATION STATE ---\n",
    "        self.save_indices = False\n",
    "        self.saved_indices = None\n",
    "        # ---------------------------\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        # self.proj = nn.Linear(dim, dim)\n",
    "        # self.proj_drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def aggregate_tokens(self, tokens):\n",
    "        if self.aggregation == 'mean':\n",
    "            return tokens.mean(dim=2, keepdim=True)\n",
    "        elif self.aggregation == 'max':\n",
    "            max_tokens, indices = tokens.topk(12, dim=3) # hardcoded 12 for imagenet-1k\n",
    "            return max_tokens, indices\n",
    "        elif self.aggregation == 'learned_weight':\n",
    "            weights = self.aggregation_weight(tokens)\n",
    "            weights = F.softmax(weights, dim=2)\n",
    "            weighted = tokens * weights\n",
    "            return weighted.sum(dim=2, keepdim=True)\n",
    "        elif self.aggregation == 'attention':\n",
    "            query = tokens.mean(dim=2, keepdim=True)\n",
    "            query = self.agg_query(query)\n",
    "            scores = (query @ tokens.transpose(-2, -1)) * self.agg_scale\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            aggregated = attn_weights @ tokens\n",
    "            return aggregated\n",
    "        elif self.aggregation == 'gated':\n",
    "            gates = self.gate_net(tokens)\n",
    "            gated = tokens * gates\n",
    "            return gated.mean(dim=2, keepdim=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation: {self.aggregation}\")\n",
    "    \n",
    "    def modulate_cls_token(self, cls_token, other_tokens, same_tokens):\n",
    "        cls_token = cls_token.unsqueeze(3)\n",
    "        other_tokens = other_tokens.unsqueeze(2)\n",
    "        same_tokens = same_tokens.unsqueeze(2)\n",
    "        interaction = cls_token * other_tokens # Qcls X Kx\n",
    "        modulated = interaction + same_tokens  # Qx + (Qcls X Kx)\n",
    "        modulated_cls, indices = self.aggregate_tokens(modulated)\n",
    "        return modulated_cls.squeeze(2), indices.squeeze(2)\n",
    "    \n",
    "    def modulate_v_cls(self, v_cls ,qk_cls, v_full):\n",
    "        v_squared = v_full ** 2\n",
    "        two_v = 2 * v_full\n",
    "        # product of original qcls and kcls are used here, mean of modulated q_cls and k_cls gives slightly better performance\n",
    "        qk_interaction = qk_cls \n",
    "        v_cls_abs = torch.abs(v_cls)\n",
    "        gating = 1 + v_cls_abs \n",
    "        interaction_term = qk_interaction * gating \n",
    "        interaction_term = interaction_term.unsqueeze(3)\n",
    "        v_squared = v_squared.unsqueeze(2)\n",
    "        two_v = two_v.unsqueeze(2)\n",
    "        modulated_v = v_squared + two_v + interaction_term\n",
    "        v_cls_modulated, indices = self.aggregate_tokens(modulated_v)\n",
    "        return v_cls_modulated.squeeze(2), indices.squeeze(2), modulated_v.squeeze(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        L = self.num_latents \n",
    "        q_cls, q_patches = q[:, :, 0:L, :], q[:, :, L:, :]\n",
    "        k_cls, k_patches = k[:, :, 0:L, :], k[:, :, L:, :]\n",
    "        v_cls, v_patches = v[:, :, 0:L, :], v[:, :, L:, :]\n",
    "    \n",
    "        q_cls_modulated, _ = self.modulate_cls_token(q_cls, k_patches, q_patches)\n",
    "        k_cls_modulated, _ = self.modulate_cls_token(k_cls, q_patches, k_patches)\n",
    "        v_cls_modulated, v_indices, modulated_v = self.modulate_v_cls(v_cls, q_cls*k_cls, v_patches)\n",
    "\n",
    "        # --- CAPTURE INDICES (Added for Visualization) ---\n",
    "        if self.save_indices:\n",
    "            # v_indices shape is likely (B, H, 12, D)\n",
    "            self.saved_indices = v_indices.detach().clone()\n",
    "        # -------------------------------------------------\n",
    "\n",
    "        q_cls_modulated = torch.cat([q_cls, q_cls_modulated], dim=2)\n",
    "        k_cls_modulated = torch.cat([k_cls, k_cls_modulated], dim=2)\n",
    "        v_cls_modulated = torch.cat([v_cls, v_cls_modulated], dim=2)\n",
    "    \n",
    "        attn = (q_cls_modulated @ k_cls_modulated.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "    \n",
    "        out = attn @ v_cls_modulated\n",
    "\n",
    "        # --- Reconstructing output to match shapes for scatter ---\n",
    "        out = out.transpose(1, 2).reshape(B, 13, C) \n",
    "        out_cls, out_ = out[:, 0], out[:, 1:]  \n",
    "        \n",
    "        # Create zeros tensor with same dtype as out_ to avoid AMP dtype mismatch\n",
    "        skip_v_zeros = torch.zeros_like(modulated_v.squeeze(1), dtype=out_.dtype)  \n",
    "        \n",
    "        # v_indices.squeeze(1) is (B, 12, D). out_ is (B, 12, C). C=D. \n",
    "        # This scatter maps the Top K features back to their original token positions (dim=1)\n",
    "        skip_v_zeros = skip_v_zeros.scatter_(dim=1, index=v_indices.squeeze(1), src=out_)\n",
    "\n",
    "        v_out = modulated_v.squeeze(1).to(out_.dtype) + skip_v_zeros\n",
    "        v_out = torch.cat([out_cls.unsqueeze(1), v_out], dim=1)\n",
    "        \n",
    "        return v_out\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhFQMKXmYGv2",
    "outputId": "02bc0db2-bbff-4f3c-fd30-96f6398d08e8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vision Transformer Block Module\n",
    "Contains the transformer block with modulated attention, MLP, and layer normalization\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple , List, Union\n",
    "import math\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (Feed-Forward Network) for Vision Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: Optional[int] = None,\n",
    "        out_features: Optional[int] = None,\n",
    "        activation: nn.Module = nn.GELU,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Input feature dimension\n",
    "            hidden_features: Hidden layer dimension (default: 4 * in_features)\n",
    "            out_features: Output feature dimension (default: in_features)\n",
    "            activation: Activation function\n",
    "            dropout: Dropout rate\n",
    "            bias: Whether to use bias in linear layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features * 4\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)\n",
    "        self.act = activation()\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, N, C)\n",
    "        Returns:\n",
    "            Output tensor of shape (B, N, C)\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HybridTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Block that can use either Modulated or Standard Attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "        attn_dropout: float = 0.0,\n",
    "        drop_path: float = 0.0,\n",
    "        activation: nn.Module = nn.GELU,\n",
    "        norm_layer: nn.Module = nn.LayerNorm,\n",
    "        attention_type: str = 'modulated',  # 'modulated' or 'standard'\n",
    "        modulate_v: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Input embedding dimension\n",
    "            num_heads: Number of attention heads\n",
    "            mlp_ratio: Ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias: Whether to add bias to qkv projection\n",
    "            dropout: Dropout rate for MLP\n",
    "            attn_dropout: Dropout rate for attention\n",
    "            drop_path: Stochastic depth rate\n",
    "            activation: Activation function for MLP\n",
    "            norm_layer: Normalization layer\n",
    "            attention_type: Type of attention ('modulated' or 'standard')\n",
    "            modulate_v: Whether to modulate V in modulated attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_type = attention_type\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        \n",
    "        # Select attention mechanism\n",
    "        if attention_type == 'modulated':\n",
    "            self.attn = ModulatedMultiHeadAttention(\n",
    "                dim=dim,\n",
    "                num_latents=1,\n",
    "                num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                dropout=attn_dropout,\n",
    "                modulate_v=modulate_v\n",
    "            )\n",
    "        elif attention_type == 'standard':\n",
    "            self.attn = MultiHeadAttention(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                dropout=attn_dropout\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown attention type: {attention_type}. Use 'modulated' or 'standard'\")\n",
    "        \n",
    "        # MLP\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            activation=activation,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Stochastic depth\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, N, C)\n",
    "        Returns:\n",
    "            Output tensor of shape (B, N, C)\n",
    "        \"\"\"\n",
    "        # Attention block with residual connection\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        # x = self.drop_path(self.attn(self.norm1(x)))\n",
    "        \n",
    "        # MLP block with residual connection\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_attention_weights(self, x):\n",
    "        \"\"\"Get attention weights for visualization\"\"\"\n",
    "        x_norm = self.norm1(x)\n",
    "        return self.attn.get_attention_weights(x_norm) if hasattr(self.attn, 'get_attention_weights') else None\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'attention_type={self.attention_type}, dim={self.dim}, num_heads={self.num_heads}'\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            drop_prob: Probability of dropping a path\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()  # binarize\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={self.drop_prob}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tp2fNJUndIJs",
    "outputId": "9e14b40d-f727-4843-dbf4-f25f9ee87f0f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete Vision Transformer Implementation\n",
    "Includes patch embedding, positional encoding, and full ViT architecture\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "import math\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding\n",
    "    Converts images into patches and linearly embeds them\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_size: Size of input image (assumed square)\n",
    "            patch_size: Size of each patch (assumed square)\n",
    "            in_channels: Number of input channels (3 for RGB)\n",
    "            embed_dim: Embedding dimension\n",
    "            bias: Whether to use bias in projection layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Calculate number of patches\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.grid_size = image_size // patch_size\n",
    "\n",
    "        # Patch embedding using convolution\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "        # Layer normalization (optional, but often helpful)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            Patch embeddings of shape (B, num_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Verify input dimensions\n",
    "        assert H == self.image_size and W == self.image_size, \\\n",
    "            f\"Input image size ({H}, {W}) doesn't match model ({self.image_size}, {self.image_size})\"\n",
    "        assert C == self.in_channels, \\\n",
    "            f\"Input channels ({C}) doesn't match model ({self.in_channels})\"\n",
    "\n",
    "        # Extract patches and flatten\n",
    "        x = self.proj(x)  # (B, embed_dim, grid_size, grid_size)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "\n",
    "        # Apply layer normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_patch_info(self):\n",
    "        \"\"\"Return patch embedding information\"\"\"\n",
    "        return {\n",
    "            'num_patches': self.num_patches,\n",
    "            'grid_size': self.grid_size,\n",
    "            'patch_size': self.patch_size,\n",
    "            'embed_dim': self.embed_dim\n",
    "        }\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable positional embeddings for Vision Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_patches: int, embed_dim: int, dropout: float = 0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_patches: Number of patches in the image\n",
    "            embed_dim: Embedding dimension\n",
    "            dropout: Dropout rate for positional embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Learnable positional embeddings (including CLS token)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim) * 0.02)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Patch embeddings of shape (B, num_patches + 1, embed_dim)\n",
    "        Returns:\n",
    "            Embeddings with positional encoding of same shape\n",
    "        \"\"\"\n",
    "        return self.dropout(x + self.pos_embed)\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Classification head for Vision Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_classes: int,\n",
    "        dropout: float = 0.0,\n",
    "        use_norm: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Input embedding dimension\n",
    "            num_classes: Number of output classes\n",
    "            dropout: Dropout rate before final linear layer\n",
    "            use_norm: Whether to apply layer norm before classification\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim) if use_norm else nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize with smaller weights for better training stability\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        if self.head.bias is not None:\n",
    "            nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: CLS token features of shape (B, embed_dim)\n",
    "        Returns:\n",
    "            Class logits of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class HybridVisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer with Hybrid Attention Architecture\n",
    "    Supports mixing modulated and standard attention layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        num_classes: int = 1000,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 1,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "        attn_dropout: float = 0.0,\n",
    "        drop_path_rate: float = 0.1,\n",
    "        attention_pattern: Union[str, List[str]] = 'modulated',\n",
    "        modulate_v: bool = False,\n",
    "        norm_layer: nn.Module = nn.LayerNorm,\n",
    "        activation: nn.Module = nn.GELU\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_size: Input image size\n",
    "            patch_size: Patch size for patch embedding\n",
    "            in_channels: Number of input channels\n",
    "            num_classes: Number of classification classes\n",
    "            embed_dim: Embedding dimension\n",
    "            depth: Number of transformer blocks\n",
    "            num_heads: Number of attention heads\n",
    "            mlp_ratio: Ratio of MLP hidden dimension to embedding dimension\n",
    "            qkv_bias: Whether to add bias to qkv projection\n",
    "            dropout: Dropout rate\n",
    "            attn_dropout: Attention dropout rate\n",
    "            drop_path_rate: Stochastic depth rate\n",
    "            attention_pattern: Pattern of attention types. Options:\n",
    "                - 'modulated': All layers use modulated attention\n",
    "                - 'standard': All layers use standard attention\n",
    "                - 'alternating': Alternates between modulated and standard\n",
    "                - 'early_modulated': First half modulated, second half standard\n",
    "                - 'late_modulated': First half standard, second half modulated\n",
    "                - List[str]: Explicit list of attention types per layer\n",
    "            modulate_v: Whether to modulate V in modulated attention layers\n",
    "            norm_layer: Normalization layer\n",
    "            activation: Activation function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        \n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embed = PositionalEmbedding(\n",
    "            num_patches=num_patches,\n",
    "            embed_dim=embed_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Determine attention pattern for each layer\n",
    "        attention_types = self._parse_attention_pattern(attention_pattern, depth)\n",
    "        self.attention_types = attention_types\n",
    "        \n",
    "        # Stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        \n",
    "        # Build transformer blocks with hybrid attention\n",
    "        self.blocks = nn.ModuleList([\n",
    "            HybridTransformerBlock(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                dropout=dropout,\n",
    "                attn_dropout=attn_dropout,\n",
    "                drop_path=dpr[i],\n",
    "                activation=activation,\n",
    "                norm_layer=norm_layer,\n",
    "                attention_type=attention_types[i],\n",
    "                modulate_v=modulate_v\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = ClassificationHead(\n",
    "            embed_dim=embed_dim,\n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _parse_attention_pattern(self, pattern: Union[str, List[str]], depth: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Parse attention pattern into a list of attention types per layer\n",
    "        \n",
    "        Args:\n",
    "            pattern: Attention pattern specification\n",
    "            depth: Number of layers\n",
    "        \n",
    "        Returns:\n",
    "            List of attention types for each layer\n",
    "        \"\"\"\n",
    "        if isinstance(pattern, list):\n",
    "            assert len(pattern) == depth, f\"Pattern list length {len(pattern)} must match depth {depth}\"\n",
    "            return pattern\n",
    "        \n",
    "        if pattern == 'modulated':\n",
    "            return ['modulated'] * depth\n",
    "        elif pattern == 'standard':\n",
    "            return ['standard'] * depth\n",
    "        elif pattern == 'alternating':\n",
    "            return ['modulated' if i % 2 == 0 else 'standard' for i in range(depth)]\n",
    "        elif pattern == 'early_modulated':\n",
    "            mid = depth // 2\n",
    "            return ['modulated'] * mid + ['standard'] * (depth - mid)\n",
    "        elif pattern == 'late_modulated':\n",
    "            mid = depth // 2\n",
    "            return ['standard'] * mid + ['modulated'] * (depth - mid)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown attention pattern: {pattern}\")\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.ones_(m.weight)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward_features(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through patch embedding, positional encoding, and transformer blocks\n",
    "        \n",
    "        Args:\n",
    "            x: Input images of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            Feature tokens of shape (B, num_patches + 1, embed_dim)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_embed(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input images of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            Class logits of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.forward_features(x)\n",
    "        cls_token = x[:, 0]\n",
    "        logits = self.head(cls_token)\n",
    "        return logits\n",
    "    \n",
    "    def get_attention_maps(self, x, block_idx: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Extract attention maps for visualization\n",
    "        \n",
    "        Args:\n",
    "            x: Input images\n",
    "            block_idx: Which block to extract attention from (None for all)\n",
    "        Returns:\n",
    "            Attention maps with layer information\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = self.pos_embed(x)\n",
    "        \n",
    "        attention_maps = []\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if block_idx is None or i == block_idx:\n",
    "                attn_info = block.get_attention_weights(x)\n",
    "                if attn_info is not None:\n",
    "                    attention_maps.append({\n",
    "                        'layer': i,\n",
    "                        'attention_type': self.attention_types[i],\n",
    "                        'weights': attn_info[0],\n",
    "                        'modulation_info': attn_info[1] if len(attn_info) > 1 else None\n",
    "                    })\n",
    "            x = block(x)\n",
    "        \n",
    "        return attention_maps if len(attention_maps) > 1 else attention_maps[0] if attention_maps else None\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get comprehensive model information including attention pattern\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Count attention types\n",
    "        modulated_count = sum(1 for t in self.attention_types if t == 'modulated')\n",
    "        standard_count = sum(1 for t in self.attention_types if t == 'standard')\n",
    "        \n",
    "        return {\n",
    "            'image_size': self.image_size,\n",
    "            'patch_size': self.patch_size,\n",
    "            'num_patches': self.patch_embed.num_patches,\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'depth': self.depth,\n",
    "            'num_classes': self.num_classes,\n",
    "            'attention_pattern': self.attention_types,\n",
    "            'modulated_layers': modulated_count,\n",
    "            'standard_layers': standard_count,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params,\n",
    "            'model_size_mb': total_params * 4 / 1024**2\n",
    "        }\n",
    "    \n",
    "    def print_architecture(self):\n",
    "        \"\"\"Print a visual representation of the architecture\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Hybrid Vision Transformer Architecture\")\n",
    "        print(\"=\" * 70)\n",
    "        info = self.get_model_info()\n",
    "        print(f\"Image Size: {info['image_size']}x{info['image_size']}\")\n",
    "        print(f\"Patches: {info['num_patches']} ({self.patch_embed.grid_size}x{self.patch_embed.grid_size})\")\n",
    "        print(f\"Embedding Dim: {info['embed_dim']}\")\n",
    "        print(f\"Total Layers: {info['depth']}\")\n",
    "        print(f\"  - Modulated Attention: {info['modulated_layers']}\")\n",
    "        print(f\"  - Standard Attention: {info['standard_layers']}\")\n",
    "        print(f\"Parameters: {info['total_params']:,} ({info['model_size_mb']:.2f} MB)\")\n",
    "        print(\"\\nLayer-by-Layer Attention Types:\")\n",
    "        print(\"-\" * 70)\n",
    "        for i, attn_type in enumerate(self.attention_types):\n",
    "            symbol = \"ðŸ”·\" if attn_type == \"modulated\" else \"â¬œ\"\n",
    "            print(f\"  Layer {i:2d}: {symbol} {attn_type.capitalize()}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def create_hybrid_vit(\n",
    "    model_size: str = 'tiny',\n",
    "    num_classes: int = 10,\n",
    "    image_size: int = 32,\n",
    "    attention_pattern: Union[str, List[str]] = 'modulated',\n",
    "    modulate_v: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Factory function to create Hybrid Vision Transformer models\n",
    "    \n",
    "    Args:\n",
    "        model_size: Size of the model ('tiny', 'small', 'base', 'large')\n",
    "        num_classes: Number of classes for classification\n",
    "        image_size: Input image size\n",
    "        attention_pattern: Pattern of attention types\n",
    "        modulate_v: Whether to modulate V in modulated layers\n",
    "    \n",
    "    Returns:\n",
    "        Hybrid Vision Transformer model\n",
    "    \"\"\"\n",
    "    configs = {\n",
    "        'tiny': {\n",
    "            'embed_dim': 192,\n",
    "            'depth': 12,\n",
    "            'num_heads': 3,\n",
    "            'patch_size': 16 if image_size >= 224 else 4,\n",
    "            'mlp_ratio': 4.0,\n",
    "            'dropout': 0.0,\n",
    "            'drop_path_rate': 0.1\n",
    "        },\n",
    "        'small': {\n",
    "            'embed_dim': 384,\n",
    "            'depth': 2,\n",
    "            'num_heads': 1,\n",
    "            'patch_size': 8,\n",
    "            'mlp_ratio': 4.0,\n",
    "            'dropout': 0.1,\n",
    "            'drop_path_rate': 0.1\n",
    "        },\n",
    "        'base': {\n",
    "            'embed_dim': 768,\n",
    "            'depth': 12,\n",
    "            'num_heads': 12,\n",
    "            'patch_size': 16 if image_size >= 224 else 4,\n",
    "            'mlp_ratio': 4.0,\n",
    "            'dropout': 0.1,\n",
    "            'drop_path_rate': 0.1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    assert model_size in configs, f\"Model size {model_size} not supported\"\n",
    "    config = configs[model_size]\n",
    "    \n",
    "    return HybridVisionTransformer(\n",
    "        image_size=image_size,\n",
    "        patch_size=config['patch_size'],\n",
    "        num_classes=num_classes,\n",
    "        embed_dim=config['embed_dim'],\n",
    "        depth=config['depth'],\n",
    "        num_heads=config['num_heads'],\n",
    "        mlp_ratio=config['mlp_ratio'],\n",
    "        dropout=config['dropout'],\n",
    "        drop_path_rate=config['drop_path_rate'],\n",
    "        attention_pattern=attention_pattern,\n",
    "        modulate_v=modulate_v\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# --- Weights & Biases setup ---\n",
    "wandb.login(key=\"\")  # add key here\n",
    "\n",
    "training_config = {\n",
    "            'embed_dim': 384,\n",
    "            'depth': 2,\n",
    "            'head': 1,\n",
    "            'patch_size': 8,\n",
    "            'mlp_ratio': 4.0,\n",
    "            'dropout': 0.1,\n",
    "            'drop_path_rate': 0.1\n",
    "        }\n",
    "\n",
    "wandb.init(\n",
    "    project=\"\",       # W&B project name\n",
    "    name=\"\",      # optional descriptive run name\n",
    "    config=training_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VwbQBV_DgAP6",
    "outputId": "82d8f07f-e079-44be-d276-e91bd825149b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "import time\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import wandb\n",
    "from typing import List, Optional\n",
    "\n",
    "class TrainingMetrics:\n",
    "    \"\"\"Class to track training metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def update_train(self, loss: float, accuracy: float, lr: float):\n",
    "        self.train_losses.append(loss)\n",
    "        self.train_accuracies.append(accuracy)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "    def update_val(self, loss: float, accuracy: float):\n",
    "        self.val_losses.append(loss)\n",
    "        self.val_accuracies.append(accuracy)\n",
    "\n",
    "    def add_epoch_time(self, time: float):\n",
    "        self.epoch_times.append(time)\n",
    "\n",
    "    def get_best_val_accuracy(self):\n",
    "        return max(self.val_accuracies) if self.val_accuracies else 0.0\n",
    "\n",
    "    def get_latest_metrics(self):\n",
    "        return {\n",
    "            'train_loss': self.train_losses[-1] if self.train_losses else 0.0,\n",
    "            'train_acc': self.train_accuracies[-1] if self.train_accuracies else 0.0,\n",
    "            'val_loss': self.val_losses[-1] if self.val_losses else 0.0,\n",
    "            'val_acc': self.val_accuracies[-1] if self.val_accuracies else 0.0,\n",
    "            'lr': self.learning_rates[-1] if self.learning_rates else 0.0\n",
    "        }\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device, print_freq=50):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "\n",
    "    Args:\n",
    "        model: Vision Transformer model\n",
    "        train_loader: Training data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        device: Device to train on\n",
    "        print_freq: How often to print progress\n",
    "\n",
    "    Returns:\n",
    "        Average loss and accuracy for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update scheduler if it's step-based\n",
    "        if scheduler is not None and hasattr(scheduler, 'step') and not isinstance(scheduler, (CosineAnnealingLR,)):\n",
    "            scheduler.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Print progress\n",
    "        if batch_idx % print_freq == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            batch_acc = torch.sum(preds == labels.data).item() / inputs.size(0)\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            print(f'  Batch [{batch_idx:3d}/{len(train_loader):3d}] | '\n",
    "                  f'Loss: {loss.item():.4f} | '\n",
    "                  f'Acc: {batch_acc:.4f} | '\n",
    "                  f'LR: {current_lr:.6f} | '\n",
    "                  f'Time: {elapsed:.1f}s')\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate for one epoch\n",
    "\n",
    "    Args:\n",
    "        model: Vision Transformer model\n",
    "        val_loader: Validation data loader\n",
    "        criterion: Loss function\n",
    "        device: Device to validate on\n",
    "\n",
    "    Returns:\n",
    "        Average loss and accuracy for the epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data).item()\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def create_optimizer(model, optimizer_name='adamw', learning_rate=1e-3, weight_decay=0.05):\n",
    "    \"\"\"Create optimizer based on name\"\"\"\n",
    "    if optimizer_name.lower() == 'adamw':\n",
    "        return optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name.lower() == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name.lower() == 'sgd':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "\n",
    "def create_scheduler(optimizer, scheduler_name='cosine', num_epochs=100, steps_per_epoch=None):\n",
    "    \"\"\"Create learning rate scheduler\"\"\"\n",
    "    if scheduler_name.lower() == 'cosine':\n",
    "        return CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    elif scheduler_name.lower() == 'onecycle' and steps_per_epoch is not None:\n",
    "        return OneCycleLR(optimizer, max_lr=optimizer.param_groups[0]['lr'],\n",
    "                         steps_per_epoch=steps_per_epoch, epochs=num_epochs)\n",
    "    elif scheduler_name.lower() == 'none':\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler: {scheduler_name}\")\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, metrics, filepath):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'metrics': {\n",
    "            'train_losses': metrics.train_losses,\n",
    "            'train_accuracies': metrics.train_accuracies,\n",
    "            'val_losses': metrics.val_losses,\n",
    "            'val_accuracies': metrics.val_accuracies,\n",
    "            'learning_rates': metrics.learning_rates\n",
    "        },\n",
    "        'best_val_acc': metrics.get_best_val_accuracy()\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved: {filepath}\")\n",
    "\n",
    "\n",
    "def train_vision_transformer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs: int = 50,\n",
    "    learning_rate: float = 1e-3,\n",
    "    weight_decay: float = 0.05,\n",
    "    optimizer_name: str = 'adamw',\n",
    "    scheduler_name: str = 'cosine',\n",
    "    device: str = 'cuda',\n",
    "    save_path: str = './checkpoints',\n",
    "    print_freq: int = 50,\n",
    "    save_freq: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete training loop for Vision Transformer\n",
    "\n",
    "    Args:\n",
    "        model: Vision Transformer model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        weight_decay: Weight decay for regularization\n",
    "        optimizer_name: Optimizer name ('adamw', 'adam', 'sgd')\n",
    "        scheduler_name: Scheduler name ('cosine', 'onecycle', 'none')\n",
    "        device: Device to train on\n",
    "        save_path: Path to save checkpoints\n",
    "        print_freq: Frequency of printing training progress\n",
    "        save_freq: Frequency of saving checkpoints\n",
    "\n",
    "    Returns:\n",
    "        Trained model and training metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = create_optimizer(model, optimizer_name, learning_rate, weight_decay)\n",
    "\n",
    "    # --- W&B GRADIENT TRACKING SETUP ---\n",
    "    # We check if a run is active to avoid errors if running locally without wandb\n",
    "    if wandb.run is not None:\n",
    "        print(\"W&B: Watching model gradients...\")\n",
    "        # log=\"gradients\" tracks gradients\n",
    "        # log=\"all\" tracks gradients AND parameter values (weights)\n",
    "        # log_freq determines how often to upload histograms (batch-wise)\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=print_freq, log_graph=True)\n",
    "    # -----------------------------------\n",
    "\n",
    "    # Scheduler\n",
    "    steps_per_epoch = len(train_loader) if scheduler_name.lower() == 'onecycle' else None\n",
    "    scheduler = create_scheduler(optimizer, scheduler_name, num_epochs, steps_per_epoch)\n",
    "\n",
    "    # Metrics tracking\n",
    "    metrics = TrainingMetrics()\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "\n",
    "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        print(f\"\\nEpoch [{epoch+1:3d}/{num_epochs:3d}]\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Training phase\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler, device, print_freq\n",
    "        )\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        # Update scheduler (for epoch-based schedulers)\n",
    "        if scheduler is not None and isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "\n",
    "        # Record metrics\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        metrics.update_train(train_loss, train_acc, current_lr)\n",
    "        metrics.update_val(val_loss, val_acc)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        metrics.add_epoch_time(epoch_time)\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"\\n  Results:\")\n",
    "        print(f\"    Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"    Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "        print(f\"    LR: {current_lr:.6f} | Time: {epoch_time:.1f}s\")\n",
    "\n",
    "        # Log to Weights & Biases\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train/loss\": train_loss,\n",
    "                \"train/acc\": train_acc,\n",
    "                \"val/loss\": val_loss,\n",
    "                \"val/acc\": val_acc,\n",
    "                \"lr\": current_lr,\n",
    "                \"epoch_time\": epoch_time,\n",
    "                \"best_val_acc\": best_val_acc\n",
    "            }, step=epoch + 1)\n",
    "        \n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_path = os.path.join(save_path, 'best_model.pth')\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, metrics, best_model_path)\n",
    "            print(f\"    *** New best validation accuracy: {best_val_acc:.4f} ***\")\n",
    "\n",
    "        # Save periodic checkpoint\n",
    "        if (epoch + 1) % save_freq == 0:\n",
    "            checkpoint_path = os.path.join(save_path, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, metrics, checkpoint_path)\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    return model, metrics\n",
    "\n",
    "\n",
    "def plot_training_history(metrics, save_path=None):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle('Training History', fontsize=16)\n",
    "\n",
    "    epochs = range(1, len(metrics.train_losses) + 1)\n",
    "\n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(epochs, metrics.train_losses, 'b-', label='Train Loss', alpha=0.8)\n",
    "    axes[0, 0].plot(epochs, metrics.val_losses, 'r-', label='Val Loss', alpha=0.8)\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy plot\n",
    "    axes[0, 1].plot(epochs, [acc*100 for acc in metrics.train_accuracies], 'b-', label='Train Acc', alpha=0.8)\n",
    "    axes[0, 1].plot(epochs, [acc*100 for acc in metrics.val_accuracies], 'r-', label='Val Acc', alpha=0.8)\n",
    "    axes[0, 1].set_title('Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Learning rate plot\n",
    "    axes[1, 0].plot(epochs, metrics.learning_rates, 'g-', alpha=0.8)\n",
    "    axes[1, 0].set_title('Learning Rate')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Training time plot\n",
    "    if metrics.epoch_times:\n",
    "        axes[1, 1].plot(epochs, metrics.epoch_times, 'm-', alpha=0.8)\n",
    "        axes[1, 1].set_title('Epoch Time')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Time (seconds)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Training history plot saved: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This is where we bring everything together!\n",
    "    print(\"MiniImageNetData Vision Transformer Training\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "    print(\"Setting up data...\")\n",
    "    data_module = MiniImageNetDataModule(\n",
    "        batch_size=128,\n",
    "        num_workers=2,\n",
    "        image_size=64,\n",
    "        augment_train=True\n",
    "    )\n",
    "\n",
    "    train_loader, test_loader = data_module.get_dataloaders()\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "    # Create Vision Transformer model (assuming create_vit_model is loaded)\n",
    "    print(f\"\\nCreating Vision Transformer model...\")\n",
    "    pattern = ['modulated', 'modulated']\n",
    "    model = create_hybrid_vit(\n",
    "            model_size='small',\n",
    "            num_classes=200,\n",
    "            image_size=64,\n",
    "            attention_pattern=pattern,\n",
    "            modulate_v=True\n",
    "        )\n",
    "\n",
    "    model_info = model.get_model_info()\n",
    "    print(f\"Model created:\")\n",
    "    print(f\"  Parameters: {model_info['total_params']:,}\")\n",
    "    print(f\"  Model size: {model_info['model_size_mb']:.2f} MB\")\n",
    "    print(f\"  Patches: {model_info['num_patches']}\")\n",
    "\n",
    "    # Training configuration\n",
    "    training_config = {\n",
    "        'num_epochs': 30,  # Start with fewer epochs for testing\n",
    "        'learning_rate': 3e-4,\n",
    "        'weight_decay': 0.05,\n",
    "        'optimizer_name': 'adamw',\n",
    "        'scheduler_name': 'cosine',\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'save_path': './vit_checkpoints',\n",
    "        'print_freq': 20,\n",
    "        'save_freq': 5\n",
    "    }\n",
    "\n",
    "    print(f\"\\nTraining configuration:\")\n",
    "    for key, value in training_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Start training!\n",
    "    print(f\"\\nðŸš€ Starting training...\")\n",
    "    trained_model, metrics = train_vision_transformer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,  # Using test set as validation for simplicity\n",
    "        **training_config\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    print(f\"\\nPlotting training history...\")\n",
    "    plot_training_history(metrics, save_path='./training_history.png')\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Training completed!\")\n",
    "    print(f\"Best validation accuracy: {metrics.get_best_val_accuracy()*100:.2f}%\")\n",
    "\n",
    "    # Training summary\n",
    "    total_time = sum(metrics.epoch_times) if metrics.epoch_times else 0\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Total training time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"  Average epoch time: {np.mean(metrics.epoch_times):.1f} seconds\")\n",
    "    print(f\"  Final learning rate: {metrics.learning_rates[-1]:.6f}\")\n",
    "\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\n",
    "            \"final/best_val_acc\": metrics.get_best_val_accuracy()\n",
    "        })\n",
    "\n",
    "        wandb.summary[\"best_val_acc\"] = metrics.get_best_val_accuracy()\n",
    "        wandb.summary[\"total_train_time_min\"] = total_time / 60.0\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00a175706fc348d68578e739da15a75d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0442c7fac7a9436cbb765eb7ae292e14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05e505ed659f4c97b6ddd55f90b162fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0ce5ccd7162245e5a40e52d9bcb2f8af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d88086521df4c4080b2386129f4af00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbdb75c8f40744078b4b589f8dd59831",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2c1b7410473548d7bb82cb9cc8ed1c9b",
      "value": 10000
     }
    },
    "0da1d8c74b864e17b162cb09d1585d2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0eb58470bfb142dca33497ee4b647c3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "11aa875433c94ecdb7c9d5cb450fe049": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12dc3cce24824735b97e8789fb01e77a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14c9091cdf304cdfbcf9464f07390c9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1ec6dab0093a4a25b70ddfbbc3064311": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_845b78b379414586b0044c2b3a46036b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2b08bc777e554917a405b5be35eb1da6",
      "value": "â€‡120M/120Mâ€‡[00:01&lt;00:00,â€‡119MB/s]"
     }
    },
    "20b0d76b1bd14de8a9a126ca1e62a13b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0442c7fac7a9436cbb765eb7ae292e14",
      "max": 23940850,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83db8b50d706434b9a993ea611dfc746",
      "value": 23940850
     }
    },
    "231fd1fc180f449ca258d07a7a119b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf1c47bad3154763aba233c890e03ea1",
       "IPY_MODEL_cc84fa50b0474d96b58265c77ae3c756",
       "IPY_MODEL_d3dd0b99423f4593a1d4e9dbded83da1"
      ],
      "layout": "IPY_MODEL_0ce5ccd7162245e5a40e52d9bcb2f8af"
     }
    },
    "23b8f48359c644aca65923ab366acb06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_556fbbbbb3cf45eebf313d4328a2b8a1",
       "IPY_MODEL_68604bf106d04bb888fd5a4e8557ec83",
       "IPY_MODEL_1ec6dab0093a4a25b70ddfbbc3064311"
      ],
      "layout": "IPY_MODEL_879a46a3084b4f8cbd0beab1cabc41db"
     }
    },
    "298787fa51554b21bb29c115fa13bbdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f26c33a7bae344ea9f29bc96bfc325b8",
       "IPY_MODEL_0d88086521df4c4080b2386129f4af00",
       "IPY_MODEL_eb762eb8398042e4914430f61586777d"
      ],
      "layout": "IPY_MODEL_a5cc87c1fb2e4317a64ff9e526f59e8b"
     }
    },
    "2a78c646586743e78e77ac3233c3854e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12dc3cce24824735b97e8789fb01e77a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_00a175706fc348d68578e739da15a75d",
      "value": "plain_text/test-00000-of-00001.parquet:â€‡100%"
     }
    },
    "2b08bc777e554917a405b5be35eb1da6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c0df1a3570d4707854d5a941325a4c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c1b7410473548d7bb82cb9cc8ed1c9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3d73913da63b4400a78b35f017ac3093": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42ce5e7287984db2b0f0a731381193d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f6418b0b6934deeabcf4a9ea1a11497": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6cb0c7246def42778f8508160c308575",
      "max": 50000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5844608062e34fc38d51a843a51bccc0",
      "value": 50000
     }
    },
    "4f7a1c9c81ea4945b9c97635d1542a54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52f358de4cfb4bd99b428847997d80c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d239b76c38004f35a4914525f4a23a4d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_42ce5e7287984db2b0f0a731381193d7",
      "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
     }
    },
    "556fbbbbb3cf45eebf313d4328a2b8a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d73913da63b4400a78b35f017ac3093",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0eb58470bfb142dca33497ee4b647c3b",
      "value": "plain_text/train-00000-of-00001.parquet:â€‡100%"
     }
    },
    "58157bd3d5ed4169a68790118c4db4cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52f358de4cfb4bd99b428847997d80c1",
       "IPY_MODEL_4f6418b0b6934deeabcf4a9ea1a11497",
       "IPY_MODEL_a9d778ca184b494fb97f63a45a7fa358"
      ],
      "layout": "IPY_MODEL_fe53f38c73524d838e3daecf04c9b937"
     }
    },
    "5844608062e34fc38d51a843a51bccc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5a4edba3c5de48e3b27bd8124e291f83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6249e679ace849cb82e4b54cf86f30f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68604bf106d04bb888fd5a4e8557ec83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d614aee36bcc4506b9d8f3818f9b19a0",
      "max": 119705255,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_05e505ed659f4c97b6ddd55f90b162fe",
      "value": 119705255
     }
    },
    "6cb0c7246def42778f8508160c308575": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83db8b50d706434b9a993ea611dfc746": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "845b78b379414586b0044c2b3a46036b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "879a46a3084b4f8cbd0beab1cabc41db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9856d182e3904f6aada33f927af455c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2a78c646586743e78e77ac3233c3854e",
       "IPY_MODEL_20b0d76b1bd14de8a9a126ca1e62a13b",
       "IPY_MODEL_bb1bb0d26315450f86936fa5384169a6"
      ],
      "layout": "IPY_MODEL_c8f6424e46a24146a815356313d2def8"
     }
    },
    "996eca95ad58400a9d85c8426cd584f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fd7cb6a25ac4138bc050555fc6f53b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a069cf545f534ce3b88fea8168d505d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "a5cc87c1fb2e4317a64ff9e526f59e8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8500a8686ce47338490733dc75beb32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9d778ca184b494fb97f63a45a7fa358": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a4edba3c5de48e3b27bd8124e291f83",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6249e679ace849cb82e4b54cf86f30f5",
      "value": "â€‡50000/50000â€‡[00:00&lt;00:00,â€‡94319.33â€‡examples/s]"
     }
    },
    "b7d307f1bebe48dd88481d0ddba87d5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb1bb0d26315450f86936fa5384169a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0da1d8c74b864e17b162cb09d1585d2c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e403cfc1766042f8be395fb393b4ee56",
      "value": "â€‡23.9M/23.9Mâ€‡[00:00&lt;00:00,â€‡80.0MB/s]"
     }
    },
    "bbdb75c8f40744078b4b589f8dd59831": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8f6424e46a24146a815356313d2def8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc84fa50b0474d96b58265c77ae3c756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a069cf545f534ce3b88fea8168d505d5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14c9091cdf304cdfbcf9464f07390c9f",
      "value": 1
     }
    },
    "cf1c47bad3154763aba233c890e03ea1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11aa875433c94ecdb7c9d5cb450fe049",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_996eca95ad58400a9d85c8426cd584f8",
      "value": "README.md:â€‡"
     }
    },
    "d239b76c38004f35a4914525f4a23a4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3dd0b99423f4593a1d4e9dbded83da1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f7a1c9c81ea4945b9c97635d1542a54",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2c0df1a3570d4707854d5a941325a4c0",
      "value": "â€‡5.16k/?â€‡[00:00&lt;00:00,â€‡537kB/s]"
     }
    },
    "d614aee36bcc4506b9d8f3818f9b19a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db859b6bd6834b41b02b7d641d86e823": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e403cfc1766042f8be395fb393b4ee56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb762eb8398042e4914430f61586777d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8500a8686ce47338490733dc75beb32",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b7d307f1bebe48dd88481d0ddba87d5b",
      "value": "â€‡10000/10000â€‡[00:00&lt;00:00,â€‡70007.23â€‡examples/s]"
     }
    },
    "f26c33a7bae344ea9f29bc96bfc325b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fd7cb6a25ac4138bc050555fc6f53b5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_db859b6bd6834b41b02b7d641d86e823",
      "value": "Generatingâ€‡testâ€‡split:â€‡100%"
     }
    },
    "fe53f38c73524d838e3daecf04c9b937": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
