{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "231fd1fc180f449ca258d07a7a119b3a",
      "cf1c47bad3154763aba233c890e03ea1",
      "cc84fa50b0474d96b58265c77ae3c756",
      "d3dd0b99423f4593a1d4e9dbded83da1",
      "0ce5ccd7162245e5a40e52d9bcb2f8af",
      "11aa875433c94ecdb7c9d5cb450fe049",
      "996eca95ad58400a9d85c8426cd584f8",
      "a069cf545f534ce3b88fea8168d505d5",
      "14c9091cdf304cdfbcf9464f07390c9f",
      "4f7a1c9c81ea4945b9c97635d1542a54",
      "2c0df1a3570d4707854d5a941325a4c0",
      "23b8f48359c644aca65923ab366acb06",
      "556fbbbbb3cf45eebf313d4328a2b8a1",
      "68604bf106d04bb888fd5a4e8557ec83",
      "1ec6dab0093a4a25b70ddfbbc3064311",
      "879a46a3084b4f8cbd0beab1cabc41db",
      "3d73913da63b4400a78b35f017ac3093",
      "0eb58470bfb142dca33497ee4b647c3b",
      "d614aee36bcc4506b9d8f3818f9b19a0",
      "05e505ed659f4c97b6ddd55f90b162fe",
      "845b78b379414586b0044c2b3a46036b",
      "2b08bc777e554917a405b5be35eb1da6",
      "9856d182e3904f6aada33f927af455c4",
      "2a78c646586743e78e77ac3233c3854e",
      "20b0d76b1bd14de8a9a126ca1e62a13b",
      "bb1bb0d26315450f86936fa5384169a6",
      "c8f6424e46a24146a815356313d2def8",
      "12dc3cce24824735b97e8789fb01e77a",
      "00a175706fc348d68578e739da15a75d",
      "0442c7fac7a9436cbb765eb7ae292e14",
      "83db8b50d706434b9a993ea611dfc746",
      "0da1d8c74b864e17b162cb09d1585d2c",
      "e403cfc1766042f8be395fb393b4ee56",
      "58157bd3d5ed4169a68790118c4db4cb",
      "52f358de4cfb4bd99b428847997d80c1",
      "4f6418b0b6934deeabcf4a9ea1a11497",
      "a9d778ca184b494fb97f63a45a7fa358",
      "fe53f38c73524d838e3daecf04c9b937",
      "d239b76c38004f35a4914525f4a23a4d",
      "42ce5e7287984db2b0f0a731381193d7",
      "6cb0c7246def42778f8508160c308575",
      "5844608062e34fc38d51a843a51bccc0",
      "5a4edba3c5de48e3b27bd8124e291f83",
      "6249e679ace849cb82e4b54cf86f30f5",
      "298787fa51554b21bb29c115fa13bbdc",
      "f26c33a7bae344ea9f29bc96bfc325b8",
      "0d88086521df4c4080b2386129f4af00",
      "eb762eb8398042e4914430f61586777d",
      "a5cc87c1fb2e4317a64ff9e526f59e8b",
      "9fd7cb6a25ac4138bc050555fc6f53b5",
      "db859b6bd6834b41b02b7d641d86e823",
      "bbdb75c8f40744078b4b589f8dd59831",
      "2c1b7410473548d7bb82cb9cc8ed1c9b",
      "a8500a8686ce47338490733dc75beb32",
      "b7d307f1bebe48dd88481d0ddba87d5b"
     ]
    },
    "id": "RE47xgz5UJ3s",
    "outputId": "376ae5d8-bc56-4607-e901-261e47fb553c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bilal/Research/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Mini-ImageNet Data Module\n",
      "==================================================\n",
      "Setting up Mini-ImageNet datasets...\n",
      "Loading Mini-ImageNet train split from Hugging Face...\n",
      "Loaded 100000 samples\n",
      "Loading Mini-ImageNet valid split from Hugging Face...\n",
      "Loaded 10000 samples\n",
      "Train dataset: 100000 samples\n",
      "Validation dataset: 10000 samples\n",
      "\n",
      "Dataset Information:\n",
      "Train dataset size: 100000\n",
      "Validation dataset size: 10000\n",
      "Number of classes: 100\n",
      "\n",
      "Sample from training set:\n",
      "Image shape: torch.Size([3, 64, 64])\n",
      "Image dtype: torch.float32\n",
      "Label: 0\n",
      "Image min/max values: -2.118 / 2.640\n",
      "\n",
      "Sample batch:\n",
      "Setting up Mini-ImageNet datasets...\n",
      "Loading Mini-ImageNet train split from Hugging Face...\n",
      "Loaded 100000 samples\n",
      "Loading Mini-ImageNet valid split from Hugging Face...\n",
      "Loaded 10000 samples\n",
      "Train dataset: 100000 samples\n",
      "Validation dataset: 10000 samples\n",
      "Batch images shape: torch.Size([16, 3, 64, 64])\n",
      "Batch labels shape: torch.Size([16])\n",
      "First 5 labels: [0, 0, 0, 0, 0]\n",
      "\n",
      "Testing data loaders:\n",
      "Setting up Mini-ImageNet datasets...\n",
      "Loading Mini-ImageNet train split from Hugging Face...\n",
      "Loaded 100000 samples\n",
      "Loading Mini-ImageNet valid split from Hugging Face...\n",
      "Loaded 10000 samples\n",
      "Train dataset: 100000 samples\n",
      "Validation dataset: 10000 samples\n",
      "Train loader batches: 6250\n",
      "Validation loader batches: 625\n",
      "\n",
      "Testing first batch from train loader:\n",
      "Batch shape: torch.Size([16, 3, 64, 64])\n",
      "Labels shape: torch.Size([16])\n",
      "\n",
      "Testing first batch from validation loader:\n",
      "Batch shape: torch.Size([16, 3, 64, 64])\n",
      "Labels shape: torch.Size([16])\n",
      "\n",
      "Mini-ImageNet Data Module test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mini-ImageNet Data Module for Vision Transformer Training\n",
    "Loads Mini-ImageNet dataset from Hugging Face datasets library\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Tuple, Optional, Dict, Any\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MiniImageNetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for Mini-ImageNet from Hugging Face datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str = \"train\",\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "        cache_dir: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Mini-ImageNet dataset\n",
    "\n",
    "        Args:\n",
    "            split: Dataset split - 'train', 'validation', or 'test'\n",
    "            transform: Torchvision transforms to apply\n",
    "            cache_dir: Directory to cache downloaded dataset\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load dataset from Hugging Face\n",
    "        print(f\"Loading Mini-ImageNet {split} split from Hugging Face...\")\n",
    "        self.dataset = load_dataset(\"zh-plus/tiny-imagenet\", split=split, cache_dir=cache_dir)\n",
    "\n",
    "        print(f\"Loaded {len(self.dataset)} samples\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "\n",
    "        Args:\n",
    "            idx: Index of the sample\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (image_tensor, label)\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # Get image and label\n",
    "        image = sample['image']  # PIL Image\n",
    "        label = sample['label']  # int\n",
    "\n",
    "        # Convert grayscale to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Default: convert PIL to tensor\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class MiniImageNetDataModule:\n",
    "    \"\"\"\n",
    "    Data module to handle Mini-ImageNet dataset loading and preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 4,\n",
    "        image_size: int = 256,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        augment_train: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Mini-ImageNet data module\n",
    "\n",
    "        Args:\n",
    "            batch_size: Batch size for data loaders\n",
    "            num_workers: Number of workers for data loading\n",
    "            image_size: Size to resize images (default 256 for Mini-ImageNet)\n",
    "            cache_dir: Directory to cache dataset\n",
    "            augment_train: Whether to apply data augmentation to training set\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.image_size = image_size\n",
    "        self.cache_dir = cache_dir\n",
    "        self.augment_train = augment_train\n",
    "\n",
    "        # Setup transforms\n",
    "        self.setup_transforms()\n",
    "\n",
    "        # Initialize datasets\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def setup_transforms(self):\n",
    "        \"\"\"Setup data transforms for training, validation, and testing\"\"\"\n",
    "\n",
    "        # Base transforms for validation/test\n",
    "        # Using bilinear interpolation (default) for high-quality resizing\n",
    "        base_transforms = [\n",
    "            # transforms.Resize((self.image_size, self.image_size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "\n",
    "        # Training transforms with augmentation\n",
    "        if self.augment_train:\n",
    "            train_transforms = [\n",
    "                # Resize to slightly larger size for random crop\n",
    "                # transforms.Resize(int(self.image_size * 1.15), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                # transforms.RandomCrop(self.image_size),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                # Random erasing for better generalization\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))\n",
    "            ]\n",
    "            self.train_transform = transforms.Compose(train_transforms)\n",
    "        else:\n",
    "            self.train_transform = transforms.Compose(base_transforms)\n",
    "\n",
    "        # Validation/Test transforms (no augmentation)\n",
    "        self.val_transform = transforms.Compose(base_transforms)\n",
    "        self.test_transform = transforms.Compose(base_transforms)\n",
    "\n",
    "    def setup_datasets(self):\n",
    "        \"\"\"Setup train, validation, and test datasets\"\"\"\n",
    "        print(\"Setting up Mini-ImageNet datasets...\")\n",
    "\n",
    "        self.train_dataset = MiniImageNetDataset(\n",
    "            split=\"train\",\n",
    "            transform=self.train_transform,\n",
    "            cache_dir=self.cache_dir\n",
    "        )\n",
    "\n",
    "        self.val_dataset = MiniImageNetDataset(\n",
    "            split=\"valid\",\n",
    "            transform=self.val_transform,\n",
    "            cache_dir=self.cache_dir\n",
    "        )\n",
    "\n",
    "        # self.test_dataset = MiniImageNetDataset(\n",
    "        #     split=\"test\",\n",
    "        #     transform=self.test_transform,\n",
    "        #     cache_dir=self.cache_dir\n",
    "        # )\n",
    "\n",
    "        print(f\"Train dataset: {len(self.train_dataset)} samples\")\n",
    "        print(f\"Validation dataset: {len(self.val_dataset)} samples\")\n",
    "        # print(f\"Test dataset: {len(self.test_dataset)} samples\")\n",
    "\n",
    "    def get_dataloaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Get train, validation, and test data loaders\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (train_loader, val_loader, test_loader)\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None or self.val_dataset is None or self.test_dataset is None:\n",
    "            self.setup_datasets()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True  # For stable batch norm statistics\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # test_loader = DataLoader(\n",
    "        #     self.test_dataset,\n",
    "        #     batch_size=self.batch_size,\n",
    "        #     shuffle=False,\n",
    "        #     num_workers=self.num_workers,\n",
    "        #     pin_memory=True\n",
    "        # )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def get_sample_batch(self, split: str = \"train\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a sample batch for testing\n",
    "\n",
    "        Args:\n",
    "            split: Which split to sample from ('train', 'validation', or 'test')\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (images, labels) tensors\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None or self.val_dataset is None or self.test_dataset is None:\n",
    "            self.setup_datasets()\n",
    "\n",
    "        if split == \"train\":\n",
    "            dataset = self.train_dataset\n",
    "        elif split == \"validation\" or split == \"valid\":\n",
    "            dataset = self.val_dataset\n",
    "        else:\n",
    "            dataset = self.test_dataset\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return next(iter(loader))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the data module\n",
    "    print(\"Testing Mini-ImageNet Data Module\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create data module\n",
    "    data_module = MiniImageNetDataModule(\n",
    "        batch_size=16,\n",
    "        num_workers=2,\n",
    "        image_size=256,\n",
    "        augment_train=True\n",
    "    )\n",
    "\n",
    "    # Setup datasets\n",
    "    data_module.setup_datasets()\n",
    "\n",
    "    # Get basic info\n",
    "    print(f\"\\nDataset Information:\")\n",
    "    print(f\"Train dataset size: {len(data_module.train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(data_module.val_dataset)}\")\n",
    "    # print(f\"Test dataset size: {len(data_module.test_dataset)}\")\n",
    "    print(f\"Number of classes: 100\")\n",
    "\n",
    "    # Get a sample from train dataset\n",
    "    print(f\"\\nSample from training set:\")\n",
    "    image, label = data_module.train_dataset[0]\n",
    "    print(f\"Image shape: {image.shape}\")\n",
    "    print(f\"Image dtype: {image.dtype}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Image min/max values: {image.min():.3f} / {image.max():.3f}\")\n",
    "\n",
    "    # Get a sample batch\n",
    "    print(f\"\\nSample batch:\")\n",
    "    images, labels = data_module.get_sample_batch(\"train\")\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    print(f\"First 5 labels: {labels[:5].tolist()}\")\n",
    "\n",
    "    # Test data loaders\n",
    "    print(f\"\\nTesting data loaders:\")\n",
    "    train_loader, val_loader = data_module.get_dataloaders()\n",
    "    print(f\"Train loader batches: {len(train_loader)}\")\n",
    "    print(f\"Validation loader batches: {len(val_loader)}\")\n",
    "    # print(f\"Test loader batches: {len(test_loader)}\")\n",
    "\n",
    "    # Test a few batches\n",
    "    print(f\"\\nTesting first batch from train loader:\")\n",
    "    batch_images, batch_labels = next(iter(train_loader))\n",
    "    print(f\"Batch shape: {batch_images.shape}\")\n",
    "    print(f\"Labels shape: {batch_labels.shape}\")\n",
    "\n",
    "    print(f\"\\nTesting first batch from validation loader:\")\n",
    "    batch_images, batch_labels = next(iter(val_loader))\n",
    "    print(f\"Batch shape: {batch_images.shape}\")\n",
    "    print(f\"Labels shape: {batch_labels.shape}\")\n",
    "\n",
    "    print(f\"\\nMini-ImageNet Data Module test completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModulatedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention with latent token modulation and learnable aggregation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_latents, num_heads=1, qkv_bias=True, dropout=0.0,\n",
    "                 modulate_v=True, aggregation='max'):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} must be divisible by num_heads {num_heads}\"\n",
    "        assert aggregation in ['mean', 'max', 'learned_weight', 'attention', 'gated'], \\\n",
    "            f\"aggregation must be one of ['mean', 'max', 'learned_weight', 'attention', 'gated'], got {aggregation}\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.modulate_v = modulate_v\n",
    "        self.aggregation = aggregation\n",
    "        self.num_latents = num_latents\n",
    "        \n",
    "        # --- VISUALIZATION STATE ---\n",
    "        self.save_indices = False\n",
    "        self.saved_indices = None\n",
    "        # ---------------------------\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        # self.proj = nn.Linear(dim, dim)\n",
    "        # self.proj_drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def aggregate_tokens(self, tokens):\n",
    "        if self.aggregation == 'mean':\n",
    "            return tokens.mean(dim=2, keepdim=True)\n",
    "        elif self.aggregation == 'max':\n",
    "            max_tokens, indices = tokens.topk(12, dim=3) # hardcoded 12 for imagenet-1k\n",
    "            return max_tokens, indices\n",
    "        elif self.aggregation == 'learned_weight':\n",
    "            weights = self.aggregation_weight(tokens)\n",
    "            weights = F.softmax(weights, dim=2)\n",
    "            weighted = tokens * weights\n",
    "            return weighted.sum(dim=2, keepdim=True)\n",
    "        elif self.aggregation == 'attention':\n",
    "            query = tokens.mean(dim=2, keepdim=True)\n",
    "            query = self.agg_query(query)\n",
    "            scores = (query @ tokens.transpose(-2, -1)) * self.agg_scale\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            aggregated = attn_weights @ tokens\n",
    "            return aggregated\n",
    "        elif self.aggregation == 'gated':\n",
    "            gates = self.gate_net(tokens)\n",
    "            gated = tokens * gates\n",
    "            return gated.mean(dim=2, keepdim=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation: {self.aggregation}\")\n",
    "    \n",
    "    def modulate_cls_token(self, cls_token, other_tokens, same_tokens):\n",
    "        cls_token = cls_token.unsqueeze(3)\n",
    "        other_tokens = other_tokens.unsqueeze(2)\n",
    "        same_tokens = same_tokens.unsqueeze(2)\n",
    "        interaction = cls_token * other_tokens # Qcls X Kx\n",
    "        modulated = interaction + same_tokens  # Qx + (Qcls X Kx)\n",
    "        modulated_cls, indices = self.aggregate_tokens(modulated)\n",
    "        return modulated_cls.squeeze(2), indices.squeeze(2)\n",
    "    \n",
    "    def modulate_v_cls(self, v_cls ,qk_cls, v_full):\n",
    "        v_squared = v_full ** 2\n",
    "        two_v = 2 * v_full\n",
    "        # product of original qcls and kcls are used here, mean of modulated q_cls and k_cls gives slightly better performance\n",
    "        qk_interaction = qk_cls \n",
    "        v_cls_abs = torch.abs(v_cls)\n",
    "        gating = 1 + v_cls_abs \n",
    "        interaction_term = qk_interaction * gating \n",
    "        interaction_term = interaction_term.unsqueeze(3)\n",
    "        v_squared = v_squared.unsqueeze(2)\n",
    "        two_v = two_v.unsqueeze(2)\n",
    "        modulated_v = v_squared + two_v + interaction_term\n",
    "        v_cls_modulated, indices = self.aggregate_tokens(modulated_v)\n",
    "        return v_cls_modulated.squeeze(2), indices.squeeze(2), modulated_v.squeeze(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        L = self.num_latents \n",
    "        q_cls, q_patches = q[:, :, 0:L, :], q[:, :, L:, :]\n",
    "        k_cls, k_patches = k[:, :, 0:L, :], k[:, :, L:, :]\n",
    "        v_cls, v_patches = v[:, :, 0:L, :], v[:, :, L:, :]\n",
    "    \n",
    "        q_cls_modulated, _ = self.modulate_cls_token(q_cls, k_patches, q_patches)\n",
    "        k_cls_modulated, _ = self.modulate_cls_token(k_cls, q_patches, k_patches)\n",
    "        v_cls_modulated, v_indices, modulated_v = self.modulate_v_cls(v_cls, q_cls*k_cls, v_patches)\n",
    "\n",
    "        # --- CAPTURE INDICES (Added for Visualization) ---\n",
    "        if self.save_indices:\n",
    "            # v_indices shape is likely (B, H, 12, D)\n",
    "            self.saved_indices = v_indices.detach().clone()\n",
    "        # -------------------------------------------------\n",
    "\n",
    "        q_cls_modulated = torch.cat([q_cls, q_cls_modulated], dim=2)\n",
    "        k_cls_modulated = torch.cat([k_cls, k_cls_modulated], dim=2)\n",
    "        v_cls_modulated = torch.cat([v_cls, v_cls_modulated], dim=2)\n",
    "    \n",
    "        attn = (q_cls_modulated @ k_cls_modulated.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "    \n",
    "        out = attn @ v_cls_modulated\n",
    "\n",
    "        # --- Reconstructing output to match shapes for scatter ---\n",
    "        out = out.transpose(1, 2).reshape(B, 13, C) \n",
    "        out_cls, out_ = out[:, 0], out[:, 1:]  \n",
    "        \n",
    "        # Create zeros tensor with same dtype as out_ to avoid AMP dtype mismatch\n",
    "        skip_v_zeros = torch.zeros_like(modulated_v.squeeze(1), dtype=out_.dtype)  \n",
    "        \n",
    "        # v_indices.squeeze(1) is (B, 12, D). out_ is (B, 12, C). C=D. \n",
    "        # This scatter maps the Top K features back to their original token positions (dim=1)\n",
    "        skip_v_zeros = skip_v_zeros.scatter_(dim=1, index=v_indices.squeeze(1), src=out_)\n",
    "\n",
    "        v_out = modulated_v.squeeze(1).to(out_.dtype) + skip_v_zeros\n",
    "        v_out = torch.cat([out_cls.unsqueeze(1), v_out], dim=1)\n",
    "        \n",
    "        return v_out\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhFQMKXmYGv2",
    "outputId": "02bc0db2-bbff-4f3c-fd30-96f6398d08e8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vision Transformer Block Module\n",
    "Contains the transformer block with modulated attention, MLP, and layer normalization\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple , List, Union\n",
    "import math\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (Feed-Forward Network) for Vision Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: Optional[int] = None,\n",
    "        out_features: Optional[int] = None,\n",
    "        activation: nn.Module = nn.GELU,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Input feature dimension\n",
    "            hidden_features: Hidden layer dimension (default: 4 * in_features)\n",
    "            out_features: Output feature dimension (default: in_features)\n",
    "            activation: Activation function\n",
    "            dropout: Dropout rate\n",
    "            bias: Whether to use bias in linear layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features * 4\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)\n",
    "        self.act = activation()\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, N, C)\n",
    "        Returns:\n",
    "            Output tensor of shape (B, N, C)\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HybridTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Block that can use either Modulated or Standard Attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "        attn_dropout: float = 0.0,\n",
    "        drop_path: float = 0.0,\n",
    "        activation: nn.Module = nn.GELU,\n",
    "        norm_layer: nn.Module = nn.LayerNorm,\n",
    "        attention_type: str = 'modulated',  # 'modulated' or 'standard'\n",
    "        modulate_v: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Input embedding dimension\n",
    "            num_heads: Number of attention heads\n",
    "            mlp_ratio: Ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias: Whether to add bias to qkv projection\n",
    "            dropout: Dropout rate for MLP\n",
    "            attn_dropout: Dropout rate for attention\n",
    "            drop_path: Stochastic depth rate\n",
    "            activation: Activation function for MLP\n",
    "            norm_layer: Normalization layer\n",
    "            attention_type: Type of attention ('modulated' or 'standard')\n",
    "            modulate_v: Whether to modulate V in modulated attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_type = attention_type\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        \n",
    "        # Select attention mechanism\n",
    "        if attention_type == 'modulated':\n",
    "            self.attn = ModulatedMultiHeadAttention(\n",
    "                dim=dim,\n",
    "                num_latents=1,\n",
    "                num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                dropout=attn_dropout,\n",
    "                modulate_v=modulate_v\n",
    "            )\n",
    "        elif attention_type == 'standard':\n",
    "            self.attn = MultiHeadAttention(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                dropout=attn_dropout\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown attention type: {attention_type}. Use 'modulated' or 'standard'\")\n",
    "        \n",
    "        # MLP\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            activation=activation,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Stochastic depth\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, N, C)\n",
    "        Returns:\n",
    "            Output tensor of shape (B, N, C)\n",
    "        \"\"\"\n",
    "        # Attention block with residual connection\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        # x = self.drop_path(self.attn(self.norm1(x)))\n",
    "        \n",
    "        # MLP block with residual connection\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_attention_weights(self, x):\n",
    "        \"\"\"Get attention weights for visualization\"\"\"\n",
    "        x_norm = self.norm1(x)\n",
    "        return self.attn.get_attention_weights(x_norm) if hasattr(self.attn, 'get_attention_weights') else None\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'attention_type={self.attention_type}, dim={self.dim}, num_heads={self.num_heads}'\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            drop_prob: Probability of dropping a path\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()  # binarize\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={self.drop_prob}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tp2fNJUndIJs",
    "outputId": "9e14b40d-f727-4843-dbf4-f25f9ee87f0f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete Vision Transformer Implementation\n",
    "Includes patch embedding, positional encoding, and full ViT architecture\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "import math\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding\n",
    "    Converts images into patches and linearly embeds them\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_size: Size of input image (assumed square)\n",
    "            patch_size: Size of each patch (assumed square)\n",
    "            in_channels: Number of input channels (3 for RGB)\n",
    "            embed_dim: Embedding dimension\n",
    "            bias: Whether to use bias in projection layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Calculate number of patches\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.grid_size = image_size // patch_size\n",
    "\n",
    "        # Patch embedding using convolution\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "        # Layer normalization (optional, but often helpful)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            Patch embeddings of shape (B, num_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Verify input dimensions\n",
    "        assert H == self.image_size and W == self.image_size, \\\n",
    "            f\"Input image size ({H}, {W}) doesn't match model ({self.image_size}, {self.image_size})\"\n",
    "        assert C == self.in_channels, \\\n",
    "            f\"Input channels ({C}) doesn't match model ({self.in_channels})\"\n",
    "\n",
    "        # Extract patches and flatten\n",
    "        x = self.proj(x)  # (B, embed_dim, grid_size, grid_size)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "\n",
    "        # Apply layer normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_patch_info(self):\n",
    "        \"\"\"Return patch embedding information\"\"\"\n",
    "        return {\n",
    "            'num_patches': self.num_patches,\n",
    "            'grid_size': self.grid_size,\n",
    "            'patch_size': self.patch_size,\n",
    "            'embed_dim': self.embed_dim\n",
    "        }\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable positional embeddings for Vision Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_patches: int, embed_dim: int, dropout: float = 0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_patches: Number of patches in the image\n",
    "            embed_dim: Embedding dimension\n",
    "            dropout: Dropout rate for positional embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Learnable positional embeddings (including CLS token)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim) * 0.02)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Patch embeddings of shape (B, num_patches + 1, embed_dim)\n",
    "        Returns:\n",
    "            Embeddings with positional encoding of same shape\n",
    "        \"\"\"\n",
    "        return self.dropout(x + self.pos_embed)\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Classification head for Vision Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_classes: int,\n",
    "        dropout: float = 0.0,\n",
    "        use_norm: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Input embedding dimension\n",
    "            num_classes: Number of output classes\n",
    "            dropout: Dropout rate before final linear layer\n",
    "            use_norm: Whether to apply layer norm before classification\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim) if use_norm else nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize with smaller weights for better training stability\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        if self.head.bias is not None:\n",
    "            nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: CLS token features of shape (B, embed_dim)\n",
    "        Returns:\n",
    "            Class logits of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class HybridVisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer with Hybrid Attention Architecture\n",
    "    Supports mixing modulated and standard attention layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        num_classes: int = 1000,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 1,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "        attn_dropout: float = 0.0,\n",
    "        drop_path_rate: float = 0.1,\n",
    "        attention_pattern: Union[str, List[str]] = 'modulated',\n",
    "        modulate_v: bool = False,\n",
    "        norm_layer: nn.Module = nn.LayerNorm,\n",
    "        activation: nn.Module = nn.GELU\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_size: Input image size\n",
    "            patch_size: Patch size for patch embedding\n",
    "            in_channels: Number of input channels\n",
    "            num_classes: Number of classification classes\n",
    "            embed_dim: Embedding dimension\n",
    "            depth: Number of transformer blocks\n",
    "            num_heads: Number of attention heads\n",
    "            mlp_ratio: Ratio of MLP hidden dimension to embedding dimension\n",
    "            qkv_bias: Whether to add bias to qkv projection\n",
    "            dropout: Dropout rate\n",
    "            attn_dropout: Attention dropout rate\n",
    "            drop_path_rate: Stochastic depth rate\n",
    "            attention_pattern: Pattern of attention types. Options:\n",
    "                - 'modulated': All layers use modulated attention\n",
    "                - 'standard': All layers use standard attention\n",
    "                - 'alternating': Alternates between modulated and standard\n",
    "                - 'early_modulated': First half modulated, second half standard\n",
    "                - 'late_modulated': First half standard, second half modulated\n",
    "                - List[str]: Explicit list of attention types per layer\n",
    "            modulate_v: Whether to modulate V in modulated attention layers\n",
    "            norm_layer: Normalization layer\n",
    "            activation: Activation function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        \n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embed = PositionalEmbedding(\n",
    "            num_patches=num_patches,\n",
    "            embed_dim=embed_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Determine attention pattern for each layer\n",
    "        attention_types = self._parse_attention_pattern(attention_pattern, depth)\n",
    "        self.attention_types = attention_types\n",
    "        \n",
    "        # Stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        \n",
    "        # Build transformer blocks with hybrid attention\n",
    "        self.blocks = nn.ModuleList([\n",
    "            HybridTransformerBlock(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                dropout=dropout,\n",
    "                attn_dropout=attn_dropout,\n",
    "                drop_path=dpr[i],\n",
    "                activation=activation,\n",
    "                norm_layer=norm_layer,\n",
    "                attention_type=attention_types[i],\n",
    "                modulate_v=modulate_v\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = ClassificationHead(\n",
    "            embed_dim=embed_dim,\n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _parse_attention_pattern(self, pattern: Union[str, List[str]], depth: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Parse attention pattern into a list of attention types per layer\n",
    "        \n",
    "        Args:\n",
    "            pattern: Attention pattern specification\n",
    "            depth: Number of layers\n",
    "        \n",
    "        Returns:\n",
    "            List of attention types for each layer\n",
    "        \"\"\"\n",
    "        if isinstance(pattern, list):\n",
    "            assert len(pattern) == depth, f\"Pattern list length {len(pattern)} must match depth {depth}\"\n",
    "            return pattern\n",
    "        \n",
    "        if pattern == 'modulated':\n",
    "            return ['modulated'] * depth\n",
    "        elif pattern == 'standard':\n",
    "            return ['standard'] * depth\n",
    "        elif pattern == 'alternating':\n",
    "            return ['modulated' if i % 2 == 0 else 'standard' for i in range(depth)]\n",
    "        elif pattern == 'early_modulated':\n",
    "            mid = depth // 2\n",
    "            return ['modulated'] * mid + ['standard'] * (depth - mid)\n",
    "        elif pattern == 'late_modulated':\n",
    "            mid = depth // 2\n",
    "            return ['standard'] * mid + ['modulated'] * (depth - mid)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown attention pattern: {pattern}\")\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.ones_(m.weight)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward_features(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through patch embedding, positional encoding, and transformer blocks\n",
    "        \n",
    "        Args:\n",
    "            x: Input images of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            Feature tokens of shape (B, num_patches + 1, embed_dim)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_embed(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input images of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            Class logits of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.forward_features(x)\n",
    "        cls_token = x[:, 0]\n",
    "        logits = self.head(cls_token)\n",
    "        return logits\n",
    "    \n",
    "    def get_attention_maps(self, x, block_idx: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Extract attention maps for visualization\n",
    "        \n",
    "        Args:\n",
    "            x: Input images\n",
    "            block_idx: Which block to extract attention from (None for all)\n",
    "        Returns:\n",
    "            Attention maps with layer information\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = self.pos_embed(x)\n",
    "        \n",
    "        attention_maps = []\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if block_idx is None or i == block_idx:\n",
    "                attn_info = block.get_attention_weights(x)\n",
    "                if attn_info is not None:\n",
    "                    attention_maps.append({\n",
    "                        'layer': i,\n",
    "                        'attention_type': self.attention_types[i],\n",
    "                        'weights': attn_info[0],\n",
    "                        'modulation_info': attn_info[1] if len(attn_info) > 1 else None\n",
    "                    })\n",
    "            x = block(x)\n",
    "        \n",
    "        return attention_maps if len(attention_maps) > 1 else attention_maps[0] if attention_maps else None\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get comprehensive model information including attention pattern\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Count attention types\n",
    "        modulated_count = sum(1 for t in self.attention_types if t == 'modulated')\n",
    "        standard_count = sum(1 for t in self.attention_types if t == 'standard')\n",
    "        \n",
    "        return {\n",
    "            'image_size': self.image_size,\n",
    "            'patch_size': self.patch_size,\n",
    "            'num_patches': self.patch_embed.num_patches,\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'depth': self.depth,\n",
    "            'num_classes': self.num_classes,\n",
    "            'attention_pattern': self.attention_types,\n",
    "            'modulated_layers': modulated_count,\n",
    "            'standard_layers': standard_count,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params,\n",
    "            'model_size_mb': total_params * 4 / 1024**2\n",
    "        }\n",
    "    \n",
    "    def print_architecture(self):\n",
    "        \"\"\"Print a visual representation of the architecture\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Hybrid Vision Transformer Architecture\")\n",
    "        print(\"=\" * 70)\n",
    "        info = self.get_model_info()\n",
    "        print(f\"Image Size: {info['image_size']}x{info['image_size']}\")\n",
    "        print(f\"Patches: {info['num_patches']} ({self.patch_embed.grid_size}x{self.patch_embed.grid_size})\")\n",
    "        print(f\"Embedding Dim: {info['embed_dim']}\")\n",
    "        print(f\"Total Layers: {info['depth']}\")\n",
    "        print(f\"  - Modulated Attention: {info['modulated_layers']}\")\n",
    "        print(f\"  - Standard Attention: {info['standard_layers']}\")\n",
    "        print(f\"Parameters: {info['total_params']:,} ({info['model_size_mb']:.2f} MB)\")\n",
    "        print(\"\\nLayer-by-Layer Attention Types:\")\n",
    "        print(\"-\" * 70)\n",
    "        for i, attn_type in enumerate(self.attention_types):\n",
    "            symbol = \"\" if attn_type == \"modulated\" else \"\"\n",
    "            print(f\"  Layer {i:2d}: {symbol} {attn_type.capitalize()}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def create_hybrid_vit(\n",
    "    model_size: str = 'tiny',\n",
    "    num_classes: int = 10,\n",
    "    image_size: int = 32,\n",
    "    attention_pattern: Union[str, List[str]] = 'modulated',\n",
    "    modulate_v: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Factory function to create Hybrid Vision Transformer models\n",
    "    \n",
    "    Args:\n",
    "        model_size: Size of the model ('tiny', 'small', 'base', 'large')\n",
    "        num_classes: Number of classes for classification\n",
    "        image_size: Input image size\n",
    "        attention_pattern: Pattern of attention types\n",
    "        modulate_v: Whether to modulate V in modulated layers\n",
    "    \n",
    "    Returns:\n",
    "        Hybrid Vision Transformer model\n",
    "    \"\"\"\n",
    "    configs = {\n",
    "        'tiny': {\n",
    "            'embed_dim': 192,\n",
    "            'depth': 12,\n",
    "            'num_heads': 3,\n",
    "            'patch_size': 16 if image_size >= 224 else 4,\n",
    "            'mlp_ratio': 4.0,\n",
    "            'dropout': 0.0,\n",
    "            'drop_path_rate': 0.1\n",
    "        },\n",
    "        'small': {\n",
    "            'embed_dim': 384,\n",
    "            'depth': 2,\n",
    "            'num_heads': 1,\n",
    "            'patch_size': 8,\n",
    "            'mlp_ratio': 4.0,\n",
    "            'dropout': 0.1,\n",
    "            'drop_path_rate': 0.1\n",
    "        },\n",
    "        'base': {\n",
    "            'embed_dim': 768,\n",
    "            'depth': 12,\n",
    "            'num_heads': 12,\n",
    "            'patch_size': 16 if image_size >= 224 else 4,\n",
    "            'mlp_ratio': 4.0,\n",
    "            'dropout': 0.1,\n",
    "            'drop_path_rate': 0.1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    assert model_size in configs, f\"Model size {model_size} not supported\"\n",
    "    config = configs[model_size]\n",
    "    \n",
    "    return HybridVisionTransformer(\n",
    "        image_size=image_size,\n",
    "        patch_size=config['patch_size'],\n",
    "        num_classes=num_classes,\n",
    "        embed_dim=config['embed_dim'],\n",
    "        depth=config['depth'],\n",
    "        num_heads=config['num_heads'],\n",
    "        mlp_ratio=config['mlp_ratio'],\n",
    "        dropout=config['dropout'],\n",
    "        drop_path_rate=config['drop_path_rate'],\n",
    "        attention_pattern=attention_pattern,\n",
    "        modulate_v=modulate_v\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# --- Weights & Biases setup ---\n",
    "wandb.login(key=\"\")  # add key here\n",
    "\n",
    "training_config = {\n",
    "            'embed_dim': 384,\n",
    "            'depth': 2,\n",
    "            'head': 1,\n",
    "            'patch_size': 8,\n",
    "            'mlp_ratio': 4.0,\n",
    "            'dropout': 0.1,\n",
    "            'drop_path_rate': 0.1\n",
    "        }\n",
    "\n",
    "wandb.init(\n",
    "    project=\"\",       # W&B project name\n",
    "    name=\"\",      # optional descriptive run name\n",
    "    config=training_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VwbQBV_DgAP6",
    "outputId": "82d8f07f-e079-44be-d276-e91bd825149b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniImageNetData Vision Transformer Training\n",
      "============================================================\n",
      "Setting up data...\n",
      "Setting up Mini-ImageNet datasets...\n",
      "Loading Mini-ImageNet train split from Hugging Face...\n",
      "Loaded 100000 samples\n",
      "Loading Mini-ImageNet valid split from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 samples\n",
      "Train dataset: 100000 samples\n",
      "Validation dataset: 10000 samples\n",
      "Train batches: 781\n",
      "Test batches: 79\n",
      "\n",
      "Creating Vision Transformer model...\n",
      "Model created:\n",
      "  Parameters: 3,726,920\n",
      "  Model size: 14.22 MB\n",
      "  Patches: 64\n",
      "\n",
      "Training configuration:\n",
      "  num_epochs: 30\n",
      "  learning_rate: 0.0003\n",
      "  weight_decay: 0.05\n",
      "  optimizer_name: adamw\n",
      "  scheduler_name: cosine\n",
      "  device: cuda\n",
      "  save_path: ./vit_checkpoints\n",
      "  print_freq: 20\n",
      "  save_freq: 5\n",
      "\n",
      " Starting training...\n",
      "Training on device: cuda\n",
      "Model parameters: 3,726,920\n",
      " W&B: Watching model gradients...\n",
      "\n",
      "Starting training for 30 epochs...\n",
      "================================================================================\n",
      "\n",
      "Epoch [  1/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 5.3966 | Acc: 0.0000 | LR: 0.000300 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 5.2771 | Acc: 0.0156 | LR: 0.000300 | Time: 2.1s\n",
      "  Batch [ 40/781] | Loss: 5.2701 | Acc: 0.0000 | LR: 0.000300 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 5.1093 | Acc: 0.0156 | LR: 0.000300 | Time: 5.2s\n",
      "  Batch [ 80/781] | Loss: 5.0695 | Acc: 0.0312 | LR: 0.000300 | Time: 7.0s\n",
      "  Batch [100/781] | Loss: 5.0195 | Acc: 0.0234 | LR: 0.000300 | Time: 8.6s\n",
      "  Batch [120/781] | Loss: 4.8886 | Acc: 0.0078 | LR: 0.000300 | Time: 10.3s\n",
      "  Batch [140/781] | Loss: 4.9951 | Acc: 0.0234 | LR: 0.000300 | Time: 11.9s\n",
      "  Batch [160/781] | Loss: 4.8671 | Acc: 0.0234 | LR: 0.000300 | Time: 13.5s\n",
      "  Batch [180/781] | Loss: 4.8838 | Acc: 0.0234 | LR: 0.000300 | Time: 15.1s\n",
      "  Batch [200/781] | Loss: 4.8271 | Acc: 0.0391 | LR: 0.000300 | Time: 16.6s\n",
      "  Batch [220/781] | Loss: 4.8682 | Acc: 0.0234 | LR: 0.000300 | Time: 18.3s\n",
      "  Batch [240/781] | Loss: 4.8142 | Acc: 0.0391 | LR: 0.000300 | Time: 20.0s\n",
      "  Batch [260/781] | Loss: 4.9747 | Acc: 0.0234 | LR: 0.000300 | Time: 21.8s\n",
      "  Batch [280/781] | Loss: 4.7809 | Acc: 0.0312 | LR: 0.000300 | Time: 23.5s\n",
      "  Batch [300/781] | Loss: 4.7137 | Acc: 0.0469 | LR: 0.000300 | Time: 25.1s\n",
      "  Batch [320/781] | Loss: 4.7664 | Acc: 0.0469 | LR: 0.000300 | Time: 26.7s\n",
      "  Batch [340/781] | Loss: 4.8255 | Acc: 0.0234 | LR: 0.000300 | Time: 28.4s\n",
      "  Batch [360/781] | Loss: 4.7277 | Acc: 0.0703 | LR: 0.000300 | Time: 30.0s\n",
      "  Batch [380/781] | Loss: 4.7650 | Acc: 0.0391 | LR: 0.000300 | Time: 31.7s\n",
      "  Batch [400/781] | Loss: 4.7484 | Acc: 0.0547 | LR: 0.000300 | Time: 33.3s\n",
      "  Batch [420/781] | Loss: 4.5962 | Acc: 0.0469 | LR: 0.000300 | Time: 35.0s\n",
      "  Batch [440/781] | Loss: 4.5161 | Acc: 0.1172 | LR: 0.000300 | Time: 36.5s\n",
      "  Batch [460/781] | Loss: 4.7035 | Acc: 0.0547 | LR: 0.000300 | Time: 38.3s\n",
      "  Batch [480/781] | Loss: 4.7046 | Acc: 0.0547 | LR: 0.000300 | Time: 39.8s\n",
      "  Batch [500/781] | Loss: 4.7246 | Acc: 0.0469 | LR: 0.000300 | Time: 41.4s\n",
      "  Batch [520/781] | Loss: 4.6526 | Acc: 0.0859 | LR: 0.000300 | Time: 43.1s\n",
      "  Batch [540/781] | Loss: 4.7537 | Acc: 0.0391 | LR: 0.000300 | Time: 44.7s\n",
      "  Batch [560/781] | Loss: 4.4833 | Acc: 0.0469 | LR: 0.000300 | Time: 46.3s\n",
      "  Batch [580/781] | Loss: 4.4770 | Acc: 0.0391 | LR: 0.000300 | Time: 48.2s\n",
      "  Batch [600/781] | Loss: 4.6380 | Acc: 0.0312 | LR: 0.000300 | Time: 49.8s\n",
      "  Batch [620/781] | Loss: 4.5192 | Acc: 0.0859 | LR: 0.000300 | Time: 51.6s\n",
      "  Batch [640/781] | Loss: 4.7903 | Acc: 0.0469 | LR: 0.000300 | Time: 53.1s\n",
      "  Batch [660/781] | Loss: 4.3900 | Acc: 0.1094 | LR: 0.000300 | Time: 54.7s\n",
      "  Batch [680/781] | Loss: 4.6734 | Acc: 0.0703 | LR: 0.000300 | Time: 56.3s\n",
      "  Batch [700/781] | Loss: 4.3112 | Acc: 0.1172 | LR: 0.000300 | Time: 57.9s\n",
      "  Batch [720/781] | Loss: 4.4001 | Acc: 0.0547 | LR: 0.000300 | Time: 59.6s\n",
      "  Batch [740/781] | Loss: 4.3630 | Acc: 0.0859 | LR: 0.000300 | Time: 61.0s\n",
      "  Batch [760/781] | Loss: 4.5059 | Acc: 0.1094 | LR: 0.000300 | Time: 62.8s\n",
      "  Batch [780/781] | Loss: 4.4512 | Acc: 0.0859 | LR: 0.000300 | Time: 64.4s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 4.7619 | Train Acc: 0.0492\n",
      "    Val Loss:   4.3294 | Val Acc:   0.0910\n",
      "    LR: 0.000299 | Time: 67.3s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.0910 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [  2/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 4.4693 | Acc: 0.0859 | LR: 0.000299 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 4.3509 | Acc: 0.0625 | LR: 0.000299 | Time: 2.1s\n",
      "  Batch [ 40/781] | Loss: 4.5901 | Acc: 0.0391 | LR: 0.000299 | Time: 3.8s\n",
      "  Batch [ 60/781] | Loss: 4.3504 | Acc: 0.1250 | LR: 0.000299 | Time: 5.5s\n",
      "  Batch [ 80/781] | Loss: 4.2602 | Acc: 0.0703 | LR: 0.000299 | Time: 7.2s\n",
      "  Batch [100/781] | Loss: 4.4749 | Acc: 0.0781 | LR: 0.000299 | Time: 8.8s\n",
      "  Batch [120/781] | Loss: 4.3423 | Acc: 0.0938 | LR: 0.000299 | Time: 10.4s\n",
      "  Batch [140/781] | Loss: 4.1753 | Acc: 0.0938 | LR: 0.000299 | Time: 12.1s\n",
      "  Batch [160/781] | Loss: 4.4613 | Acc: 0.0938 | LR: 0.000299 | Time: 13.7s\n",
      "  Batch [180/781] | Loss: 4.3875 | Acc: 0.0859 | LR: 0.000299 | Time: 15.4s\n",
      "  Batch [200/781] | Loss: 4.3315 | Acc: 0.1250 | LR: 0.000299 | Time: 17.0s\n",
      "  Batch [220/781] | Loss: 4.3045 | Acc: 0.0781 | LR: 0.000299 | Time: 18.8s\n",
      "  Batch [240/781] | Loss: 4.1496 | Acc: 0.1641 | LR: 0.000299 | Time: 20.4s\n",
      "  Batch [260/781] | Loss: 4.4021 | Acc: 0.0938 | LR: 0.000299 | Time: 22.0s\n",
      "  Batch [280/781] | Loss: 4.5587 | Acc: 0.0781 | LR: 0.000299 | Time: 23.4s\n",
      "  Batch [300/781] | Loss: 4.2323 | Acc: 0.1797 | LR: 0.000299 | Time: 25.2s\n",
      "  Batch [320/781] | Loss: 4.2756 | Acc: 0.0625 | LR: 0.000299 | Time: 27.0s\n",
      "  Batch [340/781] | Loss: 4.2994 | Acc: 0.0859 | LR: 0.000299 | Time: 28.6s\n",
      "  Batch [360/781] | Loss: 4.2643 | Acc: 0.1016 | LR: 0.000299 | Time: 30.3s\n",
      "  Batch [380/781] | Loss: 4.0455 | Acc: 0.1016 | LR: 0.000299 | Time: 31.8s\n",
      "  Batch [400/781] | Loss: 4.1751 | Acc: 0.0859 | LR: 0.000299 | Time: 33.4s\n",
      "  Batch [420/781] | Loss: 4.1735 | Acc: 0.1250 | LR: 0.000299 | Time: 35.0s\n",
      "  Batch [440/781] | Loss: 4.2099 | Acc: 0.1328 | LR: 0.000299 | Time: 36.6s\n",
      "  Batch [460/781] | Loss: 4.1891 | Acc: 0.0938 | LR: 0.000299 | Time: 38.2s\n",
      "  Batch [480/781] | Loss: 4.1528 | Acc: 0.1328 | LR: 0.000299 | Time: 39.7s\n",
      "  Batch [500/781] | Loss: 4.1807 | Acc: 0.0781 | LR: 0.000299 | Time: 41.4s\n",
      "  Batch [520/781] | Loss: 4.0802 | Acc: 0.1406 | LR: 0.000299 | Time: 43.0s\n",
      "  Batch [540/781] | Loss: 4.3661 | Acc: 0.0859 | LR: 0.000299 | Time: 44.9s\n",
      "  Batch [560/781] | Loss: 4.4404 | Acc: 0.1250 | LR: 0.000299 | Time: 46.4s\n",
      "  Batch [580/781] | Loss: 4.2751 | Acc: 0.0547 | LR: 0.000299 | Time: 48.1s\n",
      "  Batch [600/781] | Loss: 4.3788 | Acc: 0.1250 | LR: 0.000299 | Time: 49.7s\n",
      "  Batch [620/781] | Loss: 4.0583 | Acc: 0.1172 | LR: 0.000299 | Time: 51.4s\n",
      "  Batch [640/781] | Loss: 4.0345 | Acc: 0.1250 | LR: 0.000299 | Time: 53.1s\n",
      "  Batch [660/781] | Loss: 4.1790 | Acc: 0.1250 | LR: 0.000299 | Time: 54.8s\n",
      "  Batch [680/781] | Loss: 4.3011 | Acc: 0.1094 | LR: 0.000299 | Time: 56.4s\n",
      "  Batch [700/781] | Loss: 4.3594 | Acc: 0.1016 | LR: 0.000299 | Time: 57.9s\n",
      "  Batch [720/781] | Loss: 4.3479 | Acc: 0.1094 | LR: 0.000299 | Time: 59.8s\n",
      "  Batch [740/781] | Loss: 4.2756 | Acc: 0.0859 | LR: 0.000299 | Time: 61.4s\n",
      "  Batch [760/781] | Loss: 4.0363 | Acc: 0.1328 | LR: 0.000299 | Time: 63.1s\n",
      "  Batch [780/781] | Loss: 4.3611 | Acc: 0.1250 | LR: 0.000299 | Time: 64.5s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 4.2716 | Train Acc: 0.1020\n",
      "    Val Loss:   3.9690 | Val Acc:   0.1388\n",
      "    LR: 0.000297 | Time: 67.2s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.1388 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [  3/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.9523 | Acc: 0.1562 | LR: 0.000297 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 4.1334 | Acc: 0.0781 | LR: 0.000297 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 4.2505 | Acc: 0.0938 | LR: 0.000297 | Time: 3.5s\n",
      "  Batch [ 60/781] | Loss: 4.0011 | Acc: 0.1250 | LR: 0.000297 | Time: 5.1s\n",
      "  Batch [ 80/781] | Loss: 3.8871 | Acc: 0.2031 | LR: 0.000297 | Time: 6.8s\n",
      "  Batch [100/781] | Loss: 4.1257 | Acc: 0.1484 | LR: 0.000297 | Time: 8.5s\n",
      "  Batch [120/781] | Loss: 4.0426 | Acc: 0.1328 | LR: 0.000297 | Time: 10.1s\n",
      "  Batch [140/781] | Loss: 4.1582 | Acc: 0.1172 | LR: 0.000297 | Time: 11.7s\n",
      "  Batch [160/781] | Loss: 3.8802 | Acc: 0.1328 | LR: 0.000297 | Time: 13.4s\n",
      "  Batch [180/781] | Loss: 3.7551 | Acc: 0.1797 | LR: 0.000297 | Time: 15.0s\n",
      "  Batch [200/781] | Loss: 4.1412 | Acc: 0.1406 | LR: 0.000297 | Time: 16.6s\n",
      "  Batch [220/781] | Loss: 4.1678 | Acc: 0.1719 | LR: 0.000297 | Time: 18.2s\n",
      "  Batch [240/781] | Loss: 3.8398 | Acc: 0.1328 | LR: 0.000297 | Time: 19.7s\n",
      "  Batch [260/781] | Loss: 4.1539 | Acc: 0.0938 | LR: 0.000297 | Time: 21.4s\n",
      "  Batch [280/781] | Loss: 4.0337 | Acc: 0.1016 | LR: 0.000297 | Time: 23.0s\n",
      "  Batch [300/781] | Loss: 3.9682 | Acc: 0.1562 | LR: 0.000297 | Time: 24.6s\n",
      "  Batch [320/781] | Loss: 3.9138 | Acc: 0.1016 | LR: 0.000297 | Time: 26.2s\n",
      "  Batch [340/781] | Loss: 4.2644 | Acc: 0.1094 | LR: 0.000297 | Time: 27.8s\n",
      "  Batch [360/781] | Loss: 4.1386 | Acc: 0.1562 | LR: 0.000297 | Time: 29.4s\n",
      "  Batch [380/781] | Loss: 3.9425 | Acc: 0.1406 | LR: 0.000297 | Time: 31.1s\n",
      "  Batch [400/781] | Loss: 4.1501 | Acc: 0.1328 | LR: 0.000297 | Time: 32.9s\n",
      "  Batch [420/781] | Loss: 3.9698 | Acc: 0.0859 | LR: 0.000297 | Time: 34.6s\n",
      "  Batch [440/781] | Loss: 4.1717 | Acc: 0.1484 | LR: 0.000297 | Time: 36.3s\n",
      "  Batch [460/781] | Loss: 4.0799 | Acc: 0.1172 | LR: 0.000297 | Time: 37.9s\n",
      "  Batch [480/781] | Loss: 3.8820 | Acc: 0.1641 | LR: 0.000297 | Time: 39.5s\n",
      "  Batch [500/781] | Loss: 4.0701 | Acc: 0.1406 | LR: 0.000297 | Time: 41.2s\n",
      "  Batch [520/781] | Loss: 3.7478 | Acc: 0.1641 | LR: 0.000297 | Time: 42.7s\n",
      "  Batch [540/781] | Loss: 3.9916 | Acc: 0.1250 | LR: 0.000297 | Time: 44.4s\n",
      "  Batch [560/781] | Loss: 4.1609 | Acc: 0.0938 | LR: 0.000297 | Time: 46.0s\n",
      "  Batch [580/781] | Loss: 4.0320 | Acc: 0.1484 | LR: 0.000297 | Time: 47.6s\n",
      "  Batch [600/781] | Loss: 3.8341 | Acc: 0.1719 | LR: 0.000297 | Time: 49.3s\n",
      "  Batch [620/781] | Loss: 3.9102 | Acc: 0.1797 | LR: 0.000297 | Time: 50.9s\n",
      "  Batch [640/781] | Loss: 3.9425 | Acc: 0.1094 | LR: 0.000297 | Time: 52.5s\n",
      "  Batch [660/781] | Loss: 4.0782 | Acc: 0.1328 | LR: 0.000297 | Time: 54.1s\n",
      "  Batch [680/781] | Loss: 3.5527 | Acc: 0.2188 | LR: 0.000297 | Time: 55.6s\n",
      "  Batch [700/781] | Loss: 3.7122 | Acc: 0.1797 | LR: 0.000297 | Time: 57.4s\n",
      "  Batch [720/781] | Loss: 3.9779 | Acc: 0.1484 | LR: 0.000297 | Time: 59.2s\n",
      "  Batch [740/781] | Loss: 4.0044 | Acc: 0.1719 | LR: 0.000297 | Time: 60.9s\n",
      "  Batch [760/781] | Loss: 4.0135 | Acc: 0.1328 | LR: 0.000297 | Time: 62.5s\n",
      "  Batch [780/781] | Loss: 3.9652 | Acc: 0.1016 | LR: 0.000297 | Time: 64.1s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 3.9921 | Train Acc: 0.1397\n",
      "    Val Loss:   3.6977 | Val Acc:   0.1787\n",
      "    LR: 0.000293 | Time: 66.9s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.1787 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [  4/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.7276 | Acc: 0.1641 | LR: 0.000293 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 3.8070 | Acc: 0.2109 | LR: 0.000293 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 3.7869 | Acc: 0.1250 | LR: 0.000293 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 3.9667 | Acc: 0.1250 | LR: 0.000293 | Time: 5.1s\n",
      "  Batch [ 80/781] | Loss: 4.1309 | Acc: 0.1016 | LR: 0.000293 | Time: 6.8s\n",
      "  Batch [100/781] | Loss: 3.6256 | Acc: 0.1875 | LR: 0.000293 | Time: 8.5s\n",
      "  Batch [120/781] | Loss: 3.8411 | Acc: 0.1484 | LR: 0.000293 | Time: 10.1s\n",
      "  Batch [140/781] | Loss: 3.9573 | Acc: 0.1328 | LR: 0.000293 | Time: 11.8s\n",
      "  Batch [160/781] | Loss: 3.7236 | Acc: 0.1562 | LR: 0.000293 | Time: 13.5s\n",
      "  Batch [180/781] | Loss: 3.9405 | Acc: 0.1484 | LR: 0.000293 | Time: 15.0s\n",
      "  Batch [200/781] | Loss: 3.8207 | Acc: 0.1562 | LR: 0.000293 | Time: 16.9s\n",
      "  Batch [220/781] | Loss: 3.8751 | Acc: 0.1328 | LR: 0.000293 | Time: 18.7s\n",
      "  Batch [240/781] | Loss: 3.8997 | Acc: 0.1094 | LR: 0.000293 | Time: 20.3s\n",
      "  Batch [260/781] | Loss: 3.7393 | Acc: 0.2344 | LR: 0.000293 | Time: 21.9s\n",
      "  Batch [280/781] | Loss: 3.7083 | Acc: 0.1875 | LR: 0.000293 | Time: 23.5s\n",
      "  Batch [300/781] | Loss: 3.9680 | Acc: 0.1406 | LR: 0.000293 | Time: 25.0s\n",
      "  Batch [320/781] | Loss: 3.6863 | Acc: 0.1953 | LR: 0.000293 | Time: 26.7s\n",
      "  Batch [340/781] | Loss: 3.9386 | Acc: 0.1406 | LR: 0.000293 | Time: 28.2s\n",
      "  Batch [360/781] | Loss: 3.9908 | Acc: 0.1562 | LR: 0.000293 | Time: 29.8s\n",
      "  Batch [380/781] | Loss: 3.8343 | Acc: 0.1953 | LR: 0.000293 | Time: 31.5s\n",
      "  Batch [400/781] | Loss: 3.7394 | Acc: 0.1875 | LR: 0.000293 | Time: 33.1s\n",
      "  Batch [420/781] | Loss: 3.8945 | Acc: 0.1875 | LR: 0.000293 | Time: 34.7s\n",
      "  Batch [440/781] | Loss: 3.7465 | Acc: 0.1797 | LR: 0.000293 | Time: 36.4s\n",
      "  Batch [460/781] | Loss: 3.7797 | Acc: 0.1406 | LR: 0.000293 | Time: 38.0s\n",
      "  Batch [480/781] | Loss: 3.6887 | Acc: 0.1562 | LR: 0.000293 | Time: 39.5s\n",
      "  Batch [500/781] | Loss: 3.7017 | Acc: 0.2031 | LR: 0.000293 | Time: 41.2s\n",
      "  Batch [520/781] | Loss: 3.7192 | Acc: 0.1797 | LR: 0.000293 | Time: 42.8s\n",
      "  Batch [540/781] | Loss: 3.8886 | Acc: 0.1250 | LR: 0.000293 | Time: 44.5s\n",
      "  Batch [560/781] | Loss: 3.7250 | Acc: 0.1484 | LR: 0.000293 | Time: 46.0s\n",
      "  Batch [580/781] | Loss: 3.9325 | Acc: 0.1797 | LR: 0.000293 | Time: 47.6s\n",
      "  Batch [600/781] | Loss: 3.5388 | Acc: 0.2109 | LR: 0.000293 | Time: 49.2s\n",
      "  Batch [620/781] | Loss: 3.9227 | Acc: 0.1641 | LR: 0.000293 | Time: 50.8s\n",
      "  Batch [640/781] | Loss: 3.7302 | Acc: 0.1875 | LR: 0.000293 | Time: 52.3s\n",
      "  Batch [660/781] | Loss: 3.6269 | Acc: 0.2188 | LR: 0.000293 | Time: 53.9s\n",
      "  Batch [680/781] | Loss: 3.7027 | Acc: 0.1797 | LR: 0.000293 | Time: 55.7s\n",
      "  Batch [700/781] | Loss: 3.7772 | Acc: 0.2031 | LR: 0.000293 | Time: 57.2s\n",
      "  Batch [720/781] | Loss: 3.9525 | Acc: 0.1406 | LR: 0.000293 | Time: 58.8s\n",
      "  Batch [740/781] | Loss: 3.7204 | Acc: 0.1719 | LR: 0.000293 | Time: 60.4s\n",
      "  Batch [760/781] | Loss: 3.7606 | Acc: 0.1562 | LR: 0.000293 | Time: 62.1s\n",
      "  Batch [780/781] | Loss: 3.7795 | Acc: 0.2109 | LR: 0.000293 | Time: 63.6s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 3.7999 | Train Acc: 0.1683\n",
      "    Val Loss:   3.5598 | Val Acc:   0.2016\n",
      "    LR: 0.000287 | Time: 66.5s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.2016 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [  5/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.5857 | Acc: 0.1641 | LR: 0.000287 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 3.4788 | Acc: 0.2188 | LR: 0.000287 | Time: 1.9s\n",
      "  Batch [ 40/781] | Loss: 3.8282 | Acc: 0.1719 | LR: 0.000287 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 3.4665 | Acc: 0.2422 | LR: 0.000287 | Time: 5.3s\n",
      "  Batch [ 80/781] | Loss: 3.7258 | Acc: 0.1328 | LR: 0.000287 | Time: 6.9s\n",
      "  Batch [100/781] | Loss: 3.5777 | Acc: 0.2109 | LR: 0.000287 | Time: 8.5s\n",
      "  Batch [120/781] | Loss: 3.7973 | Acc: 0.1719 | LR: 0.000287 | Time: 10.1s\n",
      "  Batch [140/781] | Loss: 3.7646 | Acc: 0.1641 | LR: 0.000287 | Time: 11.8s\n",
      "  Batch [160/781] | Loss: 3.5888 | Acc: 0.2188 | LR: 0.000287 | Time: 13.4s\n",
      "  Batch [180/781] | Loss: 3.8734 | Acc: 0.1797 | LR: 0.000287 | Time: 15.0s\n",
      "  Batch [200/781] | Loss: 3.7619 | Acc: 0.1484 | LR: 0.000287 | Time: 16.6s\n",
      "  Batch [220/781] | Loss: 4.0387 | Acc: 0.1250 | LR: 0.000287 | Time: 18.2s\n",
      "  Batch [240/781] | Loss: 3.8503 | Acc: 0.1641 | LR: 0.000287 | Time: 19.8s\n",
      "  Batch [260/781] | Loss: 3.6986 | Acc: 0.1953 | LR: 0.000287 | Time: 21.5s\n",
      "  Batch [280/781] | Loss: 3.8807 | Acc: 0.1328 | LR: 0.000287 | Time: 23.1s\n",
      "  Batch [300/781] | Loss: 3.5467 | Acc: 0.1953 | LR: 0.000287 | Time: 24.8s\n",
      "  Batch [320/781] | Loss: 3.8168 | Acc: 0.1562 | LR: 0.000287 | Time: 26.5s\n",
      "  Batch [340/781] | Loss: 3.7815 | Acc: 0.1484 | LR: 0.000287 | Time: 28.2s\n",
      "  Batch [360/781] | Loss: 3.4844 | Acc: 0.2656 | LR: 0.000287 | Time: 29.8s\n",
      "  Batch [380/781] | Loss: 3.5865 | Acc: 0.1953 | LR: 0.000287 | Time: 31.4s\n",
      "  Batch [400/781] | Loss: 3.6240 | Acc: 0.2109 | LR: 0.000287 | Time: 32.9s\n",
      "  Batch [420/781] | Loss: 3.5785 | Acc: 0.2422 | LR: 0.000287 | Time: 34.5s\n",
      "  Batch [440/781] | Loss: 3.9026 | Acc: 0.1328 | LR: 0.000287 | Time: 36.1s\n",
      "  Batch [460/781] | Loss: 3.6180 | Acc: 0.2031 | LR: 0.000287 | Time: 37.7s\n",
      "  Batch [480/781] | Loss: 3.5558 | Acc: 0.2266 | LR: 0.000287 | Time: 39.5s\n",
      "  Batch [500/781] | Loss: 3.6014 | Acc: 0.2031 | LR: 0.000287 | Time: 41.0s\n",
      "  Batch [520/781] | Loss: 3.5568 | Acc: 0.2031 | LR: 0.000287 | Time: 42.7s\n",
      "  Batch [540/781] | Loss: 3.8825 | Acc: 0.2266 | LR: 0.000287 | Time: 44.3s\n",
      "  Batch [560/781] | Loss: 3.4264 | Acc: 0.2969 | LR: 0.000287 | Time: 45.9s\n",
      "  Batch [580/781] | Loss: 3.6404 | Acc: 0.2031 | LR: 0.000287 | Time: 47.5s\n",
      "  Batch [600/781] | Loss: 3.4719 | Acc: 0.1953 | LR: 0.000287 | Time: 49.1s\n",
      "  Batch [620/781] | Loss: 3.7174 | Acc: 0.1953 | LR: 0.000287 | Time: 50.9s\n",
      "  Batch [640/781] | Loss: 3.6503 | Acc: 0.1719 | LR: 0.000287 | Time: 52.5s\n",
      "  Batch [660/781] | Loss: 3.5271 | Acc: 0.2266 | LR: 0.000287 | Time: 54.2s\n",
      "  Batch [680/781] | Loss: 3.7800 | Acc: 0.2266 | LR: 0.000287 | Time: 55.8s\n",
      "  Batch [700/781] | Loss: 3.9154 | Acc: 0.1328 | LR: 0.000287 | Time: 57.4s\n",
      "  Batch [720/781] | Loss: 3.5004 | Acc: 0.2578 | LR: 0.000287 | Time: 59.0s\n",
      "  Batch [740/781] | Loss: 3.8300 | Acc: 0.1562 | LR: 0.000287 | Time: 60.7s\n",
      "  Batch [760/781] | Loss: 3.4921 | Acc: 0.2422 | LR: 0.000287 | Time: 62.3s\n",
      "  Batch [780/781] | Loss: 3.4936 | Acc: 0.2344 | LR: 0.000287 | Time: 63.9s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 3.6614 | Train Acc: 0.1918\n",
      "    Val Loss:   3.4209 | Val Acc:   0.2286\n",
      "    LR: 0.000280 | Time: 66.7s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.2286 ***\n",
      "Checkpoint saved: ./vit_checkpoints/checkpoint_epoch_5.pth\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [  6/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.4208 | Acc: 0.2109 | LR: 0.000280 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 3.6365 | Acc: 0.2344 | LR: 0.000280 | Time: 1.9s\n",
      "  Batch [ 40/781] | Loss: 3.6814 | Acc: 0.1641 | LR: 0.000280 | Time: 3.5s\n",
      "  Batch [ 60/781] | Loss: 3.4645 | Acc: 0.1875 | LR: 0.000280 | Time: 5.2s\n",
      "  Batch [ 80/781] | Loss: 3.5205 | Acc: 0.1719 | LR: 0.000280 | Time: 6.9s\n",
      "  Batch [100/781] | Loss: 3.6741 | Acc: 0.1875 | LR: 0.000280 | Time: 8.6s\n",
      "  Batch [120/781] | Loss: 3.6737 | Acc: 0.1641 | LR: 0.000280 | Time: 10.3s\n",
      "  Batch [140/781] | Loss: 3.4498 | Acc: 0.2812 | LR: 0.000280 | Time: 11.9s\n",
      "  Batch [160/781] | Loss: 3.6488 | Acc: 0.1641 | LR: 0.000280 | Time: 13.5s\n",
      "  Batch [180/781] | Loss: 3.6345 | Acc: 0.1953 | LR: 0.000280 | Time: 15.1s\n",
      "  Batch [200/781] | Loss: 3.4404 | Acc: 0.2031 | LR: 0.000280 | Time: 16.7s\n",
      "  Batch [220/781] | Loss: 3.4957 | Acc: 0.2266 | LR: 0.000280 | Time: 18.3s\n",
      "  Batch [240/781] | Loss: 3.5457 | Acc: 0.2188 | LR: 0.000280 | Time: 19.8s\n",
      "  Batch [260/781] | Loss: 3.5121 | Acc: 0.1797 | LR: 0.000280 | Time: 21.5s\n",
      "  Batch [280/781] | Loss: 3.4424 | Acc: 0.2422 | LR: 0.000280 | Time: 23.1s\n",
      "  Batch [300/781] | Loss: 3.7010 | Acc: 0.2031 | LR: 0.000280 | Time: 24.9s\n",
      "  Batch [320/781] | Loss: 3.5652 | Acc: 0.2656 | LR: 0.000280 | Time: 26.5s\n",
      "  Batch [340/781] | Loss: 3.5843 | Acc: 0.1797 | LR: 0.000280 | Time: 28.1s\n",
      "  Batch [360/781] | Loss: 3.5616 | Acc: 0.1953 | LR: 0.000280 | Time: 29.8s\n",
      "  Batch [380/781] | Loss: 3.5549 | Acc: 0.2109 | LR: 0.000280 | Time: 31.5s\n",
      "  Batch [400/781] | Loss: 3.7830 | Acc: 0.1641 | LR: 0.000280 | Time: 33.2s\n",
      "  Batch [420/781] | Loss: 3.8435 | Acc: 0.1875 | LR: 0.000280 | Time: 34.9s\n",
      "  Batch [440/781] | Loss: 3.4506 | Acc: 0.1641 | LR: 0.000280 | Time: 36.5s\n",
      "  Batch [460/781] | Loss: 3.2963 | Acc: 0.2734 | LR: 0.000280 | Time: 38.2s\n",
      "  Batch [480/781] | Loss: 3.4025 | Acc: 0.2578 | LR: 0.000280 | Time: 39.8s\n",
      "  Batch [500/781] | Loss: 3.6075 | Acc: 0.2344 | LR: 0.000280 | Time: 41.4s\n",
      "  Batch [520/781] | Loss: 3.4814 | Acc: 0.2031 | LR: 0.000280 | Time: 43.1s\n",
      "  Batch [540/781] | Loss: 3.3861 | Acc: 0.2188 | LR: 0.000280 | Time: 44.8s\n",
      "  Batch [560/781] | Loss: 3.5247 | Acc: 0.2266 | LR: 0.000280 | Time: 46.4s\n",
      "  Batch [580/781] | Loss: 3.5328 | Acc: 0.2266 | LR: 0.000280 | Time: 47.9s\n",
      "  Batch [600/781] | Loss: 3.5931 | Acc: 0.1562 | LR: 0.000280 | Time: 49.5s\n",
      "  Batch [620/781] | Loss: 3.7539 | Acc: 0.1797 | LR: 0.000280 | Time: 51.1s\n",
      "  Batch [640/781] | Loss: 3.2684 | Acc: 0.2578 | LR: 0.000280 | Time: 52.8s\n",
      "  Batch [660/781] | Loss: 3.5588 | Acc: 0.1719 | LR: 0.000280 | Time: 54.4s\n",
      "  Batch [680/781] | Loss: 3.4043 | Acc: 0.2812 | LR: 0.000280 | Time: 56.1s\n",
      "  Batch [700/781] | Loss: 3.6385 | Acc: 0.1719 | LR: 0.000280 | Time: 57.9s\n",
      "  Batch [720/781] | Loss: 3.3930 | Acc: 0.1953 | LR: 0.000280 | Time: 59.5s\n",
      "  Batch [740/781] | Loss: 3.6790 | Acc: 0.2109 | LR: 0.000280 | Time: 61.1s\n",
      "  Batch [760/781] | Loss: 3.6869 | Acc: 0.1953 | LR: 0.000280 | Time: 62.7s\n",
      "  Batch [780/781] | Loss: 3.5464 | Acc: 0.1797 | LR: 0.000280 | Time: 64.3s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 3.5444 | Train Acc: 0.2116\n",
      "    Val Loss:   3.3243 | Val Acc:   0.2447\n",
      "    LR: 0.000271 | Time: 67.1s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.2447 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [  7/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.6906 | Acc: 0.1641 | LR: 0.000271 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 3.4344 | Acc: 0.1875 | LR: 0.000271 | Time: 2.5s\n",
      "  Batch [ 40/781] | Loss: 3.4807 | Acc: 0.2031 | LR: 0.000271 | Time: 4.1s\n",
      "  Batch [ 60/781] | Loss: 3.4193 | Acc: 0.2500 | LR: 0.000271 | Time: 5.7s\n",
      "  Batch [ 80/781] | Loss: 3.5090 | Acc: 0.2188 | LR: 0.000271 | Time: 7.5s\n",
      "  Batch [100/781] | Loss: 3.4711 | Acc: 0.2188 | LR: 0.000271 | Time: 9.3s\n",
      "  Batch [120/781] | Loss: 3.4260 | Acc: 0.2734 | LR: 0.000271 | Time: 10.9s\n",
      "  Batch [140/781] | Loss: 3.2125 | Acc: 0.2031 | LR: 0.000271 | Time: 12.5s\n",
      "  Batch [160/781] | Loss: 3.6226 | Acc: 0.1875 | LR: 0.000271 | Time: 14.2s\n",
      "  Batch [180/781] | Loss: 3.3727 | Acc: 0.2266 | LR: 0.000271 | Time: 15.8s\n",
      "  Batch [200/781] | Loss: 3.4826 | Acc: 0.2031 | LR: 0.000271 | Time: 17.6s\n",
      "  Batch [220/781] | Loss: 3.4990 | Acc: 0.2188 | LR: 0.000271 | Time: 19.4s\n",
      "  Batch [240/781] | Loss: 3.4006 | Acc: 0.2500 | LR: 0.000271 | Time: 21.1s\n",
      "  Batch [260/781] | Loss: 3.4316 | Acc: 0.1719 | LR: 0.000271 | Time: 22.7s\n",
      "  Batch [280/781] | Loss: 3.4534 | Acc: 0.1875 | LR: 0.000271 | Time: 24.3s\n",
      "  Batch [300/781] | Loss: 3.5278 | Acc: 0.2031 | LR: 0.000271 | Time: 26.1s\n",
      "  Batch [320/781] | Loss: 3.5532 | Acc: 0.2500 | LR: 0.000271 | Time: 27.7s\n",
      "  Batch [340/781] | Loss: 3.3505 | Acc: 0.2188 | LR: 0.000271 | Time: 29.4s\n",
      "  Batch [360/781] | Loss: 3.2530 | Acc: 0.2734 | LR: 0.000271 | Time: 31.0s\n",
      "  Batch [380/781] | Loss: 3.3963 | Acc: 0.2266 | LR: 0.000271 | Time: 32.6s\n",
      "  Batch [400/781] | Loss: 3.7638 | Acc: 0.2266 | LR: 0.000271 | Time: 34.2s\n",
      "  Batch [420/781] | Loss: 3.2749 | Acc: 0.2891 | LR: 0.000271 | Time: 35.8s\n",
      "  Batch [440/781] | Loss: 3.4137 | Acc: 0.2656 | LR: 0.000271 | Time: 37.6s\n",
      "  Batch [460/781] | Loss: 3.2684 | Acc: 0.2500 | LR: 0.000271 | Time: 39.3s\n",
      "  Batch [480/781] | Loss: 3.4608 | Acc: 0.2734 | LR: 0.000271 | Time: 41.0s\n",
      "  Batch [500/781] | Loss: 3.6518 | Acc: 0.1953 | LR: 0.000271 | Time: 42.6s\n",
      "  Batch [520/781] | Loss: 3.7771 | Acc: 0.1484 | LR: 0.000271 | Time: 44.3s\n",
      "  Batch [540/781] | Loss: 3.6096 | Acc: 0.1562 | LR: 0.000271 | Time: 46.0s\n",
      "  Batch [560/781] | Loss: 3.2395 | Acc: 0.2656 | LR: 0.000271 | Time: 47.7s\n",
      "  Batch [580/781] | Loss: 3.7627 | Acc: 0.1406 | LR: 0.000271 | Time: 49.3s\n",
      "  Batch [600/781] | Loss: 3.2463 | Acc: 0.2500 | LR: 0.000271 | Time: 51.0s\n",
      "  Batch [620/781] | Loss: 3.4828 | Acc: 0.1562 | LR: 0.000271 | Time: 52.7s\n",
      "  Batch [640/781] | Loss: 3.2799 | Acc: 0.2891 | LR: 0.000271 | Time: 54.3s\n",
      "  Batch [660/781] | Loss: 3.2870 | Acc: 0.2969 | LR: 0.000271 | Time: 56.0s\n",
      "  Batch [680/781] | Loss: 3.3546 | Acc: 0.2344 | LR: 0.000271 | Time: 57.6s\n",
      "  Batch [700/781] | Loss: 3.2837 | Acc: 0.2734 | LR: 0.000271 | Time: 59.3s\n",
      "  Batch [720/781] | Loss: 3.6738 | Acc: 0.1719 | LR: 0.000271 | Time: 61.0s\n",
      "  Batch [740/781] | Loss: 3.3179 | Acc: 0.2500 | LR: 0.000271 | Time: 62.7s\n",
      "  Batch [760/781] | Loss: 3.1743 | Acc: 0.2734 | LR: 0.000271 | Time: 64.4s\n",
      "  Batch [780/781] | Loss: 3.2850 | Acc: 0.2969 | LR: 0.000271 | Time: 66.0s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 3.4457 | Train Acc: 0.2254\n",
      "    Val Loss:   3.2485 | Val Acc:   0.2550\n",
      "    LR: 0.000261 | Time: 69.6s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.2550 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [  8/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.5467 | Acc: 0.2656 | LR: 0.000261 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 3.1711 | Acc: 0.2578 | LR: 0.000261 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 3.3584 | Acc: 0.2578 | LR: 0.000261 | Time: 3.7s\n",
      "  Batch [ 60/781] | Loss: 3.2870 | Acc: 0.2734 | LR: 0.000261 | Time: 5.4s\n",
      "  Batch [ 80/781] | Loss: 3.5316 | Acc: 0.1953 | LR: 0.000261 | Time: 7.0s\n",
      "  Batch [100/781] | Loss: 3.2529 | Acc: 0.2656 | LR: 0.000261 | Time: 8.7s\n",
      "  Batch [120/781] | Loss: 3.3146 | Acc: 0.2578 | LR: 0.000261 | Time: 10.4s\n",
      "  Batch [140/781] | Loss: 3.1563 | Acc: 0.2812 | LR: 0.000261 | Time: 12.0s\n",
      "  Batch [160/781] | Loss: 3.1438 | Acc: 0.3125 | LR: 0.000261 | Time: 13.7s\n",
      "  Batch [180/781] | Loss: 3.4710 | Acc: 0.2891 | LR: 0.000261 | Time: 15.4s\n",
      "  Batch [200/781] | Loss: 3.5710 | Acc: 0.2422 | LR: 0.000261 | Time: 17.1s\n",
      "  Batch [220/781] | Loss: 3.4982 | Acc: 0.2031 | LR: 0.000261 | Time: 18.7s\n",
      "  Batch [240/781] | Loss: 3.5490 | Acc: 0.2656 | LR: 0.000261 | Time: 20.5s\n",
      "  Batch [260/781] | Loss: 3.3967 | Acc: 0.2266 | LR: 0.000261 | Time: 22.2s\n",
      "  Batch [280/781] | Loss: 3.1171 | Acc: 0.2812 | LR: 0.000261 | Time: 23.8s\n",
      "  Batch [300/781] | Loss: 3.4234 | Acc: 0.2344 | LR: 0.000261 | Time: 25.5s\n",
      "  Batch [320/781] | Loss: 3.4990 | Acc: 0.2109 | LR: 0.000261 | Time: 27.1s\n",
      "  Batch [340/781] | Loss: 3.3526 | Acc: 0.2734 | LR: 0.000261 | Time: 28.9s\n",
      "  Batch [360/781] | Loss: 3.4593 | Acc: 0.2109 | LR: 0.000261 | Time: 30.5s\n",
      "  Batch [380/781] | Loss: 3.5267 | Acc: 0.2031 | LR: 0.000261 | Time: 32.1s\n",
      "  Batch [400/781] | Loss: 3.2848 | Acc: 0.2578 | LR: 0.000261 | Time: 33.8s\n",
      "  Batch [420/781] | Loss: 3.2954 | Acc: 0.2422 | LR: 0.000261 | Time: 35.4s\n",
      "  Batch [440/781] | Loss: 3.2209 | Acc: 0.3125 | LR: 0.000261 | Time: 37.0s\n",
      "  Batch [460/781] | Loss: 3.3187 | Acc: 0.2578 | LR: 0.000261 | Time: 38.7s\n",
      "  Batch [480/781] | Loss: 3.5083 | Acc: 0.2031 | LR: 0.000261 | Time: 40.3s\n",
      "  Batch [500/781] | Loss: 3.2073 | Acc: 0.2500 | LR: 0.000261 | Time: 41.9s\n",
      "  Batch [520/781] | Loss: 3.1613 | Acc: 0.3359 | LR: 0.000261 | Time: 43.6s\n",
      "  Batch [540/781] | Loss: 3.5696 | Acc: 0.2031 | LR: 0.000261 | Time: 45.2s\n",
      "  Batch [560/781] | Loss: 3.2631 | Acc: 0.2578 | LR: 0.000261 | Time: 46.8s\n",
      "  Batch [580/781] | Loss: 3.6334 | Acc: 0.1562 | LR: 0.000261 | Time: 48.4s\n",
      "  Batch [600/781] | Loss: 3.4447 | Acc: 0.2500 | LR: 0.000261 | Time: 50.1s\n",
      "  Batch [620/781] | Loss: 3.5539 | Acc: 0.2188 | LR: 0.000261 | Time: 51.8s\n",
      "  Batch [640/781] | Loss: 3.2855 | Acc: 0.2500 | LR: 0.000261 | Time: 53.5s\n",
      "  Batch [660/781] | Loss: 3.2976 | Acc: 0.2500 | LR: 0.000261 | Time: 55.1s\n",
      "  Batch [680/781] | Loss: 3.4553 | Acc: 0.2422 | LR: 0.000261 | Time: 56.8s\n",
      "  Batch [700/781] | Loss: 3.1358 | Acc: 0.3047 | LR: 0.000261 | Time: 58.4s\n",
      "  Batch [720/781] | Loss: 3.5971 | Acc: 0.1797 | LR: 0.000261 | Time: 60.1s\n",
      "  Batch [740/781] | Loss: 3.2161 | Acc: 0.2656 | LR: 0.000261 | Time: 61.8s\n",
      "  Batch [760/781] | Loss: 3.5383 | Acc: 0.1641 | LR: 0.000261 | Time: 63.4s\n",
      "  Batch [780/781] | Loss: 3.5267 | Acc: 0.2188 | LR: 0.000261 | Time: 65.2s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 3.3501 | Train Acc: 0.2411\n",
      "    Val Loss:   3.1803 | Val Acc:   0.2683\n",
      "    LR: 0.000250 | Time: 68.1s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.2683 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [  9/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.0681 | Acc: 0.3203 | LR: 0.000250 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 3.2514 | Acc: 0.2969 | LR: 0.000250 | Time: 2.1s\n",
      "  Batch [ 40/781] | Loss: 3.0080 | Acc: 0.2812 | LR: 0.000250 | Time: 3.7s\n",
      "  Batch [ 60/781] | Loss: 3.0520 | Acc: 0.2578 | LR: 0.000250 | Time: 5.3s\n",
      "  Batch [ 80/781] | Loss: 3.2465 | Acc: 0.2578 | LR: 0.000250 | Time: 6.9s\n",
      "  Batch [100/781] | Loss: 3.2437 | Acc: 0.1953 | LR: 0.000250 | Time: 8.5s\n",
      "  Batch [120/781] | Loss: 3.3097 | Acc: 0.2812 | LR: 0.000250 | Time: 10.1s\n",
      "  Batch [140/781] | Loss: 3.1178 | Acc: 0.2969 | LR: 0.000250 | Time: 11.7s\n",
      "  Batch [160/781] | Loss: 2.8553 | Acc: 0.2891 | LR: 0.000250 | Time: 13.2s\n",
      "  Batch [180/781] | Loss: 3.2670 | Acc: 0.2656 | LR: 0.000250 | Time: 14.9s\n",
      "  Batch [200/781] | Loss: 3.3960 | Acc: 0.2656 | LR: 0.000250 | Time: 16.6s\n",
      "  Batch [220/781] | Loss: 3.1587 | Acc: 0.2734 | LR: 0.000250 | Time: 18.1s\n",
      "  Batch [240/781] | Loss: 3.3876 | Acc: 0.2500 | LR: 0.000250 | Time: 19.8s\n",
      "  Batch [260/781] | Loss: 3.3022 | Acc: 0.2266 | LR: 0.000250 | Time: 21.5s\n",
      "  Batch [280/781] | Loss: 3.4249 | Acc: 0.2188 | LR: 0.000250 | Time: 23.2s\n",
      "  Batch [300/781] | Loss: 3.1966 | Acc: 0.2500 | LR: 0.000250 | Time: 24.9s\n",
      "  Batch [320/781] | Loss: 3.2261 | Acc: 0.2500 | LR: 0.000250 | Time: 26.5s\n",
      "  Batch [340/781] | Loss: 3.1339 | Acc: 0.3125 | LR: 0.000250 | Time: 28.3s\n",
      "  Batch [360/781] | Loss: 2.9719 | Acc: 0.2891 | LR: 0.000250 | Time: 29.9s\n",
      "  Batch [380/781] | Loss: 3.4614 | Acc: 0.2266 | LR: 0.000250 | Time: 31.6s\n",
      "  Batch [400/781] | Loss: 3.2797 | Acc: 0.2422 | LR: 0.000250 | Time: 33.2s\n",
      "  Batch [420/781] | Loss: 3.1220 | Acc: 0.2656 | LR: 0.000250 | Time: 34.9s\n",
      "  Batch [440/781] | Loss: 3.5751 | Acc: 0.2031 | LR: 0.000250 | Time: 36.5s\n",
      "  Batch [460/781] | Loss: 3.1722 | Acc: 0.2891 | LR: 0.000250 | Time: 38.1s\n",
      "  Batch [480/781] | Loss: 3.0943 | Acc: 0.2500 | LR: 0.000250 | Time: 39.7s\n",
      "  Batch [500/781] | Loss: 3.3928 | Acc: 0.2578 | LR: 0.000250 | Time: 41.5s\n",
      "  Batch [520/781] | Loss: 3.4108 | Acc: 0.2188 | LR: 0.000250 | Time: 43.1s\n",
      "  Batch [540/781] | Loss: 3.2887 | Acc: 0.1953 | LR: 0.000250 | Time: 44.7s\n",
      "  Batch [560/781] | Loss: 3.2453 | Acc: 0.3125 | LR: 0.000250 | Time: 46.5s\n",
      "  Batch [580/781] | Loss: 3.3422 | Acc: 0.2109 | LR: 0.000250 | Time: 48.2s\n",
      "  Batch [600/781] | Loss: 3.2834 | Acc: 0.2266 | LR: 0.000250 | Time: 49.9s\n",
      "  Batch [620/781] | Loss: 3.2423 | Acc: 0.2891 | LR: 0.000250 | Time: 51.7s\n",
      "  Batch [640/781] | Loss: 3.2935 | Acc: 0.3047 | LR: 0.000250 | Time: 53.4s\n",
      "  Batch [660/781] | Loss: 3.2588 | Acc: 0.3281 | LR: 0.000250 | Time: 55.0s\n",
      "  Batch [680/781] | Loss: 3.3001 | Acc: 0.2578 | LR: 0.000250 | Time: 56.6s\n",
      "  Batch [700/781] | Loss: 3.0533 | Acc: 0.2734 | LR: 0.000250 | Time: 58.3s\n",
      "  Batch [720/781] | Loss: 3.4998 | Acc: 0.2266 | LR: 0.000250 | Time: 60.0s\n",
      "  Batch [740/781] | Loss: 3.0807 | Acc: 0.3359 | LR: 0.000250 | Time: 61.7s\n",
      "  Batch [760/781] | Loss: 3.2789 | Acc: 0.2500 | LR: 0.000250 | Time: 63.3s\n",
      "  Batch [780/781] | Loss: 3.5152 | Acc: 0.2344 | LR: 0.000250 | Time: 65.1s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 3.2682 | Train Acc: 0.2561\n",
      "    Val Loss:   3.1430 | Val Acc:   0.2786\n",
      "    LR: 0.000238 | Time: 68.0s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.2786 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 10/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.2884 | Acc: 0.2578 | LR: 0.000238 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 3.2458 | Acc: 0.2500 | LR: 0.000238 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 3.1528 | Acc: 0.2656 | LR: 0.000238 | Time: 3.7s\n",
      "  Batch [ 60/781] | Loss: 2.8738 | Acc: 0.3281 | LR: 0.000238 | Time: 5.4s\n",
      "  Batch [ 80/781] | Loss: 3.0902 | Acc: 0.2188 | LR: 0.000238 | Time: 7.1s\n",
      "  Batch [100/781] | Loss: 3.0648 | Acc: 0.3125 | LR: 0.000238 | Time: 8.8s\n",
      "  Batch [120/781] | Loss: 3.0792 | Acc: 0.2656 | LR: 0.000238 | Time: 10.6s\n",
      "  Batch [140/781] | Loss: 3.3226 | Acc: 0.1953 | LR: 0.000238 | Time: 12.3s\n",
      "  Batch [160/781] | Loss: 3.3932 | Acc: 0.2344 | LR: 0.000238 | Time: 14.1s\n",
      "  Batch [180/781] | Loss: 3.3529 | Acc: 0.2266 | LR: 0.000238 | Time: 15.7s\n",
      "  Batch [200/781] | Loss: 3.1528 | Acc: 0.2969 | LR: 0.000238 | Time: 17.4s\n",
      "  Batch [220/781] | Loss: 3.2393 | Acc: 0.2344 | LR: 0.000238 | Time: 19.1s\n",
      "  Batch [240/781] | Loss: 3.4860 | Acc: 0.1875 | LR: 0.000238 | Time: 20.8s\n",
      "  Batch [260/781] | Loss: 3.0392 | Acc: 0.3438 | LR: 0.000238 | Time: 22.5s\n",
      "  Batch [280/781] | Loss: 3.2428 | Acc: 0.2500 | LR: 0.000238 | Time: 24.3s\n",
      "  Batch [300/781] | Loss: 3.0015 | Acc: 0.3047 | LR: 0.000238 | Time: 25.9s\n",
      "  Batch [320/781] | Loss: 3.2750 | Acc: 0.2578 | LR: 0.000238 | Time: 27.6s\n",
      "  Batch [340/781] | Loss: 3.4268 | Acc: 0.2422 | LR: 0.000238 | Time: 29.2s\n",
      "  Batch [360/781] | Loss: 3.2598 | Acc: 0.2891 | LR: 0.000238 | Time: 30.9s\n",
      "  Batch [380/781] | Loss: 3.1587 | Acc: 0.3125 | LR: 0.000238 | Time: 32.5s\n",
      "  Batch [400/781] | Loss: 3.5193 | Acc: 0.2344 | LR: 0.000238 | Time: 34.2s\n",
      "  Batch [420/781] | Loss: 3.2367 | Acc: 0.2500 | LR: 0.000238 | Time: 35.8s\n",
      "  Batch [440/781] | Loss: 3.4607 | Acc: 0.1953 | LR: 0.000238 | Time: 37.4s\n",
      "  Batch [460/781] | Loss: 3.3649 | Acc: 0.2109 | LR: 0.000238 | Time: 39.1s\n",
      "  Batch [480/781] | Loss: 3.2078 | Acc: 0.2578 | LR: 0.000238 | Time: 40.7s\n",
      "  Batch [500/781] | Loss: 3.2153 | Acc: 0.2578 | LR: 0.000238 | Time: 42.4s\n",
      "  Batch [520/781] | Loss: 3.1227 | Acc: 0.2344 | LR: 0.000238 | Time: 44.1s\n",
      "  Batch [540/781] | Loss: 3.1705 | Acc: 0.2500 | LR: 0.000238 | Time: 45.7s\n",
      "  Batch [560/781] | Loss: 3.2858 | Acc: 0.2812 | LR: 0.000238 | Time: 47.4s\n",
      "  Batch [580/781] | Loss: 3.1089 | Acc: 0.3516 | LR: 0.000238 | Time: 49.0s\n",
      "  Batch [600/781] | Loss: 2.9974 | Acc: 0.3594 | LR: 0.000238 | Time: 50.7s\n",
      "  Batch [620/781] | Loss: 3.1972 | Acc: 0.2812 | LR: 0.000238 | Time: 52.4s\n",
      "  Batch [640/781] | Loss: 3.2303 | Acc: 0.2734 | LR: 0.000238 | Time: 54.1s\n",
      "  Batch [660/781] | Loss: 3.2261 | Acc: 0.2188 | LR: 0.000238 | Time: 55.8s\n",
      "  Batch [680/781] | Loss: 2.8475 | Acc: 0.3438 | LR: 0.000238 | Time: 57.5s\n",
      "  Batch [700/781] | Loss: 3.0663 | Acc: 0.2656 | LR: 0.000238 | Time: 59.1s\n",
      "  Batch [720/781] | Loss: 3.0579 | Acc: 0.3125 | LR: 0.000238 | Time: 60.8s\n",
      "  Batch [740/781] | Loss: 3.3230 | Acc: 0.2109 | LR: 0.000238 | Time: 62.4s\n",
      "  Batch [760/781] | Loss: 3.2507 | Acc: 0.2656 | LR: 0.000238 | Time: 64.0s\n",
      "  Batch [780/781] | Loss: 3.0047 | Acc: 0.3281 | LR: 0.000238 | Time: 65.6s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 3.1858 | Train Acc: 0.2704\n",
      "    Val Loss:   3.0722 | Val Acc:   0.2886\n",
      "    LR: 0.000225 | Time: 68.7s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.2886 ***\n",
      "Checkpoint saved: ./vit_checkpoints/checkpoint_epoch_10.pth\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 11/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.2823 | Acc: 0.2969 | LR: 0.000225 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 3.0719 | Acc: 0.2969 | LR: 0.000225 | Time: 2.1s\n",
      "  Batch [ 40/781] | Loss: 3.3297 | Acc: 0.2188 | LR: 0.000225 | Time: 3.7s\n",
      "  Batch [ 60/781] | Loss: 3.0228 | Acc: 0.3047 | LR: 0.000225 | Time: 5.4s\n",
      "  Batch [ 80/781] | Loss: 3.1879 | Acc: 0.2734 | LR: 0.000225 | Time: 7.0s\n",
      "  Batch [100/781] | Loss: 3.3256 | Acc: 0.2734 | LR: 0.000225 | Time: 8.8s\n",
      "  Batch [120/781] | Loss: 3.0310 | Acc: 0.2891 | LR: 0.000225 | Time: 10.4s\n",
      "  Batch [140/781] | Loss: 2.9966 | Acc: 0.3359 | LR: 0.000225 | Time: 12.2s\n",
      "  Batch [160/781] | Loss: 2.9188 | Acc: 0.2500 | LR: 0.000225 | Time: 13.8s\n",
      "  Batch [180/781] | Loss: 2.8274 | Acc: 0.3281 | LR: 0.000225 | Time: 15.5s\n",
      "  Batch [200/781] | Loss: 2.9409 | Acc: 0.3125 | LR: 0.000225 | Time: 17.1s\n",
      "  Batch [220/781] | Loss: 3.1730 | Acc: 0.3203 | LR: 0.000225 | Time: 18.8s\n",
      "  Batch [240/781] | Loss: 3.1232 | Acc: 0.3203 | LR: 0.000225 | Time: 20.5s\n",
      "  Batch [260/781] | Loss: 3.4140 | Acc: 0.2969 | LR: 0.000225 | Time: 22.0s\n",
      "  Batch [280/781] | Loss: 2.7919 | Acc: 0.3359 | LR: 0.000225 | Time: 23.7s\n",
      "  Batch [300/781] | Loss: 2.9162 | Acc: 0.2734 | LR: 0.000225 | Time: 25.4s\n",
      "  Batch [320/781] | Loss: 3.1823 | Acc: 0.2891 | LR: 0.000225 | Time: 27.0s\n",
      "  Batch [340/781] | Loss: 3.2667 | Acc: 0.2500 | LR: 0.000225 | Time: 28.7s\n",
      "  Batch [360/781] | Loss: 3.1339 | Acc: 0.2500 | LR: 0.000225 | Time: 30.3s\n",
      "  Batch [380/781] | Loss: 3.6617 | Acc: 0.1953 | LR: 0.000225 | Time: 31.9s\n",
      "  Batch [400/781] | Loss: 3.2575 | Acc: 0.2656 | LR: 0.000225 | Time: 33.5s\n",
      "  Batch [420/781] | Loss: 3.0492 | Acc: 0.3672 | LR: 0.000225 | Time: 35.2s\n",
      "  Batch [440/781] | Loss: 3.1077 | Acc: 0.2656 | LR: 0.000225 | Time: 36.9s\n",
      "  Batch [460/781] | Loss: 3.1912 | Acc: 0.2578 | LR: 0.000225 | Time: 38.6s\n",
      "  Batch [480/781] | Loss: 3.0930 | Acc: 0.2578 | LR: 0.000225 | Time: 40.2s\n",
      "  Batch [500/781] | Loss: 3.0001 | Acc: 0.3125 | LR: 0.000225 | Time: 41.9s\n",
      "  Batch [520/781] | Loss: 3.1819 | Acc: 0.3047 | LR: 0.000225 | Time: 43.6s\n",
      "  Batch [540/781] | Loss: 3.3246 | Acc: 0.2344 | LR: 0.000225 | Time: 45.2s\n",
      "  Batch [560/781] | Loss: 3.3025 | Acc: 0.2656 | LR: 0.000225 | Time: 46.9s\n",
      "  Batch [580/781] | Loss: 2.9786 | Acc: 0.3281 | LR: 0.000225 | Time: 48.5s\n",
      "  Batch [600/781] | Loss: 3.2198 | Acc: 0.2422 | LR: 0.000225 | Time: 50.1s\n",
      "  Batch [620/781] | Loss: 3.2945 | Acc: 0.2344 | LR: 0.000225 | Time: 51.7s\n",
      "  Batch [640/781] | Loss: 3.3439 | Acc: 0.2344 | LR: 0.000225 | Time: 53.3s\n",
      "  Batch [660/781] | Loss: 3.1074 | Acc: 0.2656 | LR: 0.000225 | Time: 54.9s\n",
      "  Batch [680/781] | Loss: 3.0459 | Acc: 0.3203 | LR: 0.000225 | Time: 56.5s\n",
      "  Batch [700/781] | Loss: 3.1977 | Acc: 0.2656 | LR: 0.000225 | Time: 58.2s\n",
      "  Batch [720/781] | Loss: 2.9638 | Acc: 0.3047 | LR: 0.000225 | Time: 60.0s\n",
      "  Batch [740/781] | Loss: 3.1500 | Acc: 0.2578 | LR: 0.000225 | Time: 61.7s\n",
      "  Batch [760/781] | Loss: 3.3227 | Acc: 0.2578 | LR: 0.000225 | Time: 63.3s\n",
      "  Batch [780/781] | Loss: 3.1429 | Acc: 0.2422 | LR: 0.000225 | Time: 65.0s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 3.1166 | Train Acc: 0.2832\n",
      "    Val Loss:   3.0216 | Val Acc:   0.3008\n",
      "    LR: 0.000211 | Time: 67.9s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3008 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 12/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.2619 | Acc: 0.3125 | LR: 0.000211 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 3.2147 | Acc: 0.2188 | LR: 0.000211 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 3.0150 | Acc: 0.2969 | LR: 0.000211 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 3.2222 | Acc: 0.2656 | LR: 0.000211 | Time: 5.4s\n",
      "  Batch [ 80/781] | Loss: 3.1573 | Acc: 0.3125 | LR: 0.000211 | Time: 7.0s\n",
      "  Batch [100/781] | Loss: 3.0883 | Acc: 0.2969 | LR: 0.000211 | Time: 8.7s\n",
      "  Batch [120/781] | Loss: 3.2113 | Acc: 0.2500 | LR: 0.000211 | Time: 10.3s\n",
      "  Batch [140/781] | Loss: 2.6033 | Acc: 0.3750 | LR: 0.000211 | Time: 12.0s\n",
      "  Batch [160/781] | Loss: 2.9941 | Acc: 0.3047 | LR: 0.000211 | Time: 13.5s\n",
      "  Batch [180/781] | Loss: 3.2038 | Acc: 0.2578 | LR: 0.000211 | Time: 15.2s\n",
      "  Batch [200/781] | Loss: 2.8289 | Acc: 0.3203 | LR: 0.000211 | Time: 16.9s\n",
      "  Batch [220/781] | Loss: 2.6625 | Acc: 0.3828 | LR: 0.000211 | Time: 18.7s\n",
      "  Batch [240/781] | Loss: 2.9309 | Acc: 0.3672 | LR: 0.000211 | Time: 20.3s\n",
      "  Batch [260/781] | Loss: 2.9170 | Acc: 0.2969 | LR: 0.000211 | Time: 21.9s\n",
      "  Batch [280/781] | Loss: 3.0261 | Acc: 0.2812 | LR: 0.000211 | Time: 23.6s\n",
      "  Batch [300/781] | Loss: 3.0847 | Acc: 0.3125 | LR: 0.000211 | Time: 25.2s\n",
      "  Batch [320/781] | Loss: 2.9039 | Acc: 0.3438 | LR: 0.000211 | Time: 26.9s\n",
      "  Batch [340/781] | Loss: 3.2051 | Acc: 0.2578 | LR: 0.000211 | Time: 28.5s\n",
      "  Batch [360/781] | Loss: 3.1347 | Acc: 0.2578 | LR: 0.000211 | Time: 30.1s\n",
      "  Batch [380/781] | Loss: 2.7797 | Acc: 0.3672 | LR: 0.000211 | Time: 31.7s\n",
      "  Batch [400/781] | Loss: 3.0693 | Acc: 0.2422 | LR: 0.000211 | Time: 33.3s\n",
      "  Batch [420/781] | Loss: 2.7193 | Acc: 0.2891 | LR: 0.000211 | Time: 34.9s\n",
      "  Batch [440/781] | Loss: 2.9644 | Acc: 0.2891 | LR: 0.000211 | Time: 36.8s\n",
      "  Batch [460/781] | Loss: 2.8051 | Acc: 0.3594 | LR: 0.000211 | Time: 38.6s\n",
      "  Batch [480/781] | Loss: 3.1291 | Acc: 0.2734 | LR: 0.000211 | Time: 40.2s\n",
      "  Batch [500/781] | Loss: 2.9102 | Acc: 0.2422 | LR: 0.000211 | Time: 41.9s\n",
      "  Batch [520/781] | Loss: 2.9955 | Acc: 0.3516 | LR: 0.000211 | Time: 43.5s\n",
      "  Batch [540/781] | Loss: 2.6146 | Acc: 0.3750 | LR: 0.000211 | Time: 45.0s\n",
      "  Batch [560/781] | Loss: 3.0386 | Acc: 0.2656 | LR: 0.000211 | Time: 46.6s\n",
      "  Batch [580/781] | Loss: 2.6068 | Acc: 0.3281 | LR: 0.000211 | Time: 48.2s\n",
      "  Batch [600/781] | Loss: 3.0021 | Acc: 0.3672 | LR: 0.000211 | Time: 49.8s\n",
      "  Batch [620/781] | Loss: 3.0157 | Acc: 0.3125 | LR: 0.000211 | Time: 51.4s\n",
      "  Batch [640/781] | Loss: 2.8117 | Acc: 0.2812 | LR: 0.000211 | Time: 53.1s\n",
      "  Batch [660/781] | Loss: 3.0433 | Acc: 0.3203 | LR: 0.000211 | Time: 54.7s\n",
      "  Batch [680/781] | Loss: 3.0682 | Acc: 0.2812 | LR: 0.000211 | Time: 56.4s\n",
      "  Batch [700/781] | Loss: 3.0874 | Acc: 0.3516 | LR: 0.000211 | Time: 58.1s\n",
      "  Batch [720/781] | Loss: 2.8817 | Acc: 0.3281 | LR: 0.000211 | Time: 59.6s\n",
      "  Batch [740/781] | Loss: 2.9726 | Acc: 0.3281 | LR: 0.000211 | Time: 61.2s\n",
      "  Batch [760/781] | Loss: 3.2131 | Acc: 0.3203 | LR: 0.000211 | Time: 62.9s\n",
      "  Batch [780/781] | Loss: 2.8876 | Acc: 0.4141 | LR: 0.000211 | Time: 64.5s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 3.0444 | Train Acc: 0.2949\n",
      "    Val Loss:   2.9471 | Val Acc:   0.3131\n",
      "    LR: 0.000196 | Time: 67.3s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3131 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 13/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 3.0259 | Acc: 0.3047 | LR: 0.000196 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.9709 | Acc: 0.2812 | LR: 0.000196 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 2.7373 | Acc: 0.3125 | LR: 0.000196 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 2.9355 | Acc: 0.3828 | LR: 0.000196 | Time: 5.3s\n",
      "  Batch [ 80/781] | Loss: 2.9122 | Acc: 0.3359 | LR: 0.000196 | Time: 6.9s\n",
      "  Batch [100/781] | Loss: 2.8210 | Acc: 0.3359 | LR: 0.000196 | Time: 8.5s\n",
      "  Batch [120/781] | Loss: 3.2623 | Acc: 0.2500 | LR: 0.000196 | Time: 10.1s\n",
      "  Batch [140/781] | Loss: 2.8630 | Acc: 0.2969 | LR: 0.000196 | Time: 11.7s\n",
      "  Batch [160/781] | Loss: 3.0485 | Acc: 0.2656 | LR: 0.000196 | Time: 13.3s\n",
      "  Batch [180/781] | Loss: 3.2591 | Acc: 0.2422 | LR: 0.000196 | Time: 15.0s\n",
      "  Batch [200/781] | Loss: 2.9830 | Acc: 0.2969 | LR: 0.000196 | Time: 16.6s\n",
      "  Batch [220/781] | Loss: 2.7913 | Acc: 0.3438 | LR: 0.000196 | Time: 18.2s\n",
      "  Batch [240/781] | Loss: 3.1539 | Acc: 0.2188 | LR: 0.000196 | Time: 19.8s\n",
      "  Batch [260/781] | Loss: 2.6722 | Acc: 0.3594 | LR: 0.000196 | Time: 21.6s\n",
      "  Batch [280/781] | Loss: 2.7735 | Acc: 0.4062 | LR: 0.000196 | Time: 23.2s\n",
      "  Batch [300/781] | Loss: 3.1252 | Acc: 0.2500 | LR: 0.000196 | Time: 24.9s\n",
      "  Batch [320/781] | Loss: 2.9259 | Acc: 0.2891 | LR: 0.000196 | Time: 26.5s\n",
      "  Batch [340/781] | Loss: 3.0458 | Acc: 0.3203 | LR: 0.000196 | Time: 28.1s\n",
      "  Batch [360/781] | Loss: 3.1261 | Acc: 0.2891 | LR: 0.000196 | Time: 29.7s\n",
      "  Batch [380/781] | Loss: 3.0682 | Acc: 0.3281 | LR: 0.000196 | Time: 31.3s\n",
      "  Batch [400/781] | Loss: 3.0519 | Acc: 0.2812 | LR: 0.000196 | Time: 32.9s\n",
      "  Batch [420/781] | Loss: 2.7451 | Acc: 0.3516 | LR: 0.000196 | Time: 34.5s\n",
      "  Batch [440/781] | Loss: 2.8401 | Acc: 0.3281 | LR: 0.000196 | Time: 36.2s\n",
      "  Batch [460/781] | Loss: 3.0736 | Acc: 0.2969 | LR: 0.000196 | Time: 37.9s\n",
      "  Batch [480/781] | Loss: 3.0984 | Acc: 0.3125 | LR: 0.000196 | Time: 39.5s\n",
      "  Batch [500/781] | Loss: 3.0319 | Acc: 0.3281 | LR: 0.000196 | Time: 41.2s\n",
      "  Batch [520/781] | Loss: 2.9082 | Acc: 0.3359 | LR: 0.000196 | Time: 42.8s\n",
      "  Batch [540/781] | Loss: 2.9834 | Acc: 0.2969 | LR: 0.000196 | Time: 44.5s\n",
      "  Batch [560/781] | Loss: 3.0387 | Acc: 0.2578 | LR: 0.000196 | Time: 46.2s\n",
      "  Batch [580/781] | Loss: 3.1992 | Acc: 0.2656 | LR: 0.000196 | Time: 47.9s\n",
      "  Batch [600/781] | Loss: 3.2781 | Acc: 0.2031 | LR: 0.000196 | Time: 49.5s\n",
      "  Batch [620/781] | Loss: 2.8891 | Acc: 0.3203 | LR: 0.000196 | Time: 51.1s\n",
      "  Batch [640/781] | Loss: 3.2401 | Acc: 0.3203 | LR: 0.000196 | Time: 52.8s\n",
      "  Batch [660/781] | Loss: 3.1104 | Acc: 0.2812 | LR: 0.000196 | Time: 54.5s\n",
      "  Batch [680/781] | Loss: 2.6915 | Acc: 0.3359 | LR: 0.000196 | Time: 56.0s\n",
      "  Batch [700/781] | Loss: 2.9794 | Acc: 0.2734 | LR: 0.000196 | Time: 57.7s\n",
      "  Batch [720/781] | Loss: 2.8912 | Acc: 0.2891 | LR: 0.000196 | Time: 59.4s\n",
      "  Batch [740/781] | Loss: 2.8306 | Acc: 0.3828 | LR: 0.000196 | Time: 61.0s\n",
      "  Batch [760/781] | Loss: 3.3148 | Acc: 0.2344 | LR: 0.000196 | Time: 62.6s\n",
      "  Batch [780/781] | Loss: 2.9824 | Acc: 0.3516 | LR: 0.000196 | Time: 64.4s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.9816 | Train Acc: 0.3074\n",
      "    Val Loss:   2.9342 | Val Acc:   0.3151\n",
      "    LR: 0.000181 | Time: 67.3s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3151 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 14/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.8016 | Acc: 0.3672 | LR: 0.000181 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.8807 | Acc: 0.3516 | LR: 0.000181 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 3.0162 | Acc: 0.3047 | LR: 0.000181 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 2.9090 | Acc: 0.3438 | LR: 0.000181 | Time: 5.3s\n",
      "  Batch [ 80/781] | Loss: 2.6592 | Acc: 0.3516 | LR: 0.000181 | Time: 7.0s\n",
      "  Batch [100/781] | Loss: 3.1491 | Acc: 0.3125 | LR: 0.000181 | Time: 8.6s\n",
      "  Batch [120/781] | Loss: 3.1903 | Acc: 0.2422 | LR: 0.000181 | Time: 10.3s\n",
      "  Batch [140/781] | Loss: 3.0657 | Acc: 0.2812 | LR: 0.000181 | Time: 11.9s\n",
      "  Batch [160/781] | Loss: 3.1204 | Acc: 0.2891 | LR: 0.000181 | Time: 13.6s\n",
      "  Batch [180/781] | Loss: 2.6494 | Acc: 0.3750 | LR: 0.000181 | Time: 15.1s\n",
      "  Batch [200/781] | Loss: 2.9920 | Acc: 0.3125 | LR: 0.000181 | Time: 16.7s\n",
      "  Batch [220/781] | Loss: 3.1356 | Acc: 0.2812 | LR: 0.000181 | Time: 18.3s\n",
      "  Batch [240/781] | Loss: 3.0919 | Acc: 0.3125 | LR: 0.000181 | Time: 20.0s\n",
      "  Batch [260/781] | Loss: 2.8703 | Acc: 0.3203 | LR: 0.000181 | Time: 21.6s\n",
      "  Batch [280/781] | Loss: 3.1662 | Acc: 0.2891 | LR: 0.000181 | Time: 23.2s\n",
      "  Batch [300/781] | Loss: 2.9011 | Acc: 0.3047 | LR: 0.000181 | Time: 24.9s\n",
      "  Batch [320/781] | Loss: 2.8187 | Acc: 0.3047 | LR: 0.000181 | Time: 26.5s\n",
      "  Batch [340/781] | Loss: 2.9296 | Acc: 0.3047 | LR: 0.000181 | Time: 28.1s\n",
      "  Batch [360/781] | Loss: 3.0145 | Acc: 0.2656 | LR: 0.000181 | Time: 30.0s\n",
      "  Batch [380/781] | Loss: 3.2670 | Acc: 0.2344 | LR: 0.000181 | Time: 31.7s\n",
      "  Batch [400/781] | Loss: 2.6804 | Acc: 0.3594 | LR: 0.000181 | Time: 33.3s\n",
      "  Batch [420/781] | Loss: 2.7496 | Acc: 0.2969 | LR: 0.000181 | Time: 34.9s\n",
      "  Batch [440/781] | Loss: 2.8959 | Acc: 0.2969 | LR: 0.000181 | Time: 36.5s\n",
      "  Batch [460/781] | Loss: 2.8696 | Acc: 0.3672 | LR: 0.000181 | Time: 38.1s\n",
      "  Batch [480/781] | Loss: 2.7557 | Acc: 0.3125 | LR: 0.000181 | Time: 39.8s\n",
      "  Batch [500/781] | Loss: 2.9079 | Acc: 0.3359 | LR: 0.000181 | Time: 41.5s\n",
      "  Batch [520/781] | Loss: 2.9869 | Acc: 0.2969 | LR: 0.000181 | Time: 43.2s\n",
      "  Batch [540/781] | Loss: 2.9159 | Acc: 0.2891 | LR: 0.000181 | Time: 44.8s\n",
      "  Batch [560/781] | Loss: 3.3188 | Acc: 0.2188 | LR: 0.000181 | Time: 46.4s\n",
      "  Batch [580/781] | Loss: 3.2152 | Acc: 0.2578 | LR: 0.000181 | Time: 48.1s\n",
      "  Batch [600/781] | Loss: 2.9439 | Acc: 0.2969 | LR: 0.000181 | Time: 49.7s\n",
      "  Batch [620/781] | Loss: 2.7060 | Acc: 0.3203 | LR: 0.000181 | Time: 51.5s\n",
      "  Batch [640/781] | Loss: 3.3220 | Acc: 0.2578 | LR: 0.000181 | Time: 53.2s\n",
      "  Batch [660/781] | Loss: 2.8911 | Acc: 0.3281 | LR: 0.000181 | Time: 54.9s\n",
      "  Batch [680/781] | Loss: 2.8426 | Acc: 0.3203 | LR: 0.000181 | Time: 56.6s\n",
      "  Batch [700/781] | Loss: 2.8235 | Acc: 0.3125 | LR: 0.000181 | Time: 58.2s\n",
      "  Batch [720/781] | Loss: 2.9042 | Acc: 0.3281 | LR: 0.000181 | Time: 59.9s\n",
      "  Batch [740/781] | Loss: 2.9814 | Acc: 0.2812 | LR: 0.000181 | Time: 61.5s\n",
      "  Batch [760/781] | Loss: 2.7228 | Acc: 0.3438 | LR: 0.000181 | Time: 63.1s\n",
      "  Batch [780/781] | Loss: 2.8224 | Acc: 0.3516 | LR: 0.000181 | Time: 64.7s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.9208 | Train Acc: 0.3169\n",
      "    Val Loss:   2.8759 | Val Acc:   0.3292\n",
      "    LR: 0.000166 | Time: 67.7s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3292 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 15/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.6654 | Acc: 0.3203 | LR: 0.000166 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.8838 | Acc: 0.3281 | LR: 0.000166 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 2.7544 | Acc: 0.3125 | LR: 0.000166 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 2.9440 | Acc: 0.2969 | LR: 0.000166 | Time: 5.4s\n",
      "  Batch [ 80/781] | Loss: 2.6399 | Acc: 0.3750 | LR: 0.000166 | Time: 7.1s\n",
      "  Batch [100/781] | Loss: 2.8194 | Acc: 0.3438 | LR: 0.000166 | Time: 8.8s\n",
      "  Batch [120/781] | Loss: 2.7491 | Acc: 0.3672 | LR: 0.000166 | Time: 10.5s\n",
      "  Batch [140/781] | Loss: 2.9748 | Acc: 0.3047 | LR: 0.000166 | Time: 12.1s\n",
      "  Batch [160/781] | Loss: 3.0833 | Acc: 0.3125 | LR: 0.000166 | Time: 13.8s\n",
      "  Batch [180/781] | Loss: 2.8484 | Acc: 0.3672 | LR: 0.000166 | Time: 15.5s\n",
      "  Batch [200/781] | Loss: 3.1519 | Acc: 0.2656 | LR: 0.000166 | Time: 17.0s\n",
      "  Batch [220/781] | Loss: 3.0731 | Acc: 0.2812 | LR: 0.000166 | Time: 18.6s\n",
      "  Batch [240/781] | Loss: 2.9902 | Acc: 0.3203 | LR: 0.000166 | Time: 20.3s\n",
      "  Batch [260/781] | Loss: 3.0318 | Acc: 0.2656 | LR: 0.000166 | Time: 21.8s\n",
      "  Batch [280/781] | Loss: 2.9060 | Acc: 0.2891 | LR: 0.000166 | Time: 23.6s\n",
      "  Batch [300/781] | Loss: 2.8040 | Acc: 0.3203 | LR: 0.000166 | Time: 25.3s\n",
      "  Batch [320/781] | Loss: 2.8402 | Acc: 0.3203 | LR: 0.000166 | Time: 26.9s\n",
      "  Batch [340/781] | Loss: 2.8388 | Acc: 0.3672 | LR: 0.000166 | Time: 28.5s\n",
      "  Batch [360/781] | Loss: 3.1179 | Acc: 0.2500 | LR: 0.000166 | Time: 30.1s\n",
      "  Batch [380/781] | Loss: 2.9009 | Acc: 0.2891 | LR: 0.000166 | Time: 31.8s\n",
      "  Batch [400/781] | Loss: 2.7980 | Acc: 0.3828 | LR: 0.000166 | Time: 33.4s\n",
      "  Batch [420/781] | Loss: 2.6863 | Acc: 0.3359 | LR: 0.000166 | Time: 35.0s\n",
      "  Batch [440/781] | Loss: 2.9471 | Acc: 0.3125 | LR: 0.000166 | Time: 36.6s\n",
      "  Batch [460/781] | Loss: 2.8140 | Acc: 0.3906 | LR: 0.000166 | Time: 38.2s\n",
      "  Batch [480/781] | Loss: 2.9145 | Acc: 0.3125 | LR: 0.000166 | Time: 39.8s\n",
      "  Batch [500/781] | Loss: 2.7021 | Acc: 0.3672 | LR: 0.000166 | Time: 41.4s\n",
      "  Batch [520/781] | Loss: 2.7039 | Acc: 0.4062 | LR: 0.000166 | Time: 43.0s\n",
      "  Batch [540/781] | Loss: 2.9841 | Acc: 0.3125 | LR: 0.000166 | Time: 44.7s\n",
      "  Batch [560/781] | Loss: 2.6572 | Acc: 0.3594 | LR: 0.000166 | Time: 46.4s\n",
      "  Batch [580/781] | Loss: 2.7360 | Acc: 0.3203 | LR: 0.000166 | Time: 48.1s\n",
      "  Batch [600/781] | Loss: 2.9793 | Acc: 0.2891 | LR: 0.000166 | Time: 49.9s\n",
      "  Batch [620/781] | Loss: 2.5933 | Acc: 0.3828 | LR: 0.000166 | Time: 51.5s\n",
      "  Batch [640/781] | Loss: 2.6380 | Acc: 0.4531 | LR: 0.000166 | Time: 53.1s\n",
      "  Batch [660/781] | Loss: 2.8402 | Acc: 0.3828 | LR: 0.000166 | Time: 54.7s\n",
      "  Batch [680/781] | Loss: 2.9448 | Acc: 0.2969 | LR: 0.000166 | Time: 56.3s\n",
      "  Batch [700/781] | Loss: 2.7776 | Acc: 0.3438 | LR: 0.000166 | Time: 58.0s\n",
      "  Batch [720/781] | Loss: 2.8868 | Acc: 0.3203 | LR: 0.000166 | Time: 59.8s\n",
      "  Batch [740/781] | Loss: 2.8616 | Acc: 0.3438 | LR: 0.000166 | Time: 61.5s\n",
      "  Batch [760/781] | Loss: 2.7473 | Acc: 0.3203 | LR: 0.000166 | Time: 63.2s\n",
      "  Batch [780/781] | Loss: 3.0832 | Acc: 0.2891 | LR: 0.000166 | Time: 64.8s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.8612 | Train Acc: 0.3291\n",
      "    Val Loss:   2.8380 | Val Acc:   0.3384\n",
      "    LR: 0.000150 | Time: 67.7s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3384 ***\n",
      "Checkpoint saved: ./vit_checkpoints/checkpoint_epoch_15.pth\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 16/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.9478 | Acc: 0.3125 | LR: 0.000150 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.5804 | Acc: 0.4062 | LR: 0.000150 | Time: 1.9s\n",
      "  Batch [ 40/781] | Loss: 2.8947 | Acc: 0.2422 | LR: 0.000150 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 2.9046 | Acc: 0.3828 | LR: 0.000150 | Time: 5.2s\n",
      "  Batch [ 80/781] | Loss: 2.7293 | Acc: 0.3359 | LR: 0.000150 | Time: 6.8s\n",
      "  Batch [100/781] | Loss: 2.8079 | Acc: 0.3828 | LR: 0.000150 | Time: 8.4s\n",
      "  Batch [120/781] | Loss: 2.8291 | Acc: 0.3203 | LR: 0.000150 | Time: 10.0s\n",
      "  Batch [140/781] | Loss: 2.8822 | Acc: 0.3047 | LR: 0.000150 | Time: 11.6s\n",
      "  Batch [160/781] | Loss: 2.8402 | Acc: 0.2891 | LR: 0.000150 | Time: 13.1s\n",
      "  Batch [180/781] | Loss: 2.8833 | Acc: 0.3594 | LR: 0.000150 | Time: 14.8s\n",
      "  Batch [200/781] | Loss: 2.8530 | Acc: 0.2969 | LR: 0.000150 | Time: 16.5s\n",
      "  Batch [220/781] | Loss: 2.4801 | Acc: 0.4219 | LR: 0.000150 | Time: 18.2s\n",
      "  Batch [240/781] | Loss: 2.8461 | Acc: 0.3828 | LR: 0.000150 | Time: 19.8s\n",
      "  Batch [260/781] | Loss: 2.6695 | Acc: 0.3672 | LR: 0.000150 | Time: 21.5s\n",
      "  Batch [280/781] | Loss: 2.8138 | Acc: 0.3438 | LR: 0.000150 | Time: 23.1s\n",
      "  Batch [300/781] | Loss: 2.5679 | Acc: 0.3594 | LR: 0.000150 | Time: 24.7s\n",
      "  Batch [320/781] | Loss: 2.7480 | Acc: 0.3594 | LR: 0.000150 | Time: 26.3s\n",
      "  Batch [340/781] | Loss: 2.9495 | Acc: 0.3750 | LR: 0.000150 | Time: 27.9s\n",
      "  Batch [360/781] | Loss: 2.8535 | Acc: 0.3594 | LR: 0.000150 | Time: 29.5s\n",
      "  Batch [380/781] | Loss: 2.8261 | Acc: 0.3828 | LR: 0.000150 | Time: 31.1s\n",
      "  Batch [400/781] | Loss: 2.7393 | Acc: 0.3672 | LR: 0.000150 | Time: 32.6s\n",
      "  Batch [420/781] | Loss: 2.7036 | Acc: 0.3281 | LR: 0.000150 | Time: 34.3s\n",
      "  Batch [440/781] | Loss: 2.8031 | Acc: 0.3125 | LR: 0.000150 | Time: 35.9s\n",
      "  Batch [460/781] | Loss: 2.7951 | Acc: 0.3281 | LR: 0.000150 | Time: 37.5s\n",
      "  Batch [480/781] | Loss: 2.7916 | Acc: 0.3047 | LR: 0.000150 | Time: 39.2s\n",
      "  Batch [500/781] | Loss: 2.6267 | Acc: 0.3594 | LR: 0.000150 | Time: 40.9s\n",
      "  Batch [520/781] | Loss: 2.5152 | Acc: 0.3984 | LR: 0.000150 | Time: 42.5s\n",
      "  Batch [540/781] | Loss: 2.7023 | Acc: 0.3125 | LR: 0.000150 | Time: 44.1s\n",
      "  Batch [560/781] | Loss: 2.8670 | Acc: 0.3125 | LR: 0.000150 | Time: 45.7s\n",
      "  Batch [580/781] | Loss: 2.9980 | Acc: 0.2969 | LR: 0.000150 | Time: 47.3s\n",
      "  Batch [600/781] | Loss: 2.7545 | Acc: 0.3359 | LR: 0.000150 | Time: 48.9s\n",
      "  Batch [620/781] | Loss: 2.7005 | Acc: 0.4062 | LR: 0.000150 | Time: 50.5s\n",
      "  Batch [640/781] | Loss: 2.7894 | Acc: 0.3594 | LR: 0.000150 | Time: 52.2s\n",
      "  Batch [660/781] | Loss: 2.9400 | Acc: 0.2812 | LR: 0.000150 | Time: 53.8s\n",
      "  Batch [680/781] | Loss: 2.9342 | Acc: 0.2734 | LR: 0.000150 | Time: 55.4s\n",
      "  Batch [700/781] | Loss: 2.5218 | Acc: 0.4141 | LR: 0.000150 | Time: 56.9s\n",
      "  Batch [720/781] | Loss: 2.9644 | Acc: 0.2656 | LR: 0.000150 | Time: 58.6s\n",
      "  Batch [740/781] | Loss: 2.5232 | Acc: 0.3906 | LR: 0.000150 | Time: 60.3s\n",
      "  Batch [760/781] | Loss: 2.6385 | Acc: 0.3750 | LR: 0.000150 | Time: 61.9s\n",
      "  Batch [780/781] | Loss: 2.9223 | Acc: 0.3125 | LR: 0.000150 | Time: 63.5s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.8099 | Train Acc: 0.3392\n",
      "    Val Loss:   2.8423 | Val Acc:   0.3370\n",
      "    LR: 0.000134 | Time: 66.5s\n",
      "    Visualizing top-k tokens...\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 17/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.9751 | Acc: 0.3516 | LR: 0.000134 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.7360 | Acc: 0.3672 | LR: 0.000134 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 2.7733 | Acc: 0.3047 | LR: 0.000134 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 2.9971 | Acc: 0.3438 | LR: 0.000134 | Time: 5.2s\n",
      "  Batch [ 80/781] | Loss: 3.0430 | Acc: 0.3359 | LR: 0.000134 | Time: 6.8s\n",
      "  Batch [100/781] | Loss: 2.7071 | Acc: 0.3906 | LR: 0.000134 | Time: 8.4s\n",
      "  Batch [120/781] | Loss: 2.8276 | Acc: 0.3516 | LR: 0.000134 | Time: 10.1s\n",
      "  Batch [140/781] | Loss: 2.9559 | Acc: 0.3125 | LR: 0.000134 | Time: 11.7s\n",
      "  Batch [160/781] | Loss: 2.8991 | Acc: 0.2656 | LR: 0.000134 | Time: 13.3s\n",
      "  Batch [180/781] | Loss: 2.6547 | Acc: 0.3359 | LR: 0.000134 | Time: 15.0s\n",
      "  Batch [200/781] | Loss: 3.0359 | Acc: 0.3125 | LR: 0.000134 | Time: 16.7s\n",
      "  Batch [220/781] | Loss: 2.7464 | Acc: 0.3359 | LR: 0.000134 | Time: 18.3s\n",
      "  Batch [240/781] | Loss: 2.9794 | Acc: 0.3594 | LR: 0.000134 | Time: 19.9s\n",
      "  Batch [260/781] | Loss: 2.5581 | Acc: 0.3516 | LR: 0.000134 | Time: 21.5s\n",
      "  Batch [280/781] | Loss: 2.4931 | Acc: 0.4141 | LR: 0.000134 | Time: 23.1s\n",
      "  Batch [300/781] | Loss: 2.6251 | Acc: 0.3516 | LR: 0.000134 | Time: 24.8s\n",
      "  Batch [320/781] | Loss: 2.7443 | Acc: 0.3438 | LR: 0.000134 | Time: 26.4s\n",
      "  Batch [340/781] | Loss: 2.5199 | Acc: 0.3984 | LR: 0.000134 | Time: 28.0s\n",
      "  Batch [360/781] | Loss: 3.0106 | Acc: 0.3203 | LR: 0.000134 | Time: 29.6s\n",
      "  Batch [380/781] | Loss: 2.9134 | Acc: 0.3203 | LR: 0.000134 | Time: 31.2s\n",
      "  Batch [400/781] | Loss: 2.6786 | Acc: 0.3359 | LR: 0.000134 | Time: 33.0s\n",
      "  Batch [420/781] | Loss: 2.7299 | Acc: 0.3594 | LR: 0.000134 | Time: 34.5s\n",
      "  Batch [440/781] | Loss: 2.7108 | Acc: 0.2969 | LR: 0.000134 | Time: 36.2s\n",
      "  Batch [460/781] | Loss: 2.5408 | Acc: 0.4141 | LR: 0.000134 | Time: 37.8s\n",
      "  Batch [480/781] | Loss: 2.8096 | Acc: 0.3359 | LR: 0.000134 | Time: 39.5s\n",
      "  Batch [500/781] | Loss: 2.5641 | Acc: 0.4375 | LR: 0.000134 | Time: 41.8s\n",
      "  Batch [520/781] | Loss: 2.4540 | Acc: 0.3750 | LR: 0.000134 | Time: 43.4s\n",
      "  Batch [540/781] | Loss: 2.5647 | Acc: 0.3906 | LR: 0.000134 | Time: 45.1s\n",
      "  Batch [560/781] | Loss: 2.8025 | Acc: 0.3438 | LR: 0.000134 | Time: 46.8s\n",
      "  Batch [580/781] | Loss: 2.9030 | Acc: 0.3438 | LR: 0.000134 | Time: 48.5s\n",
      "  Batch [600/781] | Loss: 2.7521 | Acc: 0.3359 | LR: 0.000134 | Time: 50.2s\n",
      "  Batch [620/781] | Loss: 2.7541 | Acc: 0.3750 | LR: 0.000134 | Time: 51.8s\n",
      "  Batch [640/781] | Loss: 3.0580 | Acc: 0.3281 | LR: 0.000134 | Time: 53.4s\n",
      "  Batch [660/781] | Loss: 2.7994 | Acc: 0.3750 | LR: 0.000134 | Time: 55.0s\n",
      "  Batch [680/781] | Loss: 2.7113 | Acc: 0.3359 | LR: 0.000134 | Time: 56.9s\n",
      "  Batch [700/781] | Loss: 2.9622 | Acc: 0.2734 | LR: 0.000134 | Time: 58.6s\n",
      "  Batch [720/781] | Loss: 2.9934 | Acc: 0.3438 | LR: 0.000134 | Time: 60.2s\n",
      "  Batch [740/781] | Loss: 2.8080 | Acc: 0.3906 | LR: 0.000134 | Time: 61.9s\n",
      "  Batch [760/781] | Loss: 3.0737 | Acc: 0.2578 | LR: 0.000134 | Time: 63.4s\n",
      "  Batch [780/781] | Loss: 2.6566 | Acc: 0.3359 | LR: 0.000134 | Time: 65.2s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.7552 | Train Acc: 0.3489\n",
      "    Val Loss:   2.7959 | Val Acc:   0.3435\n",
      "    LR: 0.000119 | Time: 68.0s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3435 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 18/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.7698 | Acc: 0.3125 | LR: 0.000119 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.8469 | Acc: 0.3594 | LR: 0.000119 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 2.6258 | Acc: 0.4141 | LR: 0.000119 | Time: 3.7s\n",
      "  Batch [ 60/781] | Loss: 2.7541 | Acc: 0.3594 | LR: 0.000119 | Time: 5.4s\n",
      "  Batch [ 80/781] | Loss: 2.6320 | Acc: 0.3516 | LR: 0.000119 | Time: 7.1s\n",
      "  Batch [100/781] | Loss: 2.6044 | Acc: 0.3828 | LR: 0.000119 | Time: 8.7s\n",
      "  Batch [120/781] | Loss: 2.5899 | Acc: 0.3984 | LR: 0.000119 | Time: 10.3s\n",
      "  Batch [140/781] | Loss: 3.0188 | Acc: 0.2812 | LR: 0.000119 | Time: 12.0s\n",
      "  Batch [160/781] | Loss: 2.9032 | Acc: 0.2969 | LR: 0.000119 | Time: 13.7s\n",
      "  Batch [180/781] | Loss: 2.6224 | Acc: 0.3906 | LR: 0.000119 | Time: 15.2s\n",
      "  Batch [200/781] | Loss: 2.7092 | Acc: 0.3594 | LR: 0.000119 | Time: 16.9s\n",
      "  Batch [220/781] | Loss: 2.9055 | Acc: 0.3281 | LR: 0.000119 | Time: 18.5s\n",
      "  Batch [240/781] | Loss: 2.5408 | Acc: 0.3828 | LR: 0.000119 | Time: 20.1s\n",
      "  Batch [260/781] | Loss: 2.5091 | Acc: 0.4141 | LR: 0.000119 | Time: 21.7s\n",
      "  Batch [280/781] | Loss: 2.6365 | Acc: 0.4219 | LR: 0.000119 | Time: 23.2s\n",
      "  Batch [300/781] | Loss: 2.4241 | Acc: 0.4531 | LR: 0.000119 | Time: 24.8s\n",
      "  Batch [320/781] | Loss: 2.7012 | Acc: 0.3750 | LR: 0.000119 | Time: 26.5s\n",
      "  Batch [340/781] | Loss: 2.8187 | Acc: 0.3906 | LR: 0.000119 | Time: 28.2s\n",
      "  Batch [360/781] | Loss: 2.7082 | Acc: 0.3672 | LR: 0.000119 | Time: 29.8s\n",
      "  Batch [380/781] | Loss: 2.5655 | Acc: 0.3984 | LR: 0.000119 | Time: 31.5s\n",
      "  Batch [400/781] | Loss: 2.7106 | Acc: 0.3906 | LR: 0.000119 | Time: 33.2s\n",
      "  Batch [420/781] | Loss: 2.8819 | Acc: 0.3203 | LR: 0.000119 | Time: 34.8s\n",
      "  Batch [440/781] | Loss: 2.6904 | Acc: 0.3594 | LR: 0.000119 | Time: 36.4s\n",
      "  Batch [460/781] | Loss: 2.6934 | Acc: 0.3516 | LR: 0.000119 | Time: 38.1s\n",
      "  Batch [480/781] | Loss: 2.9727 | Acc: 0.2578 | LR: 0.000119 | Time: 39.7s\n",
      "  Batch [500/781] | Loss: 2.7706 | Acc: 0.3203 | LR: 0.000119 | Time: 41.3s\n",
      "  Batch [520/781] | Loss: 2.5912 | Acc: 0.3984 | LR: 0.000119 | Time: 42.9s\n",
      "  Batch [540/781] | Loss: 2.8523 | Acc: 0.3828 | LR: 0.000119 | Time: 44.5s\n",
      "  Batch [560/781] | Loss: 2.5678 | Acc: 0.4062 | LR: 0.000119 | Time: 46.1s\n",
      "  Batch [580/781] | Loss: 2.9942 | Acc: 0.3125 | LR: 0.000119 | Time: 47.7s\n",
      "  Batch [600/781] | Loss: 2.7532 | Acc: 0.3750 | LR: 0.000119 | Time: 49.4s\n",
      "  Batch [620/781] | Loss: 2.8988 | Acc: 0.3516 | LR: 0.000119 | Time: 50.9s\n",
      "  Batch [640/781] | Loss: 2.6327 | Acc: 0.3125 | LR: 0.000119 | Time: 52.6s\n",
      "  Batch [660/781] | Loss: 2.6125 | Acc: 0.4141 | LR: 0.000119 | Time: 54.3s\n",
      "  Batch [680/781] | Loss: 2.6414 | Acc: 0.3672 | LR: 0.000119 | Time: 55.9s\n",
      "  Batch [700/781] | Loss: 2.6255 | Acc: 0.3984 | LR: 0.000119 | Time: 57.5s\n",
      "  Batch [720/781] | Loss: 3.3307 | Acc: 0.2734 | LR: 0.000119 | Time: 59.1s\n",
      "  Batch [740/781] | Loss: 2.4959 | Acc: 0.3594 | LR: 0.000119 | Time: 60.8s\n",
      "  Batch [760/781] | Loss: 2.7004 | Acc: 0.3281 | LR: 0.000119 | Time: 62.4s\n",
      "  Batch [780/781] | Loss: 2.7285 | Acc: 0.3594 | LR: 0.000119 | Time: 64.0s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.6997 | Train Acc: 0.3592\n",
      "    Val Loss:   2.7837 | Val Acc:   0.3450\n",
      "    LR: 0.000104 | Time: 66.7s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3450 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 19/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.4196 | Acc: 0.3984 | LR: 0.000104 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.7004 | Acc: 0.3828 | LR: 0.000104 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 2.4801 | Acc: 0.4609 | LR: 0.000104 | Time: 3.7s\n",
      "  Batch [ 60/781] | Loss: 2.6123 | Acc: 0.3594 | LR: 0.000104 | Time: 5.3s\n",
      "  Batch [ 80/781] | Loss: 2.7272 | Acc: 0.3828 | LR: 0.000104 | Time: 6.8s\n",
      "  Batch [100/781] | Loss: 2.6429 | Acc: 0.3594 | LR: 0.000104 | Time: 8.4s\n",
      "  Batch [120/781] | Loss: 2.5621 | Acc: 0.4062 | LR: 0.000104 | Time: 10.1s\n",
      "  Batch [140/781] | Loss: 2.8405 | Acc: 0.2734 | LR: 0.000104 | Time: 11.8s\n",
      "  Batch [160/781] | Loss: 2.9659 | Acc: 0.2812 | LR: 0.000104 | Time: 13.5s\n",
      "  Batch [180/781] | Loss: 2.7930 | Acc: 0.3516 | LR: 0.000104 | Time: 15.2s\n",
      "  Batch [200/781] | Loss: 2.4789 | Acc: 0.3984 | LR: 0.000104 | Time: 16.9s\n",
      "  Batch [220/781] | Loss: 2.8584 | Acc: 0.2812 | LR: 0.000104 | Time: 18.5s\n",
      "  Batch [240/781] | Loss: 2.7610 | Acc: 0.3984 | LR: 0.000104 | Time: 20.2s\n",
      "  Batch [260/781] | Loss: 2.5242 | Acc: 0.4531 | LR: 0.000104 | Time: 21.8s\n",
      "  Batch [280/781] | Loss: 2.6802 | Acc: 0.3672 | LR: 0.000104 | Time: 23.4s\n",
      "  Batch [300/781] | Loss: 2.5054 | Acc: 0.3984 | LR: 0.000104 | Time: 25.1s\n",
      "  Batch [320/781] | Loss: 2.6353 | Acc: 0.3984 | LR: 0.000104 | Time: 26.9s\n",
      "  Batch [340/781] | Loss: 2.5932 | Acc: 0.4219 | LR: 0.000104 | Time: 28.5s\n",
      "  Batch [360/781] | Loss: 2.7521 | Acc: 0.3438 | LR: 0.000104 | Time: 30.1s\n",
      "  Batch [380/781] | Loss: 2.4669 | Acc: 0.3984 | LR: 0.000104 | Time: 31.8s\n",
      "  Batch [400/781] | Loss: 2.3876 | Acc: 0.3672 | LR: 0.000104 | Time: 33.3s\n",
      "  Batch [420/781] | Loss: 2.4694 | Acc: 0.4297 | LR: 0.000104 | Time: 34.9s\n",
      "  Batch [440/781] | Loss: 2.6043 | Acc: 0.3359 | LR: 0.000104 | Time: 36.6s\n",
      "  Batch [460/781] | Loss: 2.7052 | Acc: 0.3281 | LR: 0.000104 | Time: 38.2s\n",
      "  Batch [480/781] | Loss: 2.4073 | Acc: 0.4141 | LR: 0.000104 | Time: 39.8s\n",
      "  Batch [500/781] | Loss: 2.6196 | Acc: 0.3672 | LR: 0.000104 | Time: 41.5s\n",
      "  Batch [520/781] | Loss: 2.5744 | Acc: 0.3984 | LR: 0.000104 | Time: 43.2s\n",
      "  Batch [540/781] | Loss: 2.8114 | Acc: 0.3359 | LR: 0.000104 | Time: 44.8s\n",
      "  Batch [560/781] | Loss: 3.2505 | Acc: 0.2422 | LR: 0.000104 | Time: 46.4s\n",
      "  Batch [580/781] | Loss: 2.6152 | Acc: 0.3359 | LR: 0.000104 | Time: 48.0s\n",
      "  Batch [600/781] | Loss: 2.3926 | Acc: 0.4531 | LR: 0.000104 | Time: 49.7s\n",
      "  Batch [620/781] | Loss: 2.3947 | Acc: 0.4297 | LR: 0.000104 | Time: 51.4s\n",
      "  Batch [640/781] | Loss: 2.6660 | Acc: 0.3125 | LR: 0.000104 | Time: 53.0s\n",
      "  Batch [660/781] | Loss: 2.5110 | Acc: 0.3672 | LR: 0.000104 | Time: 54.6s\n",
      "  Batch [680/781] | Loss: 2.6382 | Acc: 0.3359 | LR: 0.000104 | Time: 56.2s\n",
      "  Batch [700/781] | Loss: 2.8603 | Acc: 0.3438 | LR: 0.000104 | Time: 57.9s\n",
      "  Batch [720/781] | Loss: 2.5721 | Acc: 0.3359 | LR: 0.000104 | Time: 59.5s\n",
      "  Batch [740/781] | Loss: 2.5645 | Acc: 0.3906 | LR: 0.000104 | Time: 61.2s\n",
      "  Batch [760/781] | Loss: 2.6718 | Acc: 0.3516 | LR: 0.000104 | Time: 62.8s\n",
      "  Batch [780/781] | Loss: 2.9977 | Acc: 0.2891 | LR: 0.000104 | Time: 64.6s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.6592 | Train Acc: 0.3667\n",
      "    Val Loss:   2.7580 | Val Acc:   0.3534\n",
      "    LR: 0.000089 | Time: 67.5s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3534 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 20/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.5643 | Acc: 0.4453 | LR: 0.000089 | Time: 0.7s\n",
      "  Batch [ 20/781] | Loss: 2.7502 | Acc: 0.3203 | LR: 0.000089 | Time: 2.5s\n",
      "  Batch [ 40/781] | Loss: 2.5411 | Acc: 0.4062 | LR: 0.000089 | Time: 4.3s\n",
      "  Batch [ 60/781] | Loss: 2.5178 | Acc: 0.3828 | LR: 0.000089 | Time: 5.7s\n",
      "  Batch [ 80/781] | Loss: 2.4991 | Acc: 0.4531 | LR: 0.000089 | Time: 7.5s\n",
      "  Batch [100/781] | Loss: 2.6138 | Acc: 0.4141 | LR: 0.000089 | Time: 9.1s\n",
      "  Batch [120/781] | Loss: 2.5120 | Acc: 0.3672 | LR: 0.000089 | Time: 10.7s\n",
      "  Batch [140/781] | Loss: 2.3870 | Acc: 0.3906 | LR: 0.000089 | Time: 12.3s\n",
      "  Batch [160/781] | Loss: 3.0488 | Acc: 0.2422 | LR: 0.000089 | Time: 14.1s\n",
      "  Batch [180/781] | Loss: 2.8647 | Acc: 0.3359 | LR: 0.000089 | Time: 15.7s\n",
      "  Batch [200/781] | Loss: 2.7639 | Acc: 0.3125 | LR: 0.000089 | Time: 17.5s\n",
      "  Batch [220/781] | Loss: 2.5231 | Acc: 0.4141 | LR: 0.000089 | Time: 19.0s\n",
      "  Batch [240/781] | Loss: 2.6820 | Acc: 0.3672 | LR: 0.000089 | Time: 20.6s\n",
      "  Batch [260/781] | Loss: 2.3711 | Acc: 0.4062 | LR: 0.000089 | Time: 22.2s\n",
      "  Batch [280/781] | Loss: 2.5775 | Acc: 0.4375 | LR: 0.000089 | Time: 24.0s\n",
      "  Batch [300/781] | Loss: 2.4228 | Acc: 0.4297 | LR: 0.000089 | Time: 25.5s\n",
      "  Batch [320/781] | Loss: 3.0302 | Acc: 0.2812 | LR: 0.000089 | Time: 27.0s\n",
      "  Batch [340/781] | Loss: 2.4591 | Acc: 0.3828 | LR: 0.000089 | Time: 28.7s\n",
      "  Batch [360/781] | Loss: 2.4695 | Acc: 0.3750 | LR: 0.000089 | Time: 30.5s\n",
      "  Batch [380/781] | Loss: 2.7868 | Acc: 0.3672 | LR: 0.000089 | Time: 32.1s\n",
      "  Batch [400/781] | Loss: 2.6389 | Acc: 0.3516 | LR: 0.000089 | Time: 33.7s\n",
      "  Batch [420/781] | Loss: 2.4918 | Acc: 0.3984 | LR: 0.000089 | Time: 35.3s\n",
      "  Batch [440/781] | Loss: 2.4523 | Acc: 0.3828 | LR: 0.000089 | Time: 36.8s\n",
      "  Batch [460/781] | Loss: 2.6954 | Acc: 0.3906 | LR: 0.000089 | Time: 38.5s\n",
      "  Batch [480/781] | Loss: 2.6703 | Acc: 0.3438 | LR: 0.000089 | Time: 40.2s\n",
      "  Batch [500/781] | Loss: 2.6868 | Acc: 0.3828 | LR: 0.000089 | Time: 41.8s\n",
      "  Batch [520/781] | Loss: 2.4391 | Acc: 0.4375 | LR: 0.000089 | Time: 43.6s\n",
      "  Batch [540/781] | Loss: 2.5835 | Acc: 0.3984 | LR: 0.000089 | Time: 45.4s\n",
      "  Batch [560/781] | Loss: 2.6383 | Acc: 0.3750 | LR: 0.000089 | Time: 46.9s\n",
      "  Batch [580/781] | Loss: 2.3415 | Acc: 0.4219 | LR: 0.000089 | Time: 48.5s\n",
      "  Batch [600/781] | Loss: 2.5303 | Acc: 0.4375 | LR: 0.000089 | Time: 50.1s\n",
      "  Batch [620/781] | Loss: 2.4063 | Acc: 0.4297 | LR: 0.000089 | Time: 51.7s\n",
      "  Batch [640/781] | Loss: 2.8399 | Acc: 0.3203 | LR: 0.000089 | Time: 53.2s\n",
      "  Batch [660/781] | Loss: 2.5152 | Acc: 0.3906 | LR: 0.000089 | Time: 54.8s\n",
      "  Batch [680/781] | Loss: 2.5615 | Acc: 0.3750 | LR: 0.000089 | Time: 56.5s\n",
      "  Batch [700/781] | Loss: 2.5619 | Acc: 0.3828 | LR: 0.000089 | Time: 58.1s\n",
      "  Batch [720/781] | Loss: 2.3269 | Acc: 0.4375 | LR: 0.000089 | Time: 59.8s\n",
      "  Batch [740/781] | Loss: 2.5760 | Acc: 0.3516 | LR: 0.000089 | Time: 61.4s\n",
      "  Batch [760/781] | Loss: 2.8064 | Acc: 0.2891 | LR: 0.000089 | Time: 62.9s\n",
      "  Batch [780/781] | Loss: 2.6411 | Acc: 0.3750 | LR: 0.000089 | Time: 64.3s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.6077 | Train Acc: 0.3760\n",
      "    Val Loss:   2.7545 | Val Acc:   0.3535\n",
      "    LR: 0.000075 | Time: 67.1s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3535 ***\n",
      "Checkpoint saved: ./vit_checkpoints/checkpoint_epoch_20.pth\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 21/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.4830 | Acc: 0.4297 | LR: 0.000075 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.6416 | Acc: 0.4062 | LR: 0.000075 | Time: 2.1s\n",
      "  Batch [ 40/781] | Loss: 2.6654 | Acc: 0.3906 | LR: 0.000075 | Time: 3.7s\n",
      "  Batch [ 60/781] | Loss: 2.5029 | Acc: 0.4297 | LR: 0.000075 | Time: 5.3s\n",
      "  Batch [ 80/781] | Loss: 2.5324 | Acc: 0.3906 | LR: 0.000075 | Time: 7.0s\n",
      "  Batch [100/781] | Loss: 2.5480 | Acc: 0.3984 | LR: 0.000075 | Time: 8.5s\n",
      "  Batch [120/781] | Loss: 2.6056 | Acc: 0.3828 | LR: 0.000075 | Time: 10.1s\n",
      "  Batch [140/781] | Loss: 2.5548 | Acc: 0.3516 | LR: 0.000075 | Time: 11.8s\n",
      "  Batch [160/781] | Loss: 2.2315 | Acc: 0.4141 | LR: 0.000075 | Time: 13.3s\n",
      "  Batch [180/781] | Loss: 2.3298 | Acc: 0.3750 | LR: 0.000075 | Time: 14.9s\n",
      "  Batch [200/781] | Loss: 2.6081 | Acc: 0.4531 | LR: 0.000075 | Time: 16.6s\n",
      "  Batch [220/781] | Loss: 2.4074 | Acc: 0.4062 | LR: 0.000075 | Time: 18.2s\n",
      "  Batch [240/781] | Loss: 2.4812 | Acc: 0.3750 | LR: 0.000075 | Time: 19.8s\n",
      "  Batch [260/781] | Loss: 2.4720 | Acc: 0.3906 | LR: 0.000075 | Time: 21.4s\n",
      "  Batch [280/781] | Loss: 2.3434 | Acc: 0.4219 | LR: 0.000075 | Time: 23.0s\n",
      "  Batch [300/781] | Loss: 2.7358 | Acc: 0.4062 | LR: 0.000075 | Time: 24.8s\n",
      "  Batch [320/781] | Loss: 2.5222 | Acc: 0.4062 | LR: 0.000075 | Time: 26.5s\n",
      "  Batch [340/781] | Loss: 2.3371 | Acc: 0.3984 | LR: 0.000075 | Time: 28.0s\n",
      "  Batch [360/781] | Loss: 2.5394 | Acc: 0.4219 | LR: 0.000075 | Time: 29.7s\n",
      "  Batch [380/781] | Loss: 2.7858 | Acc: 0.2812 | LR: 0.000075 | Time: 31.3s\n",
      "  Batch [400/781] | Loss: 2.6019 | Acc: 0.3984 | LR: 0.000075 | Time: 32.8s\n",
      "  Batch [420/781] | Loss: 2.7047 | Acc: 0.3750 | LR: 0.000075 | Time: 34.4s\n",
      "  Batch [440/781] | Loss: 2.7081 | Acc: 0.3594 | LR: 0.000075 | Time: 36.1s\n",
      "  Batch [460/781] | Loss: 2.8265 | Acc: 0.2891 | LR: 0.000075 | Time: 37.8s\n",
      "  Batch [480/781] | Loss: 2.5026 | Acc: 0.3984 | LR: 0.000075 | Time: 39.3s\n",
      "  Batch [500/781] | Loss: 2.4036 | Acc: 0.4062 | LR: 0.000075 | Time: 41.0s\n",
      "  Batch [520/781] | Loss: 2.6886 | Acc: 0.3750 | LR: 0.000075 | Time: 42.5s\n",
      "  Batch [540/781] | Loss: 2.6564 | Acc: 0.3750 | LR: 0.000075 | Time: 44.2s\n",
      "  Batch [560/781] | Loss: 2.7593 | Acc: 0.3594 | LR: 0.000075 | Time: 45.7s\n",
      "  Batch [580/781] | Loss: 2.6629 | Acc: 0.3750 | LR: 0.000075 | Time: 47.4s\n",
      "  Batch [600/781] | Loss: 2.7021 | Acc: 0.3203 | LR: 0.000075 | Time: 49.0s\n",
      "  Batch [620/781] | Loss: 2.7340 | Acc: 0.3203 | LR: 0.000075 | Time: 50.7s\n",
      "  Batch [640/781] | Loss: 2.3632 | Acc: 0.4297 | LR: 0.000075 | Time: 52.4s\n",
      "  Batch [660/781] | Loss: 2.5444 | Acc: 0.4141 | LR: 0.000075 | Time: 53.9s\n",
      "  Batch [680/781] | Loss: 2.5226 | Acc: 0.3672 | LR: 0.000075 | Time: 55.7s\n",
      "  Batch [700/781] | Loss: 2.6142 | Acc: 0.3906 | LR: 0.000075 | Time: 57.3s\n",
      "  Batch [720/781] | Loss: 2.5589 | Acc: 0.3594 | LR: 0.000075 | Time: 59.0s\n",
      "  Batch [740/781] | Loss: 2.7698 | Acc: 0.3359 | LR: 0.000075 | Time: 60.5s\n",
      "  Batch [760/781] | Loss: 2.4530 | Acc: 0.4297 | LR: 0.000075 | Time: 62.3s\n",
      "  Batch [780/781] | Loss: 2.5713 | Acc: 0.3828 | LR: 0.000075 | Time: 63.7s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.5665 | Train Acc: 0.3840\n",
      "    Val Loss:   2.7240 | Val Acc:   0.3623\n",
      "    LR: 0.000062 | Time: 66.4s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3623 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 22/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.5328 | Acc: 0.3672 | LR: 0.000062 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.4883 | Acc: 0.3828 | LR: 0.000062 | Time: 2.1s\n",
      "  Batch [ 40/781] | Loss: 2.5635 | Acc: 0.3906 | LR: 0.000062 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 2.5132 | Acc: 0.4141 | LR: 0.000062 | Time: 5.4s\n",
      "  Batch [ 80/781] | Loss: 2.5154 | Acc: 0.3594 | LR: 0.000062 | Time: 7.0s\n",
      "  Batch [100/781] | Loss: 2.8041 | Acc: 0.3438 | LR: 0.000062 | Time: 8.7s\n",
      "  Batch [120/781] | Loss: 2.8278 | Acc: 0.3281 | LR: 0.000062 | Time: 10.4s\n",
      "  Batch [140/781] | Loss: 2.6454 | Acc: 0.3594 | LR: 0.000062 | Time: 12.1s\n",
      "  Batch [160/781] | Loss: 2.4383 | Acc: 0.3984 | LR: 0.000062 | Time: 13.6s\n",
      "  Batch [180/781] | Loss: 2.8141 | Acc: 0.3594 | LR: 0.000062 | Time: 15.2s\n",
      "  Batch [200/781] | Loss: 2.5764 | Acc: 0.3438 | LR: 0.000062 | Time: 16.7s\n",
      "  Batch [220/781] | Loss: 2.2928 | Acc: 0.4531 | LR: 0.000062 | Time: 18.4s\n",
      "  Batch [240/781] | Loss: 2.5752 | Acc: 0.3672 | LR: 0.000062 | Time: 19.9s\n",
      "  Batch [260/781] | Loss: 2.4027 | Acc: 0.3750 | LR: 0.000062 | Time: 21.5s\n",
      "  Batch [280/781] | Loss: 2.4086 | Acc: 0.3906 | LR: 0.000062 | Time: 23.1s\n",
      "  Batch [300/781] | Loss: 2.4023 | Acc: 0.4297 | LR: 0.000062 | Time: 24.7s\n",
      "  Batch [320/781] | Loss: 2.7480 | Acc: 0.3906 | LR: 0.000062 | Time: 26.5s\n",
      "  Batch [340/781] | Loss: 2.6968 | Acc: 0.3750 | LR: 0.000062 | Time: 28.0s\n",
      "  Batch [360/781] | Loss: 2.3708 | Acc: 0.4141 | LR: 0.000062 | Time: 29.6s\n",
      "  Batch [380/781] | Loss: 2.3832 | Acc: 0.3750 | LR: 0.000062 | Time: 31.4s\n",
      "  Batch [400/781] | Loss: 2.3421 | Acc: 0.4375 | LR: 0.000062 | Time: 33.2s\n",
      "  Batch [420/781] | Loss: 2.4155 | Acc: 0.3984 | LR: 0.000062 | Time: 34.7s\n",
      "  Batch [440/781] | Loss: 2.5361 | Acc: 0.4141 | LR: 0.000062 | Time: 36.4s\n",
      "  Batch [460/781] | Loss: 2.4702 | Acc: 0.4141 | LR: 0.000062 | Time: 38.1s\n",
      "  Batch [480/781] | Loss: 2.4533 | Acc: 0.3984 | LR: 0.000062 | Time: 39.7s\n",
      "  Batch [500/781] | Loss: 2.4958 | Acc: 0.3516 | LR: 0.000062 | Time: 41.4s\n",
      "  Batch [520/781] | Loss: 2.5567 | Acc: 0.3359 | LR: 0.000062 | Time: 43.0s\n",
      "  Batch [540/781] | Loss: 2.6342 | Acc: 0.3906 | LR: 0.000062 | Time: 44.5s\n",
      "  Batch [560/781] | Loss: 2.1828 | Acc: 0.4375 | LR: 0.000062 | Time: 46.1s\n",
      "  Batch [580/781] | Loss: 2.5445 | Acc: 0.3672 | LR: 0.000062 | Time: 47.8s\n",
      "  Batch [600/781] | Loss: 2.4510 | Acc: 0.4141 | LR: 0.000062 | Time: 49.3s\n",
      "  Batch [620/781] | Loss: 2.4124 | Acc: 0.4609 | LR: 0.000062 | Time: 51.1s\n",
      "  Batch [640/781] | Loss: 2.5310 | Acc: 0.3750 | LR: 0.000062 | Time: 52.7s\n",
      "  Batch [660/781] | Loss: 2.7171 | Acc: 0.3516 | LR: 0.000062 | Time: 54.2s\n",
      "  Batch [680/781] | Loss: 2.4859 | Acc: 0.4141 | LR: 0.000062 | Time: 56.0s\n",
      "  Batch [700/781] | Loss: 2.4511 | Acc: 0.3828 | LR: 0.000062 | Time: 57.7s\n",
      "  Batch [720/781] | Loss: 2.6525 | Acc: 0.3984 | LR: 0.000062 | Time: 59.4s\n",
      "  Batch [740/781] | Loss: 2.4035 | Acc: 0.3984 | LR: 0.000062 | Time: 61.0s\n",
      "  Batch [760/781] | Loss: 2.6734 | Acc: 0.3984 | LR: 0.000062 | Time: 62.7s\n",
      "  Batch [780/781] | Loss: 2.8530 | Acc: 0.3281 | LR: 0.000062 | Time: 64.2s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.5317 | Train Acc: 0.3911\n",
      "    Val Loss:   2.7218 | Val Acc:   0.3647\n",
      "    LR: 0.000050 | Time: 67.0s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3647 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 23/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.5171 | Acc: 0.3359 | LR: 0.000050 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.8558 | Acc: 0.3125 | LR: 0.000050 | Time: 2.1s\n",
      "  Batch [ 40/781] | Loss: 2.2679 | Acc: 0.4453 | LR: 0.000050 | Time: 3.7s\n",
      "  Batch [ 60/781] | Loss: 2.2100 | Acc: 0.4609 | LR: 0.000050 | Time: 5.5s\n",
      "  Batch [ 80/781] | Loss: 2.3636 | Acc: 0.3984 | LR: 0.000050 | Time: 7.1s\n",
      "  Batch [100/781] | Loss: 2.4080 | Acc: 0.3750 | LR: 0.000050 | Time: 8.7s\n",
      "  Batch [120/781] | Loss: 2.5300 | Acc: 0.3750 | LR: 0.000050 | Time: 10.3s\n",
      "  Batch [140/781] | Loss: 2.5708 | Acc: 0.3828 | LR: 0.000050 | Time: 11.9s\n",
      "  Batch [160/781] | Loss: 2.5806 | Acc: 0.3672 | LR: 0.000050 | Time: 13.5s\n",
      "  Batch [180/781] | Loss: 2.2939 | Acc: 0.4531 | LR: 0.000050 | Time: 15.1s\n",
      "  Batch [200/781] | Loss: 2.5207 | Acc: 0.4141 | LR: 0.000050 | Time: 16.7s\n",
      "  Batch [220/781] | Loss: 2.4983 | Acc: 0.4453 | LR: 0.000050 | Time: 18.3s\n",
      "  Batch [240/781] | Loss: 2.3782 | Acc: 0.3906 | LR: 0.000050 | Time: 19.9s\n",
      "  Batch [260/781] | Loss: 2.4968 | Acc: 0.3984 | LR: 0.000050 | Time: 21.6s\n",
      "  Batch [280/781] | Loss: 2.5858 | Acc: 0.3516 | LR: 0.000050 | Time: 23.1s\n",
      "  Batch [300/781] | Loss: 2.4548 | Acc: 0.4219 | LR: 0.000050 | Time: 24.8s\n",
      "  Batch [320/781] | Loss: 2.4451 | Acc: 0.4219 | LR: 0.000050 | Time: 26.4s\n",
      "  Batch [340/781] | Loss: 2.3349 | Acc: 0.4297 | LR: 0.000050 | Time: 28.1s\n",
      "  Batch [360/781] | Loss: 2.2864 | Acc: 0.4531 | LR: 0.000050 | Time: 29.8s\n",
      "  Batch [380/781] | Loss: 2.4843 | Acc: 0.3906 | LR: 0.000050 | Time: 31.5s\n",
      "  Batch [400/781] | Loss: 2.4491 | Acc: 0.4375 | LR: 0.000050 | Time: 33.1s\n",
      "  Batch [420/781] | Loss: 2.6041 | Acc: 0.3594 | LR: 0.000050 | Time: 34.8s\n",
      "  Batch [440/781] | Loss: 2.5122 | Acc: 0.3906 | LR: 0.000050 | Time: 36.4s\n",
      "  Batch [460/781] | Loss: 2.5684 | Acc: 0.3594 | LR: 0.000050 | Time: 38.1s\n",
      "  Batch [480/781] | Loss: 2.2606 | Acc: 0.4609 | LR: 0.000050 | Time: 39.7s\n",
      "  Batch [500/781] | Loss: 2.4787 | Acc: 0.3438 | LR: 0.000050 | Time: 41.3s\n",
      "  Batch [520/781] | Loss: 2.2632 | Acc: 0.4219 | LR: 0.000050 | Time: 42.9s\n",
      "  Batch [540/781] | Loss: 2.4082 | Acc: 0.3828 | LR: 0.000050 | Time: 44.5s\n",
      "  Batch [560/781] | Loss: 2.3873 | Acc: 0.3828 | LR: 0.000050 | Time: 46.1s\n",
      "  Batch [580/781] | Loss: 2.4302 | Acc: 0.4062 | LR: 0.000050 | Time: 47.6s\n",
      "  Batch [600/781] | Loss: 2.5579 | Acc: 0.3750 | LR: 0.000050 | Time: 49.3s\n",
      "  Batch [620/781] | Loss: 2.7655 | Acc: 0.2891 | LR: 0.000050 | Time: 50.9s\n",
      "  Batch [640/781] | Loss: 2.8387 | Acc: 0.3438 | LR: 0.000050 | Time: 52.5s\n",
      "  Batch [660/781] | Loss: 2.4727 | Acc: 0.3672 | LR: 0.000050 | Time: 54.1s\n",
      "  Batch [680/781] | Loss: 2.4886 | Acc: 0.4219 | LR: 0.000050 | Time: 55.6s\n",
      "  Batch [700/781] | Loss: 2.1909 | Acc: 0.4609 | LR: 0.000050 | Time: 57.2s\n",
      "  Batch [720/781] | Loss: 2.3247 | Acc: 0.4453 | LR: 0.000050 | Time: 58.7s\n",
      "  Batch [740/781] | Loss: 2.5777 | Acc: 0.3438 | LR: 0.000050 | Time: 60.3s\n",
      "  Batch [760/781] | Loss: 2.4815 | Acc: 0.4141 | LR: 0.000050 | Time: 61.9s\n",
      "  Batch [780/781] | Loss: 2.7847 | Acc: 0.3672 | LR: 0.000050 | Time: 63.6s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.4925 | Train Acc: 0.3988\n",
      "    Val Loss:   2.7157 | Val Acc:   0.3617\n",
      "    LR: 0.000039 | Time: 66.4s\n",
      "    Visualizing top-k tokens...\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 24/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.3484 | Acc: 0.4375 | LR: 0.000039 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.2284 | Acc: 0.4453 | LR: 0.000039 | Time: 1.9s\n",
      "  Batch [ 40/781] | Loss: 2.7448 | Acc: 0.3438 | LR: 0.000039 | Time: 3.7s\n",
      "  Batch [ 60/781] | Loss: 2.6214 | Acc: 0.3672 | LR: 0.000039 | Time: 5.3s\n",
      "  Batch [ 80/781] | Loss: 2.4323 | Acc: 0.3750 | LR: 0.000039 | Time: 7.0s\n",
      "  Batch [100/781] | Loss: 2.3182 | Acc: 0.3516 | LR: 0.000039 | Time: 8.6s\n",
      "  Batch [120/781] | Loss: 2.5548 | Acc: 0.4375 | LR: 0.000039 | Time: 10.2s\n",
      "  Batch [140/781] | Loss: 2.5546 | Acc: 0.3438 | LR: 0.000039 | Time: 11.8s\n",
      "  Batch [160/781] | Loss: 2.4704 | Acc: 0.3672 | LR: 0.000039 | Time: 13.4s\n",
      "  Batch [180/781] | Loss: 2.8344 | Acc: 0.3203 | LR: 0.000039 | Time: 15.0s\n",
      "  Batch [200/781] | Loss: 2.1098 | Acc: 0.4531 | LR: 0.000039 | Time: 16.6s\n",
      "  Batch [220/781] | Loss: 2.2374 | Acc: 0.4688 | LR: 0.000039 | Time: 18.1s\n",
      "  Batch [240/781] | Loss: 2.1980 | Acc: 0.4297 | LR: 0.000039 | Time: 19.8s\n",
      "  Batch [260/781] | Loss: 2.5634 | Acc: 0.3750 | LR: 0.000039 | Time: 21.5s\n",
      "  Batch [280/781] | Loss: 2.5614 | Acc: 0.3359 | LR: 0.000039 | Time: 23.1s\n",
      "  Batch [300/781] | Loss: 2.5876 | Acc: 0.3984 | LR: 0.000039 | Time: 24.7s\n",
      "  Batch [320/781] | Loss: 2.4886 | Acc: 0.4062 | LR: 0.000039 | Time: 26.4s\n",
      "  Batch [340/781] | Loss: 2.8093 | Acc: 0.3516 | LR: 0.000039 | Time: 28.0s\n",
      "  Batch [360/781] | Loss: 2.6596 | Acc: 0.3984 | LR: 0.000039 | Time: 29.5s\n",
      "  Batch [380/781] | Loss: 2.3952 | Acc: 0.4297 | LR: 0.000039 | Time: 31.1s\n",
      "  Batch [400/781] | Loss: 2.3099 | Acc: 0.4297 | LR: 0.000039 | Time: 32.7s\n",
      "  Batch [420/781] | Loss: 2.6932 | Acc: 0.3359 | LR: 0.000039 | Time: 34.4s\n",
      "  Batch [440/781] | Loss: 2.6545 | Acc: 0.3828 | LR: 0.000039 | Time: 36.0s\n",
      "  Batch [460/781] | Loss: 2.5360 | Acc: 0.3672 | LR: 0.000039 | Time: 37.6s\n",
      "  Batch [480/781] | Loss: 2.3277 | Acc: 0.4297 | LR: 0.000039 | Time: 39.2s\n",
      "  Batch [500/781] | Loss: 2.2783 | Acc: 0.4375 | LR: 0.000039 | Time: 40.9s\n",
      "  Batch [520/781] | Loss: 2.6559 | Acc: 0.4141 | LR: 0.000039 | Time: 42.5s\n",
      "  Batch [540/781] | Loss: 2.7001 | Acc: 0.3359 | LR: 0.000039 | Time: 44.2s\n",
      "  Batch [560/781] | Loss: 2.2078 | Acc: 0.4453 | LR: 0.000039 | Time: 45.8s\n",
      "  Batch [580/781] | Loss: 2.3663 | Acc: 0.4375 | LR: 0.000039 | Time: 47.5s\n",
      "  Batch [600/781] | Loss: 2.6317 | Acc: 0.3984 | LR: 0.000039 | Time: 49.1s\n",
      "  Batch [620/781] | Loss: 2.6932 | Acc: 0.3828 | LR: 0.000039 | Time: 50.8s\n",
      "  Batch [640/781] | Loss: 2.4695 | Acc: 0.3438 | LR: 0.000039 | Time: 52.4s\n",
      "  Batch [660/781] | Loss: 2.3612 | Acc: 0.4219 | LR: 0.000039 | Time: 54.0s\n",
      "  Batch [680/781] | Loss: 2.6387 | Acc: 0.3594 | LR: 0.000039 | Time: 55.7s\n",
      "  Batch [700/781] | Loss: 2.5453 | Acc: 0.3906 | LR: 0.000039 | Time: 57.3s\n",
      "  Batch [720/781] | Loss: 2.3814 | Acc: 0.4453 | LR: 0.000039 | Time: 58.9s\n",
      "  Batch [740/781] | Loss: 2.4639 | Acc: 0.3750 | LR: 0.000039 | Time: 60.5s\n",
      "  Batch [760/781] | Loss: 2.5326 | Acc: 0.3984 | LR: 0.000039 | Time: 62.1s\n",
      "  Batch [780/781] | Loss: 2.2251 | Acc: 0.4609 | LR: 0.000039 | Time: 63.8s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.4677 | Train Acc: 0.4018\n",
      "    Val Loss:   2.7129 | Val Acc:   0.3636\n",
      "    LR: 0.000029 | Time: 66.7s\n",
      "    Visualizing top-k tokens...\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 25/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.3291 | Acc: 0.4375 | LR: 0.000029 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.7254 | Acc: 0.3828 | LR: 0.000029 | Time: 1.9s\n",
      "  Batch [ 40/781] | Loss: 2.3498 | Acc: 0.3906 | LR: 0.000029 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 2.3073 | Acc: 0.4453 | LR: 0.000029 | Time: 5.3s\n",
      "  Batch [ 80/781] | Loss: 2.4231 | Acc: 0.4453 | LR: 0.000029 | Time: 7.0s\n",
      "  Batch [100/781] | Loss: 2.6990 | Acc: 0.3438 | LR: 0.000029 | Time: 8.6s\n",
      "  Batch [120/781] | Loss: 2.4087 | Acc: 0.4297 | LR: 0.000029 | Time: 10.2s\n",
      "  Batch [140/781] | Loss: 2.2915 | Acc: 0.4453 | LR: 0.000029 | Time: 11.9s\n",
      "  Batch [160/781] | Loss: 2.4707 | Acc: 0.4297 | LR: 0.000029 | Time: 13.4s\n",
      "  Batch [180/781] | Loss: 2.3635 | Acc: 0.4297 | LR: 0.000029 | Time: 15.1s\n",
      "  Batch [200/781] | Loss: 2.4776 | Acc: 0.4219 | LR: 0.000029 | Time: 16.6s\n",
      "  Batch [220/781] | Loss: 2.3517 | Acc: 0.4375 | LR: 0.000029 | Time: 18.2s\n",
      "  Batch [240/781] | Loss: 2.5034 | Acc: 0.3906 | LR: 0.000029 | Time: 19.8s\n",
      "  Batch [260/781] | Loss: 2.1087 | Acc: 0.4844 | LR: 0.000029 | Time: 21.4s\n",
      "  Batch [280/781] | Loss: 2.4546 | Acc: 0.3906 | LR: 0.000029 | Time: 23.0s\n",
      "  Batch [300/781] | Loss: 2.6859 | Acc: 0.3984 | LR: 0.000029 | Time: 24.7s\n",
      "  Batch [320/781] | Loss: 2.5444 | Acc: 0.4375 | LR: 0.000029 | Time: 26.3s\n",
      "  Batch [340/781] | Loss: 2.3122 | Acc: 0.4219 | LR: 0.000029 | Time: 27.9s\n",
      "  Batch [360/781] | Loss: 2.6286 | Acc: 0.3828 | LR: 0.000029 | Time: 29.5s\n",
      "  Batch [380/781] | Loss: 2.3541 | Acc: 0.4297 | LR: 0.000029 | Time: 31.3s\n",
      "  Batch [400/781] | Loss: 2.3799 | Acc: 0.4219 | LR: 0.000029 | Time: 32.9s\n",
      "  Batch [420/781] | Loss: 2.5023 | Acc: 0.4297 | LR: 0.000029 | Time: 34.6s\n",
      "  Batch [440/781] | Loss: 2.6794 | Acc: 0.3672 | LR: 0.000029 | Time: 36.2s\n",
      "  Batch [460/781] | Loss: 2.4186 | Acc: 0.3984 | LR: 0.000029 | Time: 37.8s\n",
      "  Batch [480/781] | Loss: 2.6669 | Acc: 0.3750 | LR: 0.000029 | Time: 39.4s\n",
      "  Batch [500/781] | Loss: 2.3965 | Acc: 0.3906 | LR: 0.000029 | Time: 41.1s\n",
      "  Batch [520/781] | Loss: 2.8291 | Acc: 0.2891 | LR: 0.000029 | Time: 42.7s\n",
      "  Batch [540/781] | Loss: 2.4141 | Acc: 0.4219 | LR: 0.000029 | Time: 44.2s\n",
      "  Batch [560/781] | Loss: 2.5697 | Acc: 0.3594 | LR: 0.000029 | Time: 45.8s\n",
      "  Batch [580/781] | Loss: 2.6608 | Acc: 0.3750 | LR: 0.000029 | Time: 47.4s\n",
      "  Batch [600/781] | Loss: 2.4380 | Acc: 0.3828 | LR: 0.000029 | Time: 49.0s\n",
      "  Batch [620/781] | Loss: 2.3838 | Acc: 0.4219 | LR: 0.000029 | Time: 50.7s\n",
      "  Batch [640/781] | Loss: 2.1584 | Acc: 0.4609 | LR: 0.000029 | Time: 52.3s\n",
      "  Batch [660/781] | Loss: 2.3424 | Acc: 0.4531 | LR: 0.000029 | Time: 54.0s\n",
      "  Batch [680/781] | Loss: 2.4058 | Acc: 0.4453 | LR: 0.000029 | Time: 55.6s\n",
      "  Batch [700/781] | Loss: 2.2943 | Acc: 0.4531 | LR: 0.000029 | Time: 57.3s\n",
      "  Batch [720/781] | Loss: 2.3183 | Acc: 0.4531 | LR: 0.000029 | Time: 58.8s\n",
      "  Batch [740/781] | Loss: 2.4597 | Acc: 0.3906 | LR: 0.000029 | Time: 60.5s\n",
      "  Batch [760/781] | Loss: 2.5681 | Acc: 0.3516 | LR: 0.000029 | Time: 62.2s\n",
      "  Batch [780/781] | Loss: 2.5356 | Acc: 0.4141 | LR: 0.000029 | Time: 63.7s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.4376 | Train Acc: 0.4111\n",
      "    Val Loss:   2.6947 | Val Acc:   0.3702\n",
      "    LR: 0.000020 | Time: 66.6s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3702 ***\n",
      "Checkpoint saved: ./vit_checkpoints/checkpoint_epoch_25.pth\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 26/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.1756 | Acc: 0.4844 | LR: 0.000020 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.7471 | Acc: 0.3906 | LR: 0.000020 | Time: 1.9s\n",
      "  Batch [ 40/781] | Loss: 2.4585 | Acc: 0.3828 | LR: 0.000020 | Time: 3.5s\n",
      "  Batch [ 60/781] | Loss: 2.2004 | Acc: 0.4219 | LR: 0.000020 | Time: 5.2s\n",
      "  Batch [ 80/781] | Loss: 2.5801 | Acc: 0.3750 | LR: 0.000020 | Time: 6.8s\n",
      "  Batch [100/781] | Loss: 2.3372 | Acc: 0.4062 | LR: 0.000020 | Time: 8.4s\n",
      "  Batch [120/781] | Loss: 2.5966 | Acc: 0.3906 | LR: 0.000020 | Time: 10.1s\n",
      "  Batch [140/781] | Loss: 2.3939 | Acc: 0.4375 | LR: 0.000020 | Time: 11.8s\n",
      "  Batch [160/781] | Loss: 2.4607 | Acc: 0.3594 | LR: 0.000020 | Time: 13.5s\n",
      "  Batch [180/781] | Loss: 2.1062 | Acc: 0.5000 | LR: 0.000020 | Time: 15.2s\n",
      "  Batch [200/781] | Loss: 2.4052 | Acc: 0.3672 | LR: 0.000020 | Time: 16.7s\n",
      "  Batch [220/781] | Loss: 2.3974 | Acc: 0.3984 | LR: 0.000020 | Time: 18.4s\n",
      "  Batch [240/781] | Loss: 2.1694 | Acc: 0.4844 | LR: 0.000020 | Time: 19.9s\n",
      "  Batch [260/781] | Loss: 2.4630 | Acc: 0.3750 | LR: 0.000020 | Time: 21.6s\n",
      "  Batch [280/781] | Loss: 1.9979 | Acc: 0.4844 | LR: 0.000020 | Time: 23.1s\n",
      "  Batch [300/781] | Loss: 2.2918 | Acc: 0.4453 | LR: 0.000020 | Time: 24.7s\n",
      "  Batch [320/781] | Loss: 2.2874 | Acc: 0.4453 | LR: 0.000020 | Time: 26.3s\n",
      "  Batch [340/781] | Loss: 2.6704 | Acc: 0.3672 | LR: 0.000020 | Time: 27.9s\n",
      "  Batch [360/781] | Loss: 2.4607 | Acc: 0.4141 | LR: 0.000020 | Time: 29.7s\n",
      "  Batch [380/781] | Loss: 2.3818 | Acc: 0.4219 | LR: 0.000020 | Time: 31.2s\n",
      "  Batch [400/781] | Loss: 2.2610 | Acc: 0.4297 | LR: 0.000020 | Time: 32.9s\n",
      "  Batch [420/781] | Loss: 2.5477 | Acc: 0.4531 | LR: 0.000020 | Time: 34.6s\n",
      "  Batch [440/781] | Loss: 2.6155 | Acc: 0.3125 | LR: 0.000020 | Time: 36.2s\n",
      "  Batch [460/781] | Loss: 2.4735 | Acc: 0.4219 | LR: 0.000020 | Time: 37.9s\n",
      "  Batch [480/781] | Loss: 2.5001 | Acc: 0.3906 | LR: 0.000020 | Time: 39.5s\n",
      "  Batch [500/781] | Loss: 2.3136 | Acc: 0.4297 | LR: 0.000020 | Time: 41.1s\n",
      "  Batch [520/781] | Loss: 2.4673 | Acc: 0.4297 | LR: 0.000020 | Time: 42.8s\n",
      "  Batch [540/781] | Loss: 2.3480 | Acc: 0.5000 | LR: 0.000020 | Time: 44.4s\n",
      "  Batch [560/781] | Loss: 2.3759 | Acc: 0.4141 | LR: 0.000020 | Time: 46.0s\n",
      "  Batch [580/781] | Loss: 2.7020 | Acc: 0.3203 | LR: 0.000020 | Time: 47.8s\n",
      "  Batch [600/781] | Loss: 2.3322 | Acc: 0.4297 | LR: 0.000020 | Time: 49.4s\n",
      "  Batch [620/781] | Loss: 2.2681 | Acc: 0.3984 | LR: 0.000020 | Time: 51.1s\n",
      "  Batch [640/781] | Loss: 2.3441 | Acc: 0.4219 | LR: 0.000020 | Time: 52.8s\n",
      "  Batch [660/781] | Loss: 2.2104 | Acc: 0.4531 | LR: 0.000020 | Time: 54.4s\n",
      "  Batch [680/781] | Loss: 2.4498 | Acc: 0.4219 | LR: 0.000020 | Time: 56.1s\n",
      "  Batch [700/781] | Loss: 2.3829 | Acc: 0.3828 | LR: 0.000020 | Time: 57.8s\n",
      "  Batch [720/781] | Loss: 2.4873 | Acc: 0.3984 | LR: 0.000020 | Time: 59.5s\n",
      "  Batch [740/781] | Loss: 2.6690 | Acc: 0.3281 | LR: 0.000020 | Time: 61.1s\n",
      "  Batch [760/781] | Loss: 2.2835 | Acc: 0.4453 | LR: 0.000020 | Time: 62.8s\n",
      "  Batch [780/781] | Loss: 2.3744 | Acc: 0.3828 | LR: 0.000020 | Time: 64.5s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.4117 | Train Acc: 0.4142\n",
      "    Val Loss:   2.6945 | Val Acc:   0.3704\n",
      "    LR: 0.000013 | Time: 67.2s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3704 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 27/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.5960 | Acc: 0.3516 | LR: 0.000013 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.6049 | Acc: 0.3438 | LR: 0.000013 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 2.3538 | Acc: 0.4062 | LR: 0.000013 | Time: 3.7s\n",
      "  Batch [ 60/781] | Loss: 2.4739 | Acc: 0.3906 | LR: 0.000013 | Time: 5.3s\n",
      "  Batch [ 80/781] | Loss: 2.2406 | Acc: 0.4922 | LR: 0.000013 | Time: 6.9s\n",
      "  Batch [100/781] | Loss: 2.1619 | Acc: 0.4297 | LR: 0.000013 | Time: 8.5s\n",
      "  Batch [120/781] | Loss: 2.3861 | Acc: 0.4141 | LR: 0.000013 | Time: 10.1s\n",
      "  Batch [140/781] | Loss: 2.6726 | Acc: 0.4219 | LR: 0.000013 | Time: 11.8s\n",
      "  Batch [160/781] | Loss: 2.2321 | Acc: 0.3906 | LR: 0.000013 | Time: 13.3s\n",
      "  Batch [180/781] | Loss: 2.2570 | Acc: 0.4297 | LR: 0.000013 | Time: 14.9s\n",
      "  Batch [200/781] | Loss: 2.5320 | Acc: 0.4062 | LR: 0.000013 | Time: 16.6s\n",
      "  Batch [220/781] | Loss: 2.3950 | Acc: 0.3750 | LR: 0.000013 | Time: 18.3s\n",
      "  Batch [240/781] | Loss: 2.3911 | Acc: 0.4531 | LR: 0.000013 | Time: 19.9s\n",
      "  Batch [260/781] | Loss: 2.2938 | Acc: 0.3984 | LR: 0.000013 | Time: 21.5s\n",
      "  Batch [280/781] | Loss: 2.3565 | Acc: 0.4062 | LR: 0.000013 | Time: 23.1s\n",
      "  Batch [300/781] | Loss: 2.2711 | Acc: 0.4531 | LR: 0.000013 | Time: 24.7s\n",
      "  Batch [320/781] | Loss: 2.3985 | Acc: 0.3750 | LR: 0.000013 | Time: 26.3s\n",
      "  Batch [340/781] | Loss: 2.3177 | Acc: 0.4297 | LR: 0.000013 | Time: 27.9s\n",
      "  Batch [360/781] | Loss: 2.4825 | Acc: 0.4062 | LR: 0.000013 | Time: 29.5s\n",
      "  Batch [380/781] | Loss: 2.4350 | Acc: 0.3828 | LR: 0.000013 | Time: 31.2s\n",
      "  Batch [400/781] | Loss: 2.5576 | Acc: 0.3906 | LR: 0.000013 | Time: 32.7s\n",
      "  Batch [420/781] | Loss: 2.2554 | Acc: 0.4766 | LR: 0.000013 | Time: 34.3s\n",
      "  Batch [440/781] | Loss: 2.1422 | Acc: 0.4453 | LR: 0.000013 | Time: 35.9s\n",
      "  Batch [460/781] | Loss: 2.4755 | Acc: 0.4453 | LR: 0.000013 | Time: 37.7s\n",
      "  Batch [480/781] | Loss: 2.4672 | Acc: 0.3672 | LR: 0.000013 | Time: 39.3s\n",
      "  Batch [500/781] | Loss: 2.2226 | Acc: 0.4531 | LR: 0.000013 | Time: 40.9s\n",
      "  Batch [520/781] | Loss: 2.5527 | Acc: 0.3672 | LR: 0.000013 | Time: 42.6s\n",
      "  Batch [540/781] | Loss: 2.4374 | Acc: 0.3828 | LR: 0.000013 | Time: 44.3s\n",
      "  Batch [560/781] | Loss: 2.4298 | Acc: 0.3906 | LR: 0.000013 | Time: 46.0s\n",
      "  Batch [580/781] | Loss: 2.4249 | Acc: 0.4062 | LR: 0.000013 | Time: 47.6s\n",
      "  Batch [600/781] | Loss: 2.4084 | Acc: 0.4297 | LR: 0.000013 | Time: 49.2s\n",
      "  Batch [620/781] | Loss: 2.4992 | Acc: 0.3984 | LR: 0.000013 | Time: 50.8s\n",
      "  Batch [640/781] | Loss: 2.6185 | Acc: 0.3750 | LR: 0.000013 | Time: 52.4s\n",
      "  Batch [660/781] | Loss: 2.3889 | Acc: 0.4141 | LR: 0.000013 | Time: 54.0s\n",
      "  Batch [680/781] | Loss: 2.4165 | Acc: 0.3906 | LR: 0.000013 | Time: 55.6s\n",
      "  Batch [700/781] | Loss: 2.6480 | Acc: 0.4141 | LR: 0.000013 | Time: 57.3s\n",
      "  Batch [720/781] | Loss: 2.5804 | Acc: 0.4141 | LR: 0.000013 | Time: 58.9s\n",
      "  Batch [740/781] | Loss: 2.7066 | Acc: 0.3594 | LR: 0.000013 | Time: 60.6s\n",
      "  Batch [760/781] | Loss: 2.4638 | Acc: 0.4453 | LR: 0.000013 | Time: 62.3s\n",
      "  Batch [780/781] | Loss: 2.2417 | Acc: 0.4609 | LR: 0.000013 | Time: 63.9s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.3955 | Train Acc: 0.4185\n",
      "    Val Loss:   2.6904 | Val Acc:   0.3711\n",
      "    LR: 0.000007 | Time: 67.9s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3711 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 28/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.5159 | Acc: 0.4062 | LR: 0.000007 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.4705 | Acc: 0.3750 | LR: 0.000007 | Time: 2.0s\n",
      "  Batch [ 40/781] | Loss: 2.1890 | Acc: 0.4688 | LR: 0.000007 | Time: 3.6s\n",
      "  Batch [ 60/781] | Loss: 2.3133 | Acc: 0.3750 | LR: 0.000007 | Time: 5.2s\n",
      "  Batch [ 80/781] | Loss: 2.4579 | Acc: 0.3594 | LR: 0.000007 | Time: 6.8s\n",
      "  Batch [100/781] | Loss: 2.4634 | Acc: 0.3828 | LR: 0.000007 | Time: 8.4s\n",
      "  Batch [120/781] | Loss: 2.5127 | Acc: 0.4375 | LR: 0.000007 | Time: 10.0s\n",
      "  Batch [140/781] | Loss: 2.3984 | Acc: 0.4297 | LR: 0.000007 | Time: 11.5s\n",
      "  Batch [160/781] | Loss: 2.0950 | Acc: 0.5000 | LR: 0.000007 | Time: 13.1s\n",
      "  Batch [180/781] | Loss: 2.2330 | Acc: 0.4453 | LR: 0.000007 | Time: 14.7s\n",
      "  Batch [200/781] | Loss: 2.4524 | Acc: 0.4297 | LR: 0.000007 | Time: 16.5s\n",
      "  Batch [220/781] | Loss: 2.1674 | Acc: 0.4297 | LR: 0.000007 | Time: 18.1s\n",
      "  Batch [240/781] | Loss: 2.0106 | Acc: 0.4766 | LR: 0.000007 | Time: 19.8s\n",
      "  Batch [260/781] | Loss: 2.2129 | Acc: 0.4062 | LR: 0.000007 | Time: 21.4s\n",
      "  Batch [280/781] | Loss: 2.3396 | Acc: 0.3984 | LR: 0.000007 | Time: 23.1s\n",
      "  Batch [300/781] | Loss: 2.3871 | Acc: 0.4141 | LR: 0.000007 | Time: 24.7s\n",
      "  Batch [320/781] | Loss: 2.1940 | Acc: 0.4766 | LR: 0.000007 | Time: 26.3s\n",
      "  Batch [340/781] | Loss: 2.7395 | Acc: 0.3281 | LR: 0.000007 | Time: 28.0s\n",
      "  Batch [360/781] | Loss: 2.2034 | Acc: 0.4531 | LR: 0.000007 | Time: 29.6s\n",
      "  Batch [380/781] | Loss: 2.2633 | Acc: 0.4531 | LR: 0.000007 | Time: 31.3s\n",
      "  Batch [400/781] | Loss: 2.1190 | Acc: 0.4922 | LR: 0.000007 | Time: 33.0s\n",
      "  Batch [420/781] | Loss: 2.7153 | Acc: 0.3359 | LR: 0.000007 | Time: 34.6s\n",
      "  Batch [440/781] | Loss: 2.3600 | Acc: 0.4219 | LR: 0.000007 | Time: 36.2s\n",
      "  Batch [460/781] | Loss: 2.4176 | Acc: 0.4688 | LR: 0.000007 | Time: 37.9s\n",
      "  Batch [480/781] | Loss: 2.3490 | Acc: 0.4062 | LR: 0.000007 | Time: 39.9s\n",
      "  Batch [500/781] | Loss: 2.3441 | Acc: 0.4141 | LR: 0.000007 | Time: 41.8s\n",
      "  Batch [520/781] | Loss: 2.3472 | Acc: 0.4219 | LR: 0.000007 | Time: 43.6s\n",
      "  Batch [540/781] | Loss: 2.7128 | Acc: 0.3281 | LR: 0.000007 | Time: 45.4s\n",
      "  Batch [560/781] | Loss: 2.1711 | Acc: 0.4844 | LR: 0.000007 | Time: 47.1s\n",
      "  Batch [580/781] | Loss: 2.2056 | Acc: 0.4766 | LR: 0.000007 | Time: 49.1s\n",
      "  Batch [600/781] | Loss: 2.4161 | Acc: 0.4844 | LR: 0.000007 | Time: 52.6s\n",
      "  Batch [620/781] | Loss: 2.3690 | Acc: 0.4453 | LR: 0.000007 | Time: 55.8s\n",
      "  Batch [640/781] | Loss: 2.4257 | Acc: 0.3984 | LR: 0.000007 | Time: 59.3s\n",
      "  Batch [660/781] | Loss: 2.5424 | Acc: 0.3438 | LR: 0.000007 | Time: 62.8s\n",
      "  Batch [680/781] | Loss: 2.2750 | Acc: 0.3984 | LR: 0.000007 | Time: 66.4s\n",
      "  Batch [700/781] | Loss: 2.4867 | Acc: 0.4141 | LR: 0.000007 | Time: 69.9s\n",
      "  Batch [720/781] | Loss: 2.3400 | Acc: 0.4141 | LR: 0.000007 | Time: 73.4s\n",
      "  Batch [740/781] | Loss: 2.8674 | Acc: 0.3438 | LR: 0.000007 | Time: 76.9s\n",
      "  Batch [760/781] | Loss: 2.7239 | Acc: 0.3828 | LR: 0.000007 | Time: 80.5s\n",
      "  Batch [780/781] | Loss: 2.1083 | Acc: 0.5078 | LR: 0.000007 | Time: 84.1s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.3912 | Train Acc: 0.4199\n",
      "    Val Loss:   2.6837 | Val Acc:   0.3726\n",
      "    LR: 0.000003 | Time: 90.6s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3726 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 29/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.4806 | Acc: 0.4375 | LR: 0.000003 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.3295 | Acc: 0.3750 | LR: 0.000003 | Time: 3.7s\n",
      "  Batch [ 40/781] | Loss: 2.2207 | Acc: 0.4453 | LR: 0.000003 | Time: 6.2s\n",
      "  Batch [ 60/781] | Loss: 2.4079 | Acc: 0.3984 | LR: 0.000003 | Time: 9.7s\n",
      "  Batch [ 80/781] | Loss: 2.2182 | Acc: 0.4609 | LR: 0.000003 | Time: 13.0s\n",
      "  Batch [100/781] | Loss: 2.0711 | Acc: 0.5312 | LR: 0.000003 | Time: 16.5s\n",
      "  Batch [120/781] | Loss: 2.7249 | Acc: 0.3672 | LR: 0.000003 | Time: 19.8s\n",
      "  Batch [140/781] | Loss: 2.6323 | Acc: 0.4141 | LR: 0.000003 | Time: 23.1s\n",
      "  Batch [160/781] | Loss: 2.6286 | Acc: 0.3594 | LR: 0.000003 | Time: 26.4s\n",
      "  Batch [180/781] | Loss: 2.1177 | Acc: 0.4766 | LR: 0.000003 | Time: 30.0s\n",
      "  Batch [200/781] | Loss: 2.3547 | Acc: 0.4141 | LR: 0.000003 | Time: 33.3s\n",
      "  Batch [220/781] | Loss: 2.3540 | Acc: 0.3594 | LR: 0.000003 | Time: 36.7s\n",
      "  Batch [240/781] | Loss: 2.4609 | Acc: 0.3750 | LR: 0.000003 | Time: 40.2s\n",
      "  Batch [260/781] | Loss: 2.0701 | Acc: 0.4688 | LR: 0.000003 | Time: 43.5s\n",
      "  Batch [280/781] | Loss: 2.5243 | Acc: 0.4141 | LR: 0.000003 | Time: 46.9s\n",
      "  Batch [300/781] | Loss: 2.5236 | Acc: 0.3750 | LR: 0.000003 | Time: 50.1s\n",
      "  Batch [320/781] | Loss: 2.2621 | Acc: 0.4141 | LR: 0.000003 | Time: 53.6s\n",
      "  Batch [340/781] | Loss: 2.5322 | Acc: 0.3906 | LR: 0.000003 | Time: 56.9s\n",
      "  Batch [360/781] | Loss: 2.4427 | Acc: 0.4141 | LR: 0.000003 | Time: 60.2s\n",
      "  Batch [380/781] | Loss: 2.5701 | Acc: 0.3281 | LR: 0.000003 | Time: 63.5s\n",
      "  Batch [400/781] | Loss: 2.6585 | Acc: 0.3203 | LR: 0.000003 | Time: 66.7s\n",
      "  Batch [420/781] | Loss: 2.5176 | Acc: 0.3516 | LR: 0.000003 | Time: 70.2s\n",
      "  Batch [440/781] | Loss: 2.4386 | Acc: 0.3906 | LR: 0.000003 | Time: 73.7s\n",
      "  Batch [460/781] | Loss: 2.3060 | Acc: 0.4688 | LR: 0.000003 | Time: 77.2s\n",
      "  Batch [480/781] | Loss: 2.1363 | Acc: 0.4609 | LR: 0.000003 | Time: 80.7s\n",
      "  Batch [500/781] | Loss: 2.1174 | Acc: 0.4844 | LR: 0.000003 | Time: 84.2s\n",
      "  Batch [520/781] | Loss: 2.3703 | Acc: 0.4141 | LR: 0.000003 | Time: 87.6s\n",
      "  Batch [540/781] | Loss: 2.3940 | Acc: 0.4062 | LR: 0.000003 | Time: 90.9s\n",
      "  Batch [560/781] | Loss: 2.3570 | Acc: 0.4141 | LR: 0.000003 | Time: 94.2s\n",
      "  Batch [580/781] | Loss: 2.6105 | Acc: 0.3047 | LR: 0.000003 | Time: 97.6s\n",
      "  Batch [600/781] | Loss: 2.1163 | Acc: 0.4922 | LR: 0.000003 | Time: 99.6s\n",
      "  Batch [620/781] | Loss: 2.2195 | Acc: 0.4531 | LR: 0.000003 | Time: 102.4s\n",
      "  Batch [640/781] | Loss: 2.4758 | Acc: 0.3750 | LR: 0.000003 | Time: 105.8s\n",
      "  Batch [660/781] | Loss: 2.1174 | Acc: 0.5156 | LR: 0.000003 | Time: 109.2s\n",
      "  Batch [680/781] | Loss: 2.3516 | Acc: 0.3828 | LR: 0.000003 | Time: 112.6s\n",
      "  Batch [700/781] | Loss: 2.4944 | Acc: 0.4531 | LR: 0.000003 | Time: 116.0s\n",
      "  Batch [720/781] | Loss: 2.4202 | Acc: 0.4531 | LR: 0.000003 | Time: 119.2s\n",
      "  Batch [740/781] | Loss: 2.5906 | Acc: 0.3438 | LR: 0.000003 | Time: 122.5s\n",
      "  Batch [760/781] | Loss: 2.4291 | Acc: 0.4219 | LR: 0.000003 | Time: 126.1s\n",
      "  Batch [780/781] | Loss: 2.1174 | Acc: 0.4531 | LR: 0.000003 | Time: 129.6s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.3723 | Train Acc: 0.4225\n",
      "    Val Loss:   2.6854 | Val Acc:   0.3741\n",
      "    LR: 0.000001 | Time: 136.5s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/best_model.pth\n",
      "    *** New best validation accuracy: 0.3741 ***\n",
      "----------------------------------------\n",
      "\n",
      "Epoch [ 30/ 30]\n",
      "----------------------------------------\n",
      "  Batch [  0/781] | Loss: 2.5138 | Acc: 0.3828 | LR: 0.000001 | Time: 0.3s\n",
      "  Batch [ 20/781] | Loss: 2.2440 | Acc: 0.4609 | LR: 0.000001 | Time: 3.6s\n",
      "  Batch [ 40/781] | Loss: 2.5059 | Acc: 0.4141 | LR: 0.000001 | Time: 6.7s\n",
      "  Batch [ 60/781] | Loss: 2.2714 | Acc: 0.5000 | LR: 0.000001 | Time: 9.8s\n",
      "  Batch [ 80/781] | Loss: 2.4602 | Acc: 0.4062 | LR: 0.000001 | Time: 13.1s\n",
      "  Batch [100/781] | Loss: 2.3155 | Acc: 0.3750 | LR: 0.000001 | Time: 16.5s\n",
      "  Batch [120/781] | Loss: 2.4564 | Acc: 0.4141 | LR: 0.000001 | Time: 20.0s\n",
      "  Batch [140/781] | Loss: 2.5789 | Acc: 0.3984 | LR: 0.000001 | Time: 23.6s\n",
      "  Batch [160/781] | Loss: 2.4223 | Acc: 0.3984 | LR: 0.000001 | Time: 27.1s\n",
      "  Batch [180/781] | Loss: 2.0986 | Acc: 0.4297 | LR: 0.000001 | Time: 30.6s\n",
      "  Batch [200/781] | Loss: 2.3248 | Acc: 0.4375 | LR: 0.000001 | Time: 34.1s\n",
      "  Batch [220/781] | Loss: 2.4738 | Acc: 0.4219 | LR: 0.000001 | Time: 37.4s\n",
      "  Batch [240/781] | Loss: 2.4191 | Acc: 0.3594 | LR: 0.000001 | Time: 40.7s\n",
      "  Batch [260/781] | Loss: 2.4611 | Acc: 0.3906 | LR: 0.000001 | Time: 44.0s\n",
      "  Batch [280/781] | Loss: 2.2604 | Acc: 0.3906 | LR: 0.000001 | Time: 47.5s\n",
      "  Batch [300/781] | Loss: 2.5559 | Acc: 0.3828 | LR: 0.000001 | Time: 50.9s\n",
      "  Batch [320/781] | Loss: 2.3325 | Acc: 0.4219 | LR: 0.000001 | Time: 54.5s\n",
      "  Batch [340/781] | Loss: 2.4559 | Acc: 0.3672 | LR: 0.000001 | Time: 58.0s\n",
      "  Batch [360/781] | Loss: 2.3911 | Acc: 0.3750 | LR: 0.000001 | Time: 61.5s\n",
      "  Batch [380/781] | Loss: 2.3997 | Acc: 0.3984 | LR: 0.000001 | Time: 65.0s\n",
      "  Batch [400/781] | Loss: 2.5144 | Acc: 0.3438 | LR: 0.000001 | Time: 68.4s\n",
      "  Batch [420/781] | Loss: 2.3932 | Acc: 0.4219 | LR: 0.000001 | Time: 72.0s\n",
      "  Batch [440/781] | Loss: 2.4482 | Acc: 0.4141 | LR: 0.000001 | Time: 75.4s\n",
      "  Batch [460/781] | Loss: 2.2907 | Acc: 0.4062 | LR: 0.000001 | Time: 78.9s\n",
      "  Batch [480/781] | Loss: 2.2344 | Acc: 0.5234 | LR: 0.000001 | Time: 82.2s\n",
      "  Batch [500/781] | Loss: 2.3864 | Acc: 0.4062 | LR: 0.000001 | Time: 85.6s\n",
      "  Batch [520/781] | Loss: 2.3029 | Acc: 0.4297 | LR: 0.000001 | Time: 88.9s\n",
      "  Batch [540/781] | Loss: 2.1289 | Acc: 0.4688 | LR: 0.000001 | Time: 91.8s\n",
      "  Batch [560/781] | Loss: 2.4294 | Acc: 0.4375 | LR: 0.000001 | Time: 93.6s\n",
      "  Batch [580/781] | Loss: 2.1954 | Acc: 0.4844 | LR: 0.000001 | Time: 96.1s\n",
      "  Batch [600/781] | Loss: 2.4745 | Acc: 0.4297 | LR: 0.000001 | Time: 98.1s\n",
      "  Batch [620/781] | Loss: 2.2423 | Acc: 0.4141 | LR: 0.000001 | Time: 101.6s\n",
      "  Batch [640/781] | Loss: 2.5113 | Acc: 0.3828 | LR: 0.000001 | Time: 103.3s\n",
      "  Batch [660/781] | Loss: 2.2740 | Acc: 0.4609 | LR: 0.000001 | Time: 106.5s\n",
      "  Batch [680/781] | Loss: 2.3393 | Acc: 0.4531 | LR: 0.000001 | Time: 110.1s\n",
      "  Batch [700/781] | Loss: 2.5065 | Acc: 0.3594 | LR: 0.000001 | Time: 113.4s\n",
      "  Batch [720/781] | Loss: 2.4222 | Acc: 0.4141 | LR: 0.000001 | Time: 116.7s\n",
      "  Batch [740/781] | Loss: 2.4003 | Acc: 0.4141 | LR: 0.000001 | Time: 120.0s\n",
      "  Batch [760/781] | Loss: 2.3297 | Acc: 0.4141 | LR: 0.000001 | Time: 123.2s\n",
      "  Batch [780/781] | Loss: 2.3194 | Acc: 0.4531 | LR: 0.000001 | Time: 126.7s\n",
      "\n",
      "  Results:\n",
      "    Train Loss: 2.3724 | Train Acc: 0.4226\n",
      "    Val Loss:   2.6827 | Val Acc:   0.3741\n",
      "    LR: 0.000000 | Time: 133.7s\n",
      "    Visualizing top-k tokens...\n",
      "Checkpoint saved: ./vit_checkpoints/checkpoint_epoch_30.pth\n",
      "----------------------------------------\n",
      "\n",
      "Training completed!\n",
      "Best validation accuracy: 0.3741\n",
      "\n",
      "Plotting training history...\n",
      "Training history plot saved: ./training_history.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL TESTING\n",
      "Testing model on test set...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 660\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINAL TESTING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 660\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtest_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Training completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal test accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 573\u001b[0m, in \u001b[0;36mtest_trained_model\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m    572\u001b[0m             label \u001b[38;5;241m=\u001b[39m labels[i]\n\u001b[0;32m--> 573\u001b[0m             class_correct[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m c[i]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    574\u001b[0m             class_total[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    576\u001b[0m overall_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "import time\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import wandb\n",
    "from typing import List, Optional\n",
    "\n",
    "class TrainingMetrics:\n",
    "    \"\"\"Class to track training metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def update_train(self, loss: float, accuracy: float, lr: float):\n",
    "        self.train_losses.append(loss)\n",
    "        self.train_accuracies.append(accuracy)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "    def update_val(self, loss: float, accuracy: float):\n",
    "        self.val_losses.append(loss)\n",
    "        self.val_accuracies.append(accuracy)\n",
    "\n",
    "    def add_epoch_time(self, time: float):\n",
    "        self.epoch_times.append(time)\n",
    "\n",
    "    def get_best_val_accuracy(self):\n",
    "        return max(self.val_accuracies) if self.val_accuracies else 0.0\n",
    "\n",
    "    def get_latest_metrics(self):\n",
    "        return {\n",
    "            'train_loss': self.train_losses[-1] if self.train_losses else 0.0,\n",
    "            'train_acc': self.train_accuracies[-1] if self.train_accuracies else 0.0,\n",
    "            'val_loss': self.val_losses[-1] if self.val_losses else 0.0,\n",
    "            'val_acc': self.val_accuracies[-1] if self.val_accuracies else 0.0,\n",
    "            'lr': self.learning_rates[-1] if self.learning_rates else 0.0\n",
    "        }\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device, print_freq=50):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "\n",
    "    Args:\n",
    "        model: Vision Transformer model\n",
    "        train_loader: Training data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        device: Device to train on\n",
    "        print_freq: How often to print progress\n",
    "\n",
    "    Returns:\n",
    "        Average loss and accuracy for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update scheduler if it's step-based\n",
    "        if scheduler is not None and hasattr(scheduler, 'step') and not isinstance(scheduler, (CosineAnnealingLR,)):\n",
    "            scheduler.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Print progress\n",
    "        if batch_idx % print_freq == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            batch_acc = torch.sum(preds == labels.data).item() / inputs.size(0)\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            print(f'  Batch [{batch_idx:3d}/{len(train_loader):3d}] | '\n",
    "                  f'Loss: {loss.item():.4f} | '\n",
    "                  f'Acc: {batch_acc:.4f} | '\n",
    "                  f'LR: {current_lr:.6f} | '\n",
    "                  f'Time: {elapsed:.1f}s')\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate for one epoch\n",
    "\n",
    "    Args:\n",
    "        model: Vision Transformer model\n",
    "        val_loader: Validation data loader\n",
    "        criterion: Loss function\n",
    "        device: Device to validate on\n",
    "\n",
    "    Returns:\n",
    "        Average loss and accuracy for the epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data).item()\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def create_optimizer(model, optimizer_name='adamw', learning_rate=1e-3, weight_decay=0.05):\n",
    "    \"\"\"Create optimizer based on name\"\"\"\n",
    "    if optimizer_name.lower() == 'adamw':\n",
    "        return optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name.lower() == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name.lower() == 'sgd':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "\n",
    "def create_scheduler(optimizer, scheduler_name='cosine', num_epochs=100, steps_per_epoch=None):\n",
    "    \"\"\"Create learning rate scheduler\"\"\"\n",
    "    if scheduler_name.lower() == 'cosine':\n",
    "        return CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    elif scheduler_name.lower() == 'onecycle' and steps_per_epoch is not None:\n",
    "        return OneCycleLR(optimizer, max_lr=optimizer.param_groups[0]['lr'],\n",
    "                         steps_per_epoch=steps_per_epoch, epochs=num_epochs)\n",
    "    elif scheduler_name.lower() == 'none':\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler: {scheduler_name}\")\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, metrics, filepath):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'metrics': {\n",
    "            'train_losses': metrics.train_losses,\n",
    "            'train_accuracies': metrics.train_accuracies,\n",
    "            'val_losses': metrics.val_losses,\n",
    "            'val_accuracies': metrics.val_accuracies,\n",
    "            'learning_rates': metrics.learning_rates\n",
    "        },\n",
    "        'best_val_acc': metrics.get_best_val_accuracy()\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved: {filepath}\")\n",
    "\n",
    "\n",
    "def train_vision_transformer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs: int = 50,\n",
    "    learning_rate: float = 1e-3,\n",
    "    weight_decay: float = 0.05,\n",
    "    optimizer_name: str = 'adamw',\n",
    "    scheduler_name: str = 'cosine',\n",
    "    device: str = 'cuda',\n",
    "    save_path: str = './checkpoints',\n",
    "    print_freq: int = 50,\n",
    "    save_freq: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete training loop for Vision Transformer\n",
    "\n",
    "    Args:\n",
    "        model: Vision Transformer model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Initial learning rate\n",
    "        weight_decay: Weight decay for regularization\n",
    "        optimizer_name: Optimizer name ('adamw', 'adam', 'sgd')\n",
    "        scheduler_name: Scheduler name ('cosine', 'onecycle', 'none')\n",
    "        device: Device to train on\n",
    "        save_path: Path to save checkpoints\n",
    "        print_freq: Frequency of printing training progress\n",
    "        save_freq: Frequency of saving checkpoints\n",
    "\n",
    "    Returns:\n",
    "        Trained model and training metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = create_optimizer(model, optimizer_name, learning_rate, weight_decay)\n",
    "\n",
    "    # --- W&B GRADIENT TRACKING SETUP ---\n",
    "    # We check if a run is active to avoid errors if running locally without wandb\n",
    "    if wandb.run is not None:\n",
    "        print(\"W&B: Watching model gradients...\")\n",
    "        # log=\"gradients\" tracks gradients\n",
    "        # log=\"all\" tracks gradients AND parameter values (weights)\n",
    "        # log_freq determines how often to upload histograms (batch-wise)\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=print_freq, log_graph=True)\n",
    "    # -----------------------------------\n",
    "\n",
    "    # Scheduler\n",
    "    steps_per_epoch = len(train_loader) if scheduler_name.lower() == 'onecycle' else None\n",
    "    scheduler = create_scheduler(optimizer, scheduler_name, num_epochs, steps_per_epoch)\n",
    "\n",
    "    # Metrics tracking\n",
    "    metrics = TrainingMetrics()\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "\n",
    "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        print(f\"\\nEpoch [{epoch+1:3d}/{num_epochs:3d}]\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Training phase\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler, device, print_freq\n",
    "        )\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        # Update scheduler (for epoch-based schedulers)\n",
    "        if scheduler is not None and isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "\n",
    "        # Record metrics\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        metrics.update_train(train_loss, train_acc, current_lr)\n",
    "        metrics.update_val(val_loss, val_acc)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        metrics.add_epoch_time(epoch_time)\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"\\n  Results:\")\n",
    "        print(f\"    Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"    Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "        print(f\"    LR: {current_lr:.6f} | Time: {epoch_time:.1f}s\")\n",
    "\n",
    "        # Log to Weights & Biases\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train/loss\": train_loss,\n",
    "                \"train/acc\": train_acc,\n",
    "                \"val/loss\": val_loss,\n",
    "                \"val/acc\": val_acc,\n",
    "                \"lr\": current_lr,\n",
    "                \"epoch_time\": epoch_time,\n",
    "                \"best_val_acc\": best_val_acc\n",
    "            }, step=epoch + 1)\n",
    "        \n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_path = os.path.join(save_path, 'best_model.pth')\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, metrics, best_model_path)\n",
    "            print(f\"    *** New best validation accuracy: {best_val_acc:.4f} ***\")\n",
    "\n",
    "        # Save periodic checkpoint\n",
    "        if (epoch + 1) % save_freq == 0:\n",
    "            checkpoint_path = os.path.join(save_path, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, metrics, checkpoint_path)\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    return model, metrics\n",
    "\n",
    "\n",
    "def plot_training_history(metrics, save_path=None):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle('Training History', fontsize=16)\n",
    "\n",
    "    epochs = range(1, len(metrics.train_losses) + 1)\n",
    "\n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(epochs, metrics.train_losses, 'b-', label='Train Loss', alpha=0.8)\n",
    "    axes[0, 0].plot(epochs, metrics.val_losses, 'r-', label='Val Loss', alpha=0.8)\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy plot\n",
    "    axes[0, 1].plot(epochs, [acc*100 for acc in metrics.train_accuracies], 'b-', label='Train Acc', alpha=0.8)\n",
    "    axes[0, 1].plot(epochs, [acc*100 for acc in metrics.val_accuracies], 'r-', label='Val Acc', alpha=0.8)\n",
    "    axes[0, 1].set_title('Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Learning rate plot\n",
    "    axes[1, 0].plot(epochs, metrics.learning_rates, 'g-', alpha=0.8)\n",
    "    axes[1, 0].set_title('Learning Rate')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Training time plot\n",
    "    if metrics.epoch_times:\n",
    "        axes[1, 1].plot(epochs, metrics.epoch_times, 'm-', alpha=0.8)\n",
    "        axes[1, 1].set_title('Epoch Time')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Time (seconds)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Training history plot saved: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This is where we bring everything together!\n",
    "    print(\"MiniImageNetData Vision Transformer Training\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "    print(\"Setting up data...\")\n",
    "    data_module = MiniImageNetDataModule(\n",
    "        batch_size=128,\n",
    "        num_workers=2,\n",
    "        image_size=64,\n",
    "        augment_train=True\n",
    "    )\n",
    "\n",
    "    train_loader, test_loader = data_module.get_dataloaders()\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "    # Create Vision Transformer model (assuming create_vit_model is loaded)\n",
    "    print(f\"\\nCreating Vision Transformer model...\")\n",
    "    pattern = ['modulated', 'modulated']\n",
    "    model = create_hybrid_vit(\n",
    "            model_size='small',\n",
    "            num_classes=200,\n",
    "            image_size=64,\n",
    "            attention_pattern=pattern,\n",
    "            modulate_v=True\n",
    "        )\n",
    "\n",
    "    model_info = model.get_model_info()\n",
    "    print(f\"Model created:\")\n",
    "    print(f\"  Parameters: {model_info['total_params']:,}\")\n",
    "    print(f\"  Model size: {model_info['model_size_mb']:.2f} MB\")\n",
    "    print(f\"  Patches: {model_info['num_patches']}\")\n",
    "\n",
    "    # Training configuration\n",
    "    training_config = {\n",
    "        'num_epochs': 30,  # Start with fewer epochs for testing\n",
    "        'learning_rate': 3e-4,\n",
    "        'weight_decay': 0.05,\n",
    "        'optimizer_name': 'adamw',\n",
    "        'scheduler_name': 'cosine',\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'save_path': './vit_checkpoints',\n",
    "        'print_freq': 20,\n",
    "        'save_freq': 5\n",
    "    }\n",
    "\n",
    "    print(f\"\\nTraining configuration:\")\n",
    "    for key, value in training_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Start training!\n",
    "    print(f\"\\n Starting training...\")\n",
    "    trained_model, metrics = train_vision_transformer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,  # Using test set as validation for simplicity\n",
    "        **training_config\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    print(f\"\\nPlotting training history...\")\n",
    "    plot_training_history(metrics, save_path='./training_history.png')\n",
    "\n",
    "    print(f\"\\n Training completed!\")\n",
    "    print(f\"Best validation accuracy: {metrics.get_best_val_accuracy()*100:.2f}%\")\n",
    "\n",
    "    # Training summary\n",
    "    total_time = sum(metrics.epoch_times) if metrics.epoch_times else 0\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Total training time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"  Average epoch time: {np.mean(metrics.epoch_times):.1f} seconds\")\n",
    "    print(f\"  Final learning rate: {metrics.learning_rates[-1]:.6f}\")\n",
    "\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\n",
    "            \"final/best_val_acc\": metrics.get_best_val_accuracy()\n",
    "        })\n",
    "\n",
    "        wandb.summary[\"best_val_acc\"] = metrics.get_best_val_accuracy()\n",
    "        wandb.summary[\"total_train_time_min\"] = total_time / 60.0\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00a175706fc348d68578e739da15a75d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0442c7fac7a9436cbb765eb7ae292e14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05e505ed659f4c97b6ddd55f90b162fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0ce5ccd7162245e5a40e52d9bcb2f8af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d88086521df4c4080b2386129f4af00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbdb75c8f40744078b4b589f8dd59831",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2c1b7410473548d7bb82cb9cc8ed1c9b",
      "value": 10000
     }
    },
    "0da1d8c74b864e17b162cb09d1585d2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0eb58470bfb142dca33497ee4b647c3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "11aa875433c94ecdb7c9d5cb450fe049": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12dc3cce24824735b97e8789fb01e77a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14c9091cdf304cdfbcf9464f07390c9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1ec6dab0093a4a25b70ddfbbc3064311": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_845b78b379414586b0044c2b3a46036b",
      "placeholder": "",
      "style": "IPY_MODEL_2b08bc777e554917a405b5be35eb1da6",
      "value": "120M/120M[00:01&lt;00:00,119MB/s]"
     }
    },
    "20b0d76b1bd14de8a9a126ca1e62a13b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0442c7fac7a9436cbb765eb7ae292e14",
      "max": 23940850,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83db8b50d706434b9a993ea611dfc746",
      "value": 23940850
     }
    },
    "231fd1fc180f449ca258d07a7a119b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf1c47bad3154763aba233c890e03ea1",
       "IPY_MODEL_cc84fa50b0474d96b58265c77ae3c756",
       "IPY_MODEL_d3dd0b99423f4593a1d4e9dbded83da1"
      ],
      "layout": "IPY_MODEL_0ce5ccd7162245e5a40e52d9bcb2f8af"
     }
    },
    "23b8f48359c644aca65923ab366acb06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_556fbbbbb3cf45eebf313d4328a2b8a1",
       "IPY_MODEL_68604bf106d04bb888fd5a4e8557ec83",
       "IPY_MODEL_1ec6dab0093a4a25b70ddfbbc3064311"
      ],
      "layout": "IPY_MODEL_879a46a3084b4f8cbd0beab1cabc41db"
     }
    },
    "298787fa51554b21bb29c115fa13bbdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f26c33a7bae344ea9f29bc96bfc325b8",
       "IPY_MODEL_0d88086521df4c4080b2386129f4af00",
       "IPY_MODEL_eb762eb8398042e4914430f61586777d"
      ],
      "layout": "IPY_MODEL_a5cc87c1fb2e4317a64ff9e526f59e8b"
     }
    },
    "2a78c646586743e78e77ac3233c3854e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12dc3cce24824735b97e8789fb01e77a",
      "placeholder": "",
      "style": "IPY_MODEL_00a175706fc348d68578e739da15a75d",
      "value": "plain_text/test-00000-of-00001.parquet:100%"
     }
    },
    "2b08bc777e554917a405b5be35eb1da6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c0df1a3570d4707854d5a941325a4c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c1b7410473548d7bb82cb9cc8ed1c9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3d73913da63b4400a78b35f017ac3093": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42ce5e7287984db2b0f0a731381193d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f6418b0b6934deeabcf4a9ea1a11497": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6cb0c7246def42778f8508160c308575",
      "max": 50000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5844608062e34fc38d51a843a51bccc0",
      "value": 50000
     }
    },
    "4f7a1c9c81ea4945b9c97635d1542a54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52f358de4cfb4bd99b428847997d80c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d239b76c38004f35a4914525f4a23a4d",
      "placeholder": "",
      "style": "IPY_MODEL_42ce5e7287984db2b0f0a731381193d7",
      "value": "Generatingtrainsplit:100%"
     }
    },
    "556fbbbbb3cf45eebf313d4328a2b8a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d73913da63b4400a78b35f017ac3093",
      "placeholder": "",
      "style": "IPY_MODEL_0eb58470bfb142dca33497ee4b647c3b",
      "value": "plain_text/train-00000-of-00001.parquet:100%"
     }
    },
    "58157bd3d5ed4169a68790118c4db4cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52f358de4cfb4bd99b428847997d80c1",
       "IPY_MODEL_4f6418b0b6934deeabcf4a9ea1a11497",
       "IPY_MODEL_a9d778ca184b494fb97f63a45a7fa358"
      ],
      "layout": "IPY_MODEL_fe53f38c73524d838e3daecf04c9b937"
     }
    },
    "5844608062e34fc38d51a843a51bccc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5a4edba3c5de48e3b27bd8124e291f83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6249e679ace849cb82e4b54cf86f30f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68604bf106d04bb888fd5a4e8557ec83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d614aee36bcc4506b9d8f3818f9b19a0",
      "max": 119705255,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_05e505ed659f4c97b6ddd55f90b162fe",
      "value": 119705255
     }
    },
    "6cb0c7246def42778f8508160c308575": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83db8b50d706434b9a993ea611dfc746": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "845b78b379414586b0044c2b3a46036b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "879a46a3084b4f8cbd0beab1cabc41db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9856d182e3904f6aada33f927af455c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2a78c646586743e78e77ac3233c3854e",
       "IPY_MODEL_20b0d76b1bd14de8a9a126ca1e62a13b",
       "IPY_MODEL_bb1bb0d26315450f86936fa5384169a6"
      ],
      "layout": "IPY_MODEL_c8f6424e46a24146a815356313d2def8"
     }
    },
    "996eca95ad58400a9d85c8426cd584f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fd7cb6a25ac4138bc050555fc6f53b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a069cf545f534ce3b88fea8168d505d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "a5cc87c1fb2e4317a64ff9e526f59e8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8500a8686ce47338490733dc75beb32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9d778ca184b494fb97f63a45a7fa358": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a4edba3c5de48e3b27bd8124e291f83",
      "placeholder": "",
      "style": "IPY_MODEL_6249e679ace849cb82e4b54cf86f30f5",
      "value": "50000/50000[00:00&lt;00:00,94319.33examples/s]"
     }
    },
    "b7d307f1bebe48dd88481d0ddba87d5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb1bb0d26315450f86936fa5384169a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0da1d8c74b864e17b162cb09d1585d2c",
      "placeholder": "",
      "style": "IPY_MODEL_e403cfc1766042f8be395fb393b4ee56",
      "value": "23.9M/23.9M[00:00&lt;00:00,80.0MB/s]"
     }
    },
    "bbdb75c8f40744078b4b589f8dd59831": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8f6424e46a24146a815356313d2def8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc84fa50b0474d96b58265c77ae3c756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a069cf545f534ce3b88fea8168d505d5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14c9091cdf304cdfbcf9464f07390c9f",
      "value": 1
     }
    },
    "cf1c47bad3154763aba233c890e03ea1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11aa875433c94ecdb7c9d5cb450fe049",
      "placeholder": "",
      "style": "IPY_MODEL_996eca95ad58400a9d85c8426cd584f8",
      "value": "README.md:"
     }
    },
    "d239b76c38004f35a4914525f4a23a4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3dd0b99423f4593a1d4e9dbded83da1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f7a1c9c81ea4945b9c97635d1542a54",
      "placeholder": "",
      "style": "IPY_MODEL_2c0df1a3570d4707854d5a941325a4c0",
      "value": "5.16k/?[00:00&lt;00:00,537kB/s]"
     }
    },
    "d614aee36bcc4506b9d8f3818f9b19a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db859b6bd6834b41b02b7d641d86e823": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e403cfc1766042f8be395fb393b4ee56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb762eb8398042e4914430f61586777d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8500a8686ce47338490733dc75beb32",
      "placeholder": "",
      "style": "IPY_MODEL_b7d307f1bebe48dd88481d0ddba87d5b",
      "value": "10000/10000[00:00&lt;00:00,70007.23examples/s]"
     }
    },
    "f26c33a7bae344ea9f29bc96bfc325b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fd7cb6a25ac4138bc050555fc6f53b5",
      "placeholder": "",
      "style": "IPY_MODEL_db859b6bd6834b41b02b7d641d86e823",
      "value": "Generatingtestsplit:100%"
     }
    },
    "fe53f38c73524d838e3daecf04c9b937": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
